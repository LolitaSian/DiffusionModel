{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LolitaSian/DiffusionModel/blob/main/jupyter/ddpm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5a94671",
      "metadata": {
        "id": "c5a94671"
      },
      "source": [
        "<h1>\n",
        "\tThe Annotated Diffusion Model\n",
        "</h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "290edb0b",
      "metadata": {
        "id": "290edb0b"
      },
      "source": [
        "\n",
        "在这篇博客文章中，我们将逐步讲解([Ho et al., 2020](https://arxiv.org/abs/2006.11239))的原始DDPM论文，并基于[Phil Wang的TensorFlow版本]((https://github.com/lucidrains/denoising-diffusion-pytorch))实现Pytorch版本。请注意，扩散用于生成建模的思想实际上已经在([Sohl-Dickstein et al., 2015](https://arxiv.org/abs/1503.03585))中介绍过，但是，直到斯坦福大学的([Song et al., 2019](https://arxiv.org/abs/1907.05600))和谷歌大脑的([Ho et al., 2020](https://arxiv.org/abs/2006.11239))分别改进了这种方法才得以流行起来。\n",
        "\n",
        "[扩散模型有几个视角](https://twitter.com/sedielem/status/1530894256168222722?s=20&t=mfv4afx1GcNQU5fZklpACw)。在这里我们采用离散时间（潜变量模型）的视角."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a1f2d714",
      "metadata": {
        "id": "a1f2d714"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U einops datasets matplotlib tqdm\n",
        "\n",
        "import math\n",
        "from inspect import isfunction\n",
        "from functools import partial\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "from einops import rearrange\n",
        "\n",
        "import torch\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fe49a34",
      "metadata": {
        "id": "6fe49a34"
      },
      "source": [
        "\n",
        "## 什么是扩散模型\n",
        "\n",
        "Diffusion model 和 Normalizing Flows, GANs or VAEs 一样，都是将噪声从一些简单的分布转换为一个数据样本，也是神经网络学习从纯噪声开始逐渐去噪数据的过程。\n",
        "包含两个步骤：\n",
        "- 一个我们选择的固定的（或者说预定义好的）前向扩散过程 $q$ ，就是逐渐给图片添加高斯噪声，直到最后获得纯噪声。\n",
        "\n",
        "- 一个需要学习的反向的去噪过程 $p_\\theta$，训练一个神经网做图像去噪，从纯噪声开始，直到获得最终图像。\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?id=1t5dUyJwgy2ZpDAqHXw7GhUAp2FE5BWHA\" width=\"600\" />\n",
        "</p>\n",
        "\n",
        "前向和反向过程都要经过时间步$t$，总步长是$T$（DDPM中$T=1000$)。\n",
        "\n",
        "从$t=0$开始，从数据集分布中采样一个真实图片$\\mathbf x_0$。前向过程就是在每一个时间步$t$中都从一个高斯分布中采样一个噪声，将其添加到上一时间步的图像上。给出一个足够大的$T$，和每一时间步中添加噪声的表格，最终在$T$时间步你会获得一个[isotropic Gaussian distribution](https://math.stackexchange.com/questions/1991961/gaussian-distribution-is-isotropic)。\n",
        "\n",
        "\n",
        "\n",
        "## In more mathematical form\n",
        "\n",
        "\n",
        "我们令$q(\\mathbf x_0)$是真实分布，也就是真实的图像的分布。\n",
        "\n",
        "我们可以从中采样一个图片，也就是$\\mathbf x_0 \\sim q(\\mathbf x_0)$ 。\n",
        "\n",
        "我们设定前向扩散过程$q(\\mathbf x_t|\\mathbf x_{t-1})$是给每个时间步$t$添加高斯噪声，这个高斯噪声不是随机选择的，是根据我们预选设定好的方差表（$0 < \\beta_1 < \\beta_2 < ... < \\beta_T < 1$）的高斯分布中获取的。\n",
        "\n",
        "然后我们就可以得到前向过程的公式为：\n",
        "$$\n",
        "q(\\mathbf {x}_t | \\mathbf {x}_{t-1}) = \\mathcal{N}(\\mathbf {x}_t; \\sqrt{1 - \\beta_t} \\mathbf {x}_{t-1}, \\beta_t \\mathbf{I}). \n",
        "$$\n",
        "\n",
        "回想一下哦。一个高斯分布（也叫正态分布）是由两个参数决定的，均值$\\mu$和方差$\\sigma^2 \\geq 0$。\n",
        "\n",
        "然后我们就可以认为每个时间步$t$的图像是从一均值为${\\mu}_t = \\sqrt{1 - \\beta_t} \\mathbf {x}_{t-1}$、方差为$\\sigma^2_t = \\beta_t$的条件高斯分布中画出来的。借助参数重整化（reparameterization trick）可以写成\n",
        "\n",
        "$$\n",
        "\\mathbf {x}_t = \\sqrt{1 - \\beta_t}\\mathbf {x}_{t-1} +  \\sqrt{\\beta_t} \\mathbf{\\epsilon}\n",
        "$$\n",
        "\n",
        "其中$\\mathbf{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$，是从标准高斯分布中采样的噪声。 \n",
        "\n",
        "$\\beta_t$在不同的时间步$t$中不是固定的，因此我们给$\\beta$加了下标。对于$\\beta_t$的选择我们可以设置为线性的、二次的、余弦的等(有点像学习率计划)。\n",
        "\n",
        "比如在DDPM中$\\beta_1 = 10^{-4}$, $\\beta_T = 0.02$，在中间是做了一个线性插值。而在Improved DDPM中是使用余弦函数。\n",
        "\n",
        "从$\\mathbf x_0$开始，我们通过$\\mathbf{x}_1,  ..., \\mathbf{x}_t, ..., \\mathbf{x}_T$,最终获得$\\mathbf{x}_T$ ，如果我们的高斯噪声表设置的合理，那最后我们获得的应该是一个纯高斯噪声。\n",
        "\n",
        "现在，如果我们能知道条件分布$p(\\mathbf {x}_{t-1} | \\mathbf {x}_t)$，那我们就可以将这个过程倒过来：采样一个随机高斯噪声$\\mathbf x_t$，我们可以对其逐步去噪，最终得到一个真实分布的图片$\\mathbf x_0$。\n",
        "\n",
        "但是我们实际上没办法知道$p(\\mathbf {x}_{t-1} | \\mathbf {x}_t)$。因为它需要知道所有可能图像的分布来计算这个条件概率。因此，我们需要借助神经网络来近似(学习)这个条件概率分布。 也就是$p_\\theta (\\mathbf {x}_{t-1} | \\mathbf {x}_t)$，其中, $\\theta$是神经网络的参数，需要使用梯度下降更新。\n",
        "\n",
        "\n",
        "所以现在我们需要一个神经网络来表示逆向过程的(条件)概率分布。如果我们假设这个反向过程也是高斯分布，那么回想一下，任何高斯分布都是由两个参数定义的:\n",
        "\n",
        "* 一个均值$\\mu_\\theta$;\n",
        "* 一个方差$\\Sigma_\\theta$。\n",
        "\n",
        "所以我们可以把这个过程参数化为\n",
        "\n",
        "$$\n",
        "p_\\theta (\\mathbf{x}_{t-1} | \\mathbf{x}_t) = \\mathcal{N}(\\mathbf{x}_{t-1}; \\mu_\\theta(\\mathbf{x}_{t},t), \\Sigma_\\theta (\\mathbf{x}_{t},t))\n",
        "$$\n",
        "\n",
        "其中均值和方差也取决于噪声水平$t$。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "从上边我们可以知道，逆向过程我们需要一个神经网络来学习（表示）高斯分布的均值和方差。\n",
        "\n",
        "DDPM中作者固定方差，只让神经网络学习条件概率分布的均值。\n",
        "\n",
        "\n",
        "> First, we set $\\Sigma_\\theta ( \\mathbf{x}_t, t) = \\sigma^2_t \\mathbf{I}$ to untrained time dependent constants. Experimentally, both $\\sigma^2_t = \\beta_t$ and $\\sigma^2_t  = \\tilde{\\beta}_t$ (see paper) had similar results. \n",
        "\n",
        "[Improved diffusion models](https://openreview.net/pdf?id=-NEXDKk8gZ)这篇文章中进行了改进，神经网络既需要学习均值也要学习方差。"
      ],
      "metadata": {
        "id": "5wjIt_ftQW6d"
      },
      "id": "5wjIt_ftQW6d"
    },
    {
      "cell_type": "markdown",
      "id": "2d747688",
      "metadata": {
        "id": "2d747688"
      },
      "source": [
        "\n",
        "\n",
        "## 通过重新参数化均值 定义目标函数\n",
        "\n",
        "为了推导出一个目标函数来学习逆向过程的均值，作者观察到$q$和$p_\\theta$可以看做是一个VAE模型 [(Kingma et al., 2013)](https://arxiv.org/abs/1312.6114). \n",
        "\n",
        "因此，变分下界（ELBO）可以用来最小化关于ground truth $\\mathbf x_0$的负对数似然。\n",
        "\n",
        "这个过程的ELBO是每个时间步$t$的损失总和：$L=L_0+L_1+…+L_𝑇$。\n",
        "\n",
        "通过构建正向$q$过程和反向过程，损失的每一项，除了$L_0$，都是两个高斯分布之间的KL散度，并且可以写为关于均值的$L_2$损失!\n",
        "\n",
        "\n",
        "因为高斯分布的特性，我们不需要在正向$q$过程中逐步添加$t$步长的噪声，我们可以直接获得$x_t$的结果：\n",
        "\n",
        "$$\n",
        "q(\\mathbf {x}_t | \\mathbf {x}_0) = \\cal{N}(\\mathbf {x}_t; \\sqrt{\\bar{\\alpha}_t} \\mathbf {x}_0, (1- \\bar{\\alpha}_t) \\mathbf{I})\n",
        "$$\n",
        "\n",
        "其中$\\alpha_t := 1 - \\beta_t$ and $\\bar{\\alpha}_t := \\Pi_{s=1}^{t} \\alpha_s$。\n",
        "\n",
        "这是一个很优秀的特性。这意味着我们可以对高斯噪声进行采样并适当缩放直接将其添加到$\\mathbf x_0$中就可以直接得到$\\mathbf x_t$。\n",
        "\n",
        "$\\bar{\\alpha}_t$是方差表$\\beta_t$的函数，因此也是已知的，我们可以对其预先计算。这样可以让我们在训练期间优化损失函数$L$的随机项（换句话说，在训练期间随机采样$t$就可以优化$L_t$）。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68574c28",
      "metadata": {
        "id": "68574c28"
      },
      "source": [
        "\n",
        "\n",
        "这个属性的另一个优美之处是通过重新参数化平均值，使神经网络学习（预测）添加的噪声。\n",
        "\n",
        "通过神经网络$\\epsilon_\\theta(\\mathbf x_t，t)$预测噪声，可以构成损失函数中时间步$t$的KL项。\n",
        "\n",
        "这意味着我们的神经网络变成了噪声预测器，而不是直接去预测均值了。\n",
        "\n",
        "均值的计算方法如下:\n",
        "\n",
        "$$ \\mathbf{\\mu}_\\theta(\\mathbf{x}_t, t) = \\frac{1}{\\sqrt{\\alpha_t}} \\left(  \\mathbf{x}_t - \\frac{\\beta_t}{\\sqrt{1- \\bar{\\alpha}_t}} \\mathbf{\\epsilon}_\\theta(\\mathbf{x}_t, t) \\right)$$\n",
        "\n",
        "最后的目标函数$L_t$ 长这样，给定随机的时间步 $t$ ，${\\epsilon} \\sim \\mathcal{N}({0}, {I})$ : \n",
        "\n",
        "$$ \\| \\mathbf{\\epsilon} - \\mathbf{\\epsilon}_\\theta(\\mathbf{x}_t, t) \\|^2 = \\| \\mathbf{\\epsilon} - \\mathbf{\\epsilon}_\\theta( \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0 + \\sqrt{(1- \\bar{\\alpha}_t)  } \\mathbf{\\epsilon}, t) \\|^2.$$\n",
        "\n",
        "\n",
        "\n",
        "$\\mathbf x_0$是初始图像，我们看到噪声$t$样本由固定的前向过程给出。$\\epsilon$是在时间步长$t$采样的纯噪声，$\\epsilon_\\theta(\\mathbf x_t，t)$是我们的神经网络。神经网络的优化使用一个简单的均方误差(MSE)计算真实噪声和预测高斯噪声之间的差异。\n",
        "\n",
        "训练算法如下：\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?id=1LJsdkZ3i1J32lmi9ONMqKFg5LMtpSfT4\" width=\"400\" />\n",
        "</p>\n",
        " \n",
        "\n",
        "1. 从未知的真实数据分布$q(\\mathbf x_0)$中随机采样$\\mathbf x_0$，\n",
        "2. 我们在1和$T$之间均匀采不同时间步的噪声，\n",
        "3. 我们从高斯分布采样一些噪声，并在$𝑡$时间步上使用前边定义的优良属性来破坏输入分布，\n",
        "4. 神经网络根据损坏的图像$\\mathbf x_t$进行训练，目的是预测施加在图片上的噪声，也就是基于已知方差表$\\beta_t$作用在$\\mathbf x_0$上的噪声\n",
        "\n",
        "\n",
        "所有这些都是在批量数据上完成的，使用随机梯度下降优化神经网络。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5153024b",
      "metadata": {
        "id": "5153024b"
      },
      "source": [
        "\n",
        "\n",
        "## 神经网络\n",
        "\n",
        "神经网络需要在特定的时间步$t$中输入一个带有噪声的图像，并返回预测的噪声。请注意，预测的噪声是一个张量，其大小与输入图像相同。因此，在技术实现上，网络的输入和输出是相同形状的张量。我们可以使用什么类型的神经网络来实现这个任务？\n",
        "\n",
        "\n",
        "在这里通常使用的是类似于[自编码器](https://en.wikipedia.org/wiki/Autoencoder)的网络，自编码器在编码器和解码器之间有一个所谓的“瓶颈”（bottleneck）层。编码器首先将图像编码为较小的隐藏表示，称为“瓶颈”，然后解码器将该隐藏表示解码回实际图像。这使网络的瓶颈层中可以保留最重要的信息。\n",
        "\n",
        "\n",
        "在体系结构方面，DDPM作者选择了U-Net，由([Ronneberger et al., 2015](https://arxiv.org/abs/1505.04597))提出，当时在医学图像分割方面实现了SOTA。与任何自编码器一样，该网络包括中间的瓶颈，以确保网络学习到最重要的信息。此外，它引入了编码器和解码器之间的残差连接，极大地改善了梯度流动（受[He et al., 2015](https://arxiv.org/abs/1512.03385))ResNet的启发）。\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?id=1_Hej_VTgdUWGsxxIuyZACCGjpbCGIUi6\" width=\"400\" />\n",
        "</p>\n",
        "\n",
        "可以看出，U-Net模型首先对输入进行下采样（即在空间分辨率上使输入更小），然后执行上采样。\n",
        "\n",
        "下面，我们将逐步实现这个网络。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 网络辅助模块\n",
        "\n",
        "首先，我们定义一些辅助函数和类，在实现神经网络时会用到它们。重要的是，我们定义了一个“残差”模块，它将输入简单地加到特定函数的输出中（为特定函数添加了一个残差连接）。\n",
        "\n",
        "我们还为上采样和下采样操作定义了别名。"
      ],
      "metadata": {
        "id": "xsm73idkszVl"
      },
      "id": "xsm73idkszVl"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "68907f5d",
      "metadata": {
        "id": "68907f5d"
      },
      "outputs": [],
      "source": [
        "def exists(x):\n",
        "    return x is not None\n",
        "\n",
        "def default(val, d):\n",
        "    if exists(val):\n",
        "        return val\n",
        "    return d() if isfunction(d) else d\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        return self.fn(x, *args, **kwargs) + x\n",
        "\n",
        "def Upsample(dim):\n",
        "    return nn.ConvTranspose2d(dim, dim, 4, 2, 1)\n",
        "\n",
        "def Downsample(dim):\n",
        "    return nn.Conv2d(dim, dim, 4, 2, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "592aa765",
      "metadata": {
        "id": "592aa765"
      },
      "source": [
        "### 位置嵌入\n",
        "\n",
        "神经网络的参数在不同时间步之间是共享的，因此作者受Transformer([Vaswani et al., 2017](https://arxiv.org/abs/1706.03762))启发，使用正弦位置嵌入来对$t$进行编码。这使得神经网络能够“知道”它正在处理的batch中每个图像的时间步长（噪声水平）。\n",
        "\n",
        "`SinusoidalPositionEmbeddings`模块以形状为`(batch_size, 1)`的张量作为输入，即当前batch中每个图片的时间步（噪声水平），并将其转换为形状为(batch_size，dim)的张量，其中dim是位置嵌入的维度。然后将其添加到每个残差块中。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5ed0757b",
      "metadata": {
        "id": "5ed0757b"
      },
      "outputs": [],
      "source": [
        "class SinusoidalPositionEmbeddings(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, time):\n",
        "        device = time.device\n",
        "        half_dim = self.dim // 2\n",
        "        embeddings = math.log(10000) / (half_dim - 1)\n",
        "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
        "        embeddings = time[:, None] * embeddings[None, :]\n",
        "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
        "        return embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ff47fbb",
      "metadata": {
        "id": "9ff47fbb"
      },
      "source": [
        "###  ResNet/ConvNeXT块\n",
        "\n",
        "接下来，我们定义U-Net模型的核心构建块。\n",
        "\n",
        "DDPM作者使用了Wide ResNet块([Zagoruyko et al., 2016](https://arxiv.org/abs/1605.07146))，但Phil Wang代码中还实现了ConvNeXT块([Liu et al., 2022](https://arxiv.org/abs/2201.03545))。在U-Net架构中，可以任选其一。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2c2d1219",
      "metadata": {
        "id": "2c2d1219"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, dim_out, groups = 8):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv2d(dim, dim_out, 3, padding = 1)\n",
        "        self.norm = nn.GroupNorm(groups, dim_out)\n",
        "        self.act = nn.SiLU()\n",
        "\n",
        "    def forward(self, x, scale_shift = None):\n",
        "        x = self.proj(x)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        if exists(scale_shift):\n",
        "            scale, shift = scale_shift\n",
        "            x = x * (scale + 1) + shift\n",
        "\n",
        "        x = self.act(x)\n",
        "        return x\n",
        "\n",
        "class ResnetBlock(nn.Module):\n",
        "    \"\"\"https://arxiv.org/abs/1512.03385\"\"\"\n",
        "    \n",
        "    def __init__(self, dim, dim_out, *, time_emb_dim=None, groups=8):\n",
        "        super().__init__()\n",
        "        self.mlp = (\n",
        "            nn.Sequential(nn.SiLU(), nn.Linear(time_emb_dim, dim_out))\n",
        "            if exists(time_emb_dim)\n",
        "            else None\n",
        "        )\n",
        "\n",
        "        self.block1 = Block(dim, dim_out, groups=groups)\n",
        "        self.block2 = Block(dim_out, dim_out, groups=groups)\n",
        "        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x, time_emb=None):\n",
        "        h = self.block1(x)\n",
        "\n",
        "        if exists(self.mlp) and exists(time_emb):\n",
        "            time_emb = self.mlp(time_emb)\n",
        "            h = rearrange(time_emb, \"b c -> b c 1 1\") + h\n",
        "\n",
        "        h = self.block2(h)\n",
        "        return h + self.res_conv(x)\n",
        "    \n",
        "class ConvNextBlock(nn.Module):\n",
        "    \"\"\"https://arxiv.org/abs/2201.03545\"\"\"\n",
        "\n",
        "    def __init__(self, dim, dim_out, *, time_emb_dim=None, mult=2, norm=True):\n",
        "        super().__init__()\n",
        "        self.mlp = (\n",
        "            nn.Sequential(nn.GELU(), nn.Linear(time_emb_dim, dim))\n",
        "            if exists(time_emb_dim)\n",
        "            else None\n",
        "        )\n",
        "\n",
        "        self.ds_conv = nn.Conv2d(dim, dim, 7, padding=3, groups=dim)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.GroupNorm(1, dim) if norm else nn.Identity(),\n",
        "            nn.Conv2d(dim, dim_out * mult, 3, padding=1),\n",
        "            nn.GELU(),\n",
        "            nn.GroupNorm(1, dim_out * mult),\n",
        "            nn.Conv2d(dim_out * mult, dim_out, 3, padding=1),\n",
        "        )\n",
        "\n",
        "        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x, time_emb=None):\n",
        "        h = self.ds_conv(x)\n",
        "\n",
        "        if exists(self.mlp) and exists(time_emb):\n",
        "            assert exists(time_emb), \"time embedding must be passed in\"\n",
        "            condition = self.mlp(time_emb)\n",
        "            h = h + rearrange(condition, \"b c -> b c 1 1\")\n",
        "\n",
        "        h = self.net(h)\n",
        "        return h + self.res_conv(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51d9a24c",
      "metadata": {
        "id": "51d9a24c"
      },
      "source": [
        "### Attention 模块\n",
        "\n",
        "接下来，我们定义注意力模块，该模块由DDPM作者添加在卷积块之间。注意力是Transformer([Vaswani et al., 2017](https://arxiv.org/abs/1706.03762))的构建块，在AI领域，从NLP和CV到蛋白质折叠，都取得了巨大的成功。Phil Wang实现了2种注意力变体：一种是常规的多头自注意力（和Transformer中的一样），另一种是[线性注意力变体](https://github.com/lucidrains/linear-attention-transformer)（[Shen et al., 2018](https://arxiv.org/abs/1812.01243)），其时间和内存需求随序列长度线性缩放，节省计算资源。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "07bbd544",
      "metadata": {
        "id": "07bbd544"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads=4, dim_head=32):\n",
        "        super().__init__()\n",
        "        self.scale = dim_head**-0.5\n",
        "        self.heads = heads\n",
        "        hidden_dim = dim_head * heads\n",
        "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
        "        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
        "        q, k, v = map(\n",
        "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
        "        )\n",
        "        q = q * self.scale\n",
        "\n",
        "        sim = einsum(\"b h d i, b h d j -> b h i j\", q, k)\n",
        "        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n",
        "        attn = sim.softmax(dim=-1)\n",
        "\n",
        "        out = einsum(\"b h i j, b h d j -> b h i d\", attn, v)\n",
        "        out = rearrange(out, \"b h (x y) d -> b (h d) x y\", x=h, y=w)\n",
        "        return self.to_out(out)\n",
        "\n",
        "class LinearAttention(nn.Module):\n",
        "    def __init__(self, dim, heads=4, dim_head=32):\n",
        "        super().__init__()\n",
        "        self.scale = dim_head**-0.5\n",
        "        self.heads = heads\n",
        "        hidden_dim = dim_head * heads\n",
        "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
        "\n",
        "        self.to_out = nn.Sequential(nn.Conv2d(hidden_dim, dim, 1), \n",
        "                                    nn.GroupNorm(1, dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
        "        q, k, v = map(\n",
        "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
        "        )\n",
        "\n",
        "        q = q.softmax(dim=-2)\n",
        "        k = k.softmax(dim=-1)\n",
        "\n",
        "        q = q * self.scale\n",
        "        context = torch.einsum(\"b h d n, b h e n -> b h d e\", k, v)\n",
        "\n",
        "        out = torch.einsum(\"b h d e, b h d n -> b h e n\", context, q)\n",
        "        out = rearrange(out, \"b h c (x y) -> b (h c) x y\", h=self.heads, x=h, y=w)\n",
        "        return self.to_out(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a8031b0",
      "metadata": {
        "id": "9a8031b0"
      },
      "source": [
        "### Group normalization\n",
        "\n",
        "DDPM 的作者在 U-Net 的卷积/注意力层之间交替使用了群组归一化([Wu et al., 2018](https://arxiv.org/abs/1803.08494))。下面，我们定义一个`PreNorm`类，它将用于在注意力层之前实现群组归一化。\n",
        "\n",
        "请注意，在 Transformer 中，是否在注意力之前或之后使用归一化存在争论，具体可以看：[Transformers without Tears](https://tnq177.github.io/data/transformers_without_tears.pdf)。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "5e2ce68f",
      "metadata": {
        "id": "5e2ce68f"
      },
      "outputs": [],
      "source": [
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        self.norm = nn.GroupNorm(1, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.norm(x)\n",
        "        return self.fn(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06b3fad0",
      "metadata": {
        "id": "06b3fad0"
      },
      "source": [
        "### Conditional U-Net\n",
        "\n",
        "有了所有构建块（位置嵌入、ResNet/ConvNeXT块、注意力和组归一化）的定义，现在定义整个神经网络。\n",
        "\n",
        "回想一下，网络 $\\mathbf{\\epsilon}_\\theta(\\mathbf{x}_t, t)$ 的任务是接收一批有噪声的图像和噪声水平，并输出加入噪声后的图像。更正式地说：\n",
        "\n",
        "网络接收一个形状为`(batch_size, num_channels, height, width)`的有噪声图像批次和一个形状为`(batch_size, 1)`的噪声水平批次作为输入，并返回一个形状为`(batch_size, num_channels, height, width)`的张量。\n",
        "\n",
        "网络的构建如下：\n",
        "\n",
        "首先，在有噪声图像的批次上应用卷积层，并为噪声水平计算位置嵌入。\n",
        "接下来，应用一系列的下采样阶段。每个下采样阶段包含2个ResNet/ConvNeXT块+组归一化+注意力+残差连接+下采样操作。\n",
        "在网络的中心，再次应用ResNet或ConvNeXT块，交替使用注意力。\n",
        "接下来，应用一系列的上采样阶段。每个上采样阶段包含2个ResNet/ConvNeXT块+组归一化+注意力+残差连接+上采样操作。\n",
        "最后，应用一个ResNet/ConvNeXT块，然后是一个卷积层。\n",
        "最终，神经网络堆叠层就像积木块一样（但理解它们的工作方式很重要）。\n",
        "\n",
        "Now that we've defined all building blocks (position embeddings, ResNet/ConvNeXT blocks, attention and group normalization), it's time to define the entire neural network. Recall that the job of the network \\\\(\\mathbf{\\epsilon}_\\theta(\\mathbf{x}_t, t)\\\\) is to take in a batch of noisy images + noise levels, and output the noise added to the input. More formally:\n",
        "\n",
        "- the network takes a batch of noisy images of shape `(batch_size, num_channels, height, width)` and a batch of noise levels of shape `(batch_size, 1)` as input, and returns a tensor of shape `(batch_size, num_channels, height, width)`\n",
        "\n",
        "The network is built up as follows:\n",
        "* first, a convolutional layer is applied on the batch of noisy images, and position embeddings are computed for the noise levels\n",
        "* next, a sequence of downsampling stages are applied. Each downsampling stage consists of 2 ResNet/ConvNeXT blocks + groupnorm + attention + residual connection + a downsample operation\n",
        "* at the middle of the network, again ResNet or ConvNeXT blocks are applied, interleaved with attention\n",
        "* next, a sequence of upsampling stages are applied. Each upsampling stage consists of 2 ResNet/ConvNeXT blocks + groupnorm + attention + residual connection + an upsample operation\n",
        "* finally, a ResNet/ConvNeXT block followed by a convolutional layer is applied.\n",
        "\n",
        "Ultimately, neural networks stack up layers as if they were lego blocks (but it's important to [understand how they work](http://karpathy.github.io/2019/04/25/recipe/)).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "3a159023",
      "metadata": {
        "id": "3a159023"
      },
      "outputs": [],
      "source": [
        "class Unet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        init_dim=None,\n",
        "        out_dim=None,\n",
        "        dim_mults=(1, 2, 4, 8),\n",
        "        channels=3,\n",
        "        with_time_emb=True,\n",
        "        resnet_block_groups=8,\n",
        "        use_convnext=True,\n",
        "        convnext_mult=2,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # determine dimensions\n",
        "        self.channels = channels\n",
        "\n",
        "        init_dim = default(init_dim, dim // 3 * 2)\n",
        "        self.init_conv = nn.Conv2d(channels, init_dim, 7, padding=3)\n",
        "\n",
        "        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n",
        "        in_out = list(zip(dims[:-1], dims[1:]))\n",
        "        \n",
        "        if use_convnext:\n",
        "            block_klass = partial(ConvNextBlock, mult=convnext_mult)\n",
        "        else:\n",
        "            block_klass = partial(ResnetBlock, groups=resnet_block_groups)\n",
        "\n",
        "        # time embeddings\n",
        "        if with_time_emb:\n",
        "            time_dim = dim * 4\n",
        "            self.time_mlp = nn.Sequential(\n",
        "                SinusoidalPositionEmbeddings(dim),\n",
        "                nn.Linear(dim, time_dim),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(time_dim, time_dim),\n",
        "            )\n",
        "        else:\n",
        "            time_dim = None\n",
        "            self.time_mlp = None\n",
        "\n",
        "        # layers\n",
        "        self.downs = nn.ModuleList([])\n",
        "        self.ups = nn.ModuleList([])\n",
        "        num_resolutions = len(in_out)\n",
        "\n",
        "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
        "            is_last = ind >= (num_resolutions - 1)\n",
        "\n",
        "            self.downs.append(\n",
        "                nn.ModuleList(\n",
        "                    [\n",
        "                        block_klass(dim_in, dim_out, time_emb_dim=time_dim),\n",
        "                        block_klass(dim_out, dim_out, time_emb_dim=time_dim),\n",
        "                        Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n",
        "                        Downsample(dim_out) if not is_last else nn.Identity(),\n",
        "                    ]\n",
        "                )\n",
        "            )\n",
        "\n",
        "        mid_dim = dims[-1]\n",
        "        self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
        "        self.mid_attn = Residual(PreNorm(mid_dim, Attention(mid_dim)))\n",
        "        self.mid_block2 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
        "\n",
        "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n",
        "            is_last = ind >= (num_resolutions - 1)\n",
        "\n",
        "            self.ups.append(\n",
        "                nn.ModuleList(\n",
        "                    [\n",
        "                        block_klass(dim_out * 2, dim_in, time_emb_dim=time_dim),\n",
        "                        block_klass(dim_in, dim_in, time_emb_dim=time_dim),\n",
        "                        Residual(PreNorm(dim_in, LinearAttention(dim_in))),\n",
        "                        Upsample(dim_in) if not is_last else nn.Identity(),\n",
        "                    ]\n",
        "                )\n",
        "            )\n",
        "\n",
        "        out_dim = default(out_dim, channels)\n",
        "        self.final_conv = nn.Sequential(\n",
        "            block_klass(dim, dim), nn.Conv2d(dim, out_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, time):\n",
        "        x = self.init_conv(x)\n",
        "\n",
        "        t = self.time_mlp(time) if exists(self.time_mlp) else None\n",
        "\n",
        "        h = []\n",
        "\n",
        "        # downsample\n",
        "        for block1, block2, attn, downsample in self.downs:\n",
        "            x = block1(x, t)\n",
        "            x = block2(x, t)\n",
        "            x = attn(x)\n",
        "            h.append(x)\n",
        "            x = downsample(x)\n",
        "\n",
        "        # bottleneck\n",
        "        x = self.mid_block1(x, t)\n",
        "        x = self.mid_attn(x)\n",
        "        x = self.mid_block2(x, t)\n",
        "\n",
        "        # upsample\n",
        "        for block1, block2, attn, upsample in self.ups:\n",
        "            x = torch.cat((x, h.pop()), dim=1)\n",
        "            x = block1(x, t)\n",
        "            x = block2(x, t)\n",
        "            x = attn(x)\n",
        "            x = upsample(x)\n",
        "\n",
        "        return self.final_conv(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a30368b2",
      "metadata": {
        "id": "a30368b2"
      },
      "source": [
        "## Defining the forward diffusion process\n",
        "\n",
        "The forward diffusion process gradually adds noise to an image from the real distribution, in a number of time steps $T$. This happens according to a **variance schedule**. The original DDPM authors employed a linear schedule:\n",
        "\n",
        "> We set the forward process variances to constants\n",
        "increasing linearly from $\\beta_1 = 10^{−4}$\n",
        "to $\\beta_T = 0.02$.\n",
        "\n",
        "However, it was shown in ([Nichol et al., 2021](https://arxiv.org/abs/2102.09672)) that better results can be achieved when employing a cosine schedule. \n",
        "\n",
        "Below, we define various schedules for the $T$ timesteps, as well as corresponding variables which we'll need, such as cumulative variances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "5d751df2",
      "metadata": {
        "id": "5d751df2"
      },
      "outputs": [],
      "source": [
        "def cosine_beta_schedule(timesteps, s=0.008):\n",
        "    \"\"\"\n",
        "    cosine schedule as proposed in https://arxiv.org/abs/2102.09672\n",
        "    \"\"\"\n",
        "    steps = timesteps + 1\n",
        "    x = torch.linspace(0, timesteps, steps)\n",
        "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
        "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
        "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
        "    return torch.clip(betas, 0.0001, 0.9999)\n",
        "\n",
        "def linear_beta_schedule(timesteps):\n",
        "    beta_start = 0.0001\n",
        "    beta_end = 0.02\n",
        "    return torch.linspace(beta_start, beta_end, timesteps)\n",
        "\n",
        "def quadratic_beta_schedule(timesteps):\n",
        "    beta_start = 0.0001\n",
        "    beta_end = 0.02\n",
        "    return torch.linspace(beta_start**0.5, beta_end**0.5, timesteps) ** 2\n",
        "\n",
        "def sigmoid_beta_schedule(timesteps):\n",
        "    beta_start = 0.0001\n",
        "    beta_end = 0.02\n",
        "    betas = torch.linspace(-6, 6, timesteps)\n",
        "    return torch.sigmoid(betas) * (beta_end - beta_start) + beta_start"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bfc3841",
      "metadata": {
        "id": "6bfc3841"
      },
      "source": [
        "To start with, let's use the linear schedule for \\\\(T=200\\\\) time steps and define the various variables from the \\\\(\\beta_t\\\\) which we will need, such as the cumulative product of the variances \\\\(\\bar{\\alpha}_t\\\\). Each of the variables below are just 1-dimensional tensors, storing values from \\\\(t\\\\) to \\\\(T\\\\). Importantly, we also define an `extract` function, which will allow us to extract the appropriate \\\\(t\\\\) index for a batch of indices.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc57b01f",
      "metadata": {
        "id": "cc57b01f"
      },
      "outputs": [],
      "source": [
        "timesteps = 200\n",
        "\n",
        "# define beta schedule\n",
        "betas = linear_beta_schedule(timesteps=timesteps)\n",
        "\n",
        "# define alphas \n",
        "alphas = 1. - betas\n",
        "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
        "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
        "sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
        "\n",
        "# calculations for diffusion q(x_t | x_{t-1}) and others\n",
        "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
        "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n",
        "\n",
        "# calculations for posterior q(x_{t-1} | x_t, x_0)\n",
        "posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
        "\n",
        "def extract(a, t, x_shape):\n",
        "    batch_size = t.shape[0]\n",
        "    out = a.gather(-1, t.cpu())\n",
        "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48f8a004",
      "metadata": {
        "id": "48f8a004"
      },
      "source": [
        "We'll illustrate with a cats image how noise is added at each time step of the diffusion process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9f13b16",
      "metadata": {
        "id": "c9f13b16",
        "lines_to_next_cell": 0
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bde062f",
      "metadata": {
        "id": "4bde062f"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=17FXnvCTl96lDhqZ_io54guXO8hM-rsQ2\" width=\"400\" />\n",
        "\n",
        "Noise is added to PyTorch tensors, rather than Pillow Images. We'll first define image transformations that allow us to go from a PIL image to a PyTorch tensor (on which we can add the noise), and vice versa.\n",
        "\n",
        "These transformations are fairly simple: we first normalize images by dividing by $255$ (such that they are in the $[0,1]$ range), and then make sure they are in the $[-1, 1]$ range. From the DPPM paper:\n",
        "\n",
        "> We assume that image data consists of integers in $\\{0, 1, ... , 255\\}$ scaled linearly to $[−1, 1]$. This\n",
        "ensures that the neural network reverse process operates on consistently scaled inputs starting from\n",
        "the standard normal prior $p(\\mathbf{x}_T )$. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71aba861",
      "metadata": {
        "id": "71aba861"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import Compose, ToTensor, Lambda, ToPILImage, CenterCrop, Resize\n",
        "\n",
        "image_size = 128\n",
        "transform = Compose([\n",
        "    Resize(image_size),\n",
        "    CenterCrop(image_size),\n",
        "    ToTensor(), # turn into Numpy array of shape HWC, divide by 255\n",
        "    Lambda(lambda t: (t * 2) - 1),\n",
        "    \n",
        "])\n",
        "\n",
        "x_start = transform(image).unsqueeze(0)\n",
        "x_start.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e143cc62",
      "metadata": {
        "id": "e143cc62"
      },
      "source": [
        "<div class=\"output stream stdout\">\n",
        "\n",
        "    Output:\n",
        "    ----------------------------------------------------------------------------------------------------\n",
        "    torch.Size([1, 3, 128, 128])\n",
        "\n",
        "</div>\n",
        "\n",
        "We also define the reverse transform, which takes in a PyTorch tensor containing values in $[-1, 1]$ and turn them back into a PIL image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b98e91ff",
      "metadata": {
        "id": "b98e91ff"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "reverse_transform = Compose([\n",
        "     Lambda(lambda t: (t + 1) / 2),\n",
        "     Lambda(lambda t: t.permute(1, 2, 0)), # CHW to HWC\n",
        "     Lambda(lambda t: t * 255.),\n",
        "     Lambda(lambda t: t.numpy().astype(np.uint8)),\n",
        "     ToPILImage(),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6869ab7",
      "metadata": {
        "id": "f6869ab7"
      },
      "source": [
        "Let's verify this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "366a770c",
      "metadata": {
        "id": "366a770c"
      },
      "outputs": [],
      "source": [
        "reverse_transform(x_start.squeeze())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f38d30ab",
      "metadata": {
        "id": "f38d30ab"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1WT22KYvqJbHFdYYfkV7ohKNO4alnvesB\" width=\"100\" />\n",
        "\n",
        "We can now define the forward diffusion process as in the paper:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3752480",
      "metadata": {
        "id": "f3752480"
      },
      "outputs": [],
      "source": [
        "# forward diffusion\n",
        "def q_sample(x_start, t, noise=None):\n",
        "    if noise is None:\n",
        "        noise = torch.randn_like(x_start)\n",
        "\n",
        "    sqrt_alphas_cumprod_t = extract(sqrt_alphas_cumprod, t, x_start.shape)\n",
        "    sqrt_one_minus_alphas_cumprod_t = extract(\n",
        "        sqrt_one_minus_alphas_cumprod, t, x_start.shape\n",
        "    )\n",
        "\n",
        "    return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e82bac28",
      "metadata": {
        "id": "e82bac28"
      },
      "source": [
        "Let's test it on a particular time step:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bd64f89",
      "metadata": {
        "id": "6bd64f89"
      },
      "outputs": [],
      "source": [
        "def get_noisy_image(x_start, t):\n",
        "  # add noise\n",
        "  x_noisy = q_sample(x_start, t=t)\n",
        "\n",
        "  # turn back into PIL image\n",
        "  noisy_image = reverse_transform(x_noisy.squeeze())\n",
        "\n",
        "  return noisy_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52d22667",
      "metadata": {
        "id": "52d22667"
      },
      "outputs": [],
      "source": [
        "# take time step\n",
        "t = torch.tensor([40])\n",
        "\n",
        "get_noisy_image(x_start, t)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "003e1d95",
      "metadata": {
        "id": "003e1d95"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1Ra33wxuw3QxPlUG0iqZGtxgKBNdjNsqz\" width=\"100\" />\n",
        "\n",
        "Let's visualize this for various time steps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e27c37d",
      "metadata": {
        "id": "8e27c37d"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# use seed for reproducability\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# source: https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py\n",
        "def plot(imgs, with_orig=False, row_title=None, **imshow_kwargs):\n",
        "    if not isinstance(imgs[0], list):\n",
        "        # Make a 2d grid even if there's just 1 row\n",
        "        imgs = [imgs]\n",
        "\n",
        "    num_rows = len(imgs)\n",
        "    num_cols = len(imgs[0]) + with_orig\n",
        "    fig, axs = plt.subplots(figsize=(200,200), nrows=num_rows, ncols=num_cols, squeeze=False)\n",
        "    for row_idx, row in enumerate(imgs):\n",
        "        row = [image] + row if with_orig else row\n",
        "        for col_idx, img in enumerate(row):\n",
        "            ax = axs[row_idx, col_idx]\n",
        "            ax.imshow(np.asarray(img), **imshow_kwargs)\n",
        "            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
        "\n",
        "    if with_orig:\n",
        "        axs[0, 0].set(title='Original image')\n",
        "        axs[0, 0].title.set_size(8)\n",
        "    if row_title is not None:\n",
        "        for row_idx in range(num_rows):\n",
        "            axs[row_idx, 0].set(ylabel=row_title[row_idx])\n",
        "\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "323102d0",
      "metadata": {
        "id": "323102d0"
      },
      "outputs": [],
      "source": [
        "plot([get_noisy_image(x_start, torch.tensor([t])) for t in [0, 50, 100, 150, 199]])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a205c24",
      "metadata": {
        "id": "4a205c24"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1QifsBnYiijwTqru6gur9C0qKkFYrm-lN\" width=\"800\" />\n",
        "    \n",
        "This means that we can now define the loss function given the model as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7725f6cf",
      "metadata": {
        "id": "7725f6cf"
      },
      "outputs": [],
      "source": [
        "def p_losses(denoise_model, x_start, t, noise=None, loss_type=\"l1\"):\n",
        "    if noise is None:\n",
        "        noise = torch.randn_like(x_start)\n",
        "\n",
        "    x_noisy = q_sample(x_start=x_start, t=t, noise=noise)\n",
        "    predicted_noise = denoise_model(x_noisy, t)\n",
        "\n",
        "    if loss_type == 'l1':\n",
        "        loss = F.l1_loss(noise, predicted_noise)\n",
        "    elif loss_type == 'l2':\n",
        "        loss = F.mse_loss(noise, predicted_noise)\n",
        "    elif loss_type == \"huber\":\n",
        "        loss = F.smooth_l1_loss(noise, predicted_noise)\n",
        "    else:\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc01c63b",
      "metadata": {
        "id": "cc01c63b"
      },
      "source": [
        "The `denoise_model` will be our U-Net defined above. We'll employ the Huber loss between the true and the predicted noise.\n",
        "\n",
        "## Define a PyTorch Dataset + DataLoader\n",
        "\n",
        "Here we define a regular [PyTorch Dataset](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html). The dataset simply consists of images from a real dataset, like Fashion-MNIST, CIFAR-10 or ImageNet, scaled linearly to \\\\([−1, 1]\\\\).\n",
        "\n",
        "Each image is resized to the same size. Interesting to note is that images are also randomly horizontally flipped. From the paper:\n",
        "\n",
        "> We used random horizontal flips during training for CIFAR10; we tried training both with and without flips, and found flips to improve sample quality slightly.\n",
        "\n",
        "Here we use the 🤗 [Datasets library](https://huggingface.co/docs/datasets/index) to easily load the Fashion MNIST dataset from the [hub](https://huggingface.co/datasets/fashion_mnist). This dataset consists of images which already have the same resolution, namely 28x28."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6134d691",
      "metadata": {
        "id": "6134d691"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# load dataset from the hub\n",
        "dataset = load_dataset(\"fashion_mnist\")\n",
        "image_size = 28\n",
        "channels = 1\n",
        "batch_size = 128"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db6f5875",
      "metadata": {
        "id": "db6f5875"
      },
      "source": [
        "Next, we define a function which we'll apply on-the-fly on the entire dataset. We use the `with_transform` [functionality](https://huggingface.co/docs/datasets/v2.2.1/en/package_reference/main_classes#datasets.Dataset.with_transform) for that. The function just applies some basic image preprocessing: random horizontal flips, rescaling and finally make them have values in the $[-1,1]$ range."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3e78945",
      "metadata": {
        "id": "b3e78945"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# define image transformations (e.g. using torchvision)\n",
        "transform = Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Lambda(lambda t: (t * 2) - 1)\n",
        "])\n",
        "\n",
        "# define function\n",
        "def transforms(examples):\n",
        "   examples[\"pixel_values\"] = [transform(image.convert(\"L\")) for image in examples[\"image\"]]\n",
        "   del examples[\"image\"]\n",
        "\n",
        "   return examples\n",
        "\n",
        "transformed_dataset = dataset.with_transform(transforms).remove_columns(\"label\")\n",
        "\n",
        "# create dataloader\n",
        "dataloader = DataLoader(transformed_dataset[\"train\"], batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52e8273b",
      "metadata": {
        "id": "52e8273b"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(dataloader))\n",
        "print(batch.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4325faf",
      "metadata": {
        "id": "e4325faf"
      },
      "source": [
        "<div class=\"output stream stdout\">\n",
        "\n",
        "    Output:\n",
        "    ----------------------------------------------------------------------------------------------------\n",
        "    dict_keys(['pixel_values'])\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cf98443",
      "metadata": {
        "id": "4cf98443"
      },
      "source": [
        "## Sampling\n",
        "\n",
        "As we'll sample from the model during training (in order to track progress), we define the code for that below. Sampling is summarized in the paper as Algorithm 2:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1ij80f8TNBDzpKtqHjk_sh8o5aby3lmD7\" width=\"500\" />\n",
        "\n",
        "Generating new images from a diffusion model happens by reversing the diffusion process: we start from $T$, where we sample pure noise from a Gaussian distribution, and then use our neural network to gradually denoise it (using the conditional probability it has learned), until we end up at time step $t = 0$. As shown above, we can derive a slighly less denoised image $\\mathbf{x}_{t-1 }$ by plugging in the reparametrization of the mean, using our noise predictor. Remember that the variance is known ahead of time.\n",
        "\n",
        "Ideally, we end up with an image that looks like it came from the real data distribution.\n",
        "\n",
        "The code below implements this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7628fb3",
      "metadata": {
        "id": "f7628fb3"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def p_sample(model, x, t, t_index):\n",
        "    betas_t = extract(betas, t, x.shape)\n",
        "    sqrt_one_minus_alphas_cumprod_t = extract(\n",
        "        sqrt_one_minus_alphas_cumprod, t, x.shape\n",
        "    )\n",
        "    sqrt_recip_alphas_t = extract(sqrt_recip_alphas, t, x.shape)\n",
        "    \n",
        "    # Equation 11 in the paper\n",
        "    # Use our model (noise predictor) to predict the mean\n",
        "    model_mean = sqrt_recip_alphas_t * (\n",
        "        x - betas_t * model(x, t) / sqrt_one_minus_alphas_cumprod_t\n",
        "    )\n",
        "\n",
        "    if t_index == 0:\n",
        "        return model_mean\n",
        "    else:\n",
        "        posterior_variance_t = extract(posterior_variance, t, x.shape)\n",
        "        noise = torch.randn_like(x)\n",
        "        # Algorithm 2 line 4:\n",
        "        return model_mean + torch.sqrt(posterior_variance_t) * noise \n",
        "\n",
        "# Algorithm 2 but save all images:\n",
        "@torch.no_grad()\n",
        "def p_sample_loop(model, shape):\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    b = shape[0]\n",
        "    # start from pure noise (for each example in the batch)\n",
        "    img = torch.randn(shape, device=device)\n",
        "    imgs = []\n",
        "    \n",
        "    for i in tqdm(reversed(range(0, timesteps)), desc='sampling loop time step', total=timesteps):\n",
        "        img = p_sample(model, img, torch.full((b,), i, device=device, dtype=torch.long), i)\n",
        "        imgs.append(img.cpu().numpy())\n",
        "    return imgs\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample(model, image_size, batch_size=16, channels=3):\n",
        "    return p_sample_loop(model, shape=(batch_size, channels, image_size, image_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f70235f8",
      "metadata": {
        "id": "f70235f8"
      },
      "source": [
        "\n",
        "Note that the code above is a simplified version of the original implementation. We found our simplification (which is in line with Algorithm 2 in the paper) to work just as well as the [original, more complex implementation](https://github.com/hojonathanho/diffusion/blob/master/diffusion_tf/diffusion_utils.py).\n",
        "\n",
        "\n",
        "## 训练\n",
        "\n",
        "Next, we train the model in regular PyTorch fashion. We also define some logic to peridiocally save generated images, using the `sample` method defined above.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c1ad663",
      "metadata": {
        "id": "0c1ad663"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def num_to_groups(num, divisor):\n",
        "    groups = num // divisor\n",
        "    remainder = num % divisor\n",
        "    arr = [divisor] * groups\n",
        "    if remainder > 0:\n",
        "        arr.append(remainder)\n",
        "    return arr\n",
        "\n",
        "results_folder = Path(\"./results\")\n",
        "results_folder.mkdir(exist_ok = True)\n",
        "save_and_sample_every = 1000"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22e4c0fd",
      "metadata": {
        "id": "22e4c0fd"
      },
      "source": [
        "\n",
        "定义好模型，将其丢到GPU上，使用Adam进行优化。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5126e21",
      "metadata": {
        "id": "a5126e21"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = Unet(\n",
        "    dim=image_size,\n",
        "    channels=channels,\n",
        "    dim_mults=(1, 2, 4,)\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7444b0b",
      "metadata": {
        "id": "f7444b0b"
      },
      "source": [
        "开始训练"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92b12ed1",
      "metadata": {
        "id": "92b12ed1"
      },
      "outputs": [],
      "source": [
        "from torchvision.utils import save_image\n",
        "\n",
        "epochs = 5\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for step, batch in enumerate(dataloader):\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      batch_size = batch[\"pixel_values\"].shape[0]\n",
        "      batch = batch[\"pixel_values\"].to(device)\n",
        "\n",
        "      # Algorithm 1 line 3: sample t uniformally for every example in the batch\n",
        "      t = torch.randint(0, timesteps, (batch_size,), device=device).long()\n",
        "\n",
        "      loss = p_losses(model, batch, t, loss_type=\"huber\")\n",
        "\n",
        "      if step % 100 == 0:\n",
        "        print(\"Loss:\", loss.item())\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # save generated images\n",
        "      if step != 0 and step % save_and_sample_every == 0:\n",
        "        milestone = step // save_and_sample_every\n",
        "        batches = num_to_groups(4, batch_size)\n",
        "        all_images_list = list(map(lambda n: sample(model, batch_size=n, channels=channels), batches))\n",
        "        all_images = torch.cat(all_images_list, dim=0)\n",
        "        all_images = (all_images + 1) * 0.5\n",
        "        save_image(all_images, str(results_folder / f'sample-{milestone}.png'), nrow = 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e617a66a",
      "metadata": {
        "id": "e617a66a"
      },
      "source": [
        "<div class=\"output stream stdout\">\n",
        "\n",
        "    Output:\n",
        "    ----------------------------------------------------------------------------------------------------\n",
        "    Loss: 0.46477368474006653\n",
        "    Loss: 0.12143351882696152\n",
        "    Loss: 0.08106148988008499\n",
        "    Loss: 0.0801810547709465\n",
        "    Loss: 0.06122320517897606\n",
        "    Loss: 0.06310459971427917\n",
        "    Loss: 0.05681884288787842\n",
        "    Loss: 0.05729678273200989\n",
        "    Loss: 0.05497899278998375\n",
        "    Loss: 0.04439849033951759\n",
        "    Loss: 0.05415581166744232\n",
        "    Loss: 0.06020551547408104\n",
        "    Loss: 0.046830907464027405\n",
        "    Loss: 0.051029372960329056\n",
        "    Loss: 0.0478244312107563\n",
        "    Loss: 0.046767622232437134\n",
        "    Loss: 0.04305662214756012\n",
        "    Loss: 0.05216279625892639\n",
        "    Loss: 0.04748568311333656\n",
        "    Loss: 0.05107741802930832\n",
        "    Loss: 0.04588869959115982\n",
        "    Loss: 0.043014321476221085\n",
        "    Loss: 0.046371955424547195\n",
        "    Loss: 0.04952816292643547\n",
        "    Loss: 0.04472338408231735\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8337c82",
      "metadata": {
        "id": "a8337c82"
      },
      "source": [
        "## 采样/推理\n",
        "\n",
        "To sample from the model, we can just use our sample function defined above:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3d8a814",
      "metadata": {
        "id": "f3d8a814"
      },
      "outputs": [],
      "source": [
        "# sample 64 images\n",
        "samples = sample(model, image_size=image_size, batch_size=64, channels=channels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_s-Al2lJ2c8T",
      "metadata": {
        "id": "_s-Al2lJ2c8T"
      },
      "outputs": [],
      "source": [
        "# show a random one\n",
        "random_index = 5\n",
        "plt.imshow(samples[-1][random_index].reshape(image_size, image_size, channels), cmap=\"gray\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26ad579f",
      "metadata": {
        "id": "26ad579f"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1ytnzS7IW7ortC6ub85q7nud1IvXe2QTE\" width=\"300\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0k4H1fmlKvzR",
      "metadata": {
        "id": "0k4H1fmlKvzR"
      },
      "source": [
        "Seems like the model is capable of generating a nice T-shirt! Keep in mind that the dataset we trained on is pretty low-resolution (28x28).\n",
        "\n",
        "We can also create a gif of the denoising process:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "spE1I9aVNwzZ",
      "metadata": {
        "id": "spE1I9aVNwzZ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.animation as animation\n",
        "\n",
        "random_index = 53\n",
        "\n",
        "fig = plt.figure()\n",
        "ims = []\n",
        "for i in range(timesteps):\n",
        "    im = plt.imshow(samples[i][random_index].reshape(image_size, image_size, channels), cmap=\"gray\", animated=True)\n",
        "    ims.append([im])\n",
        "\n",
        "animate = animation.ArtistAnimation(fig, ims, interval=50, blit=True, repeat_delay=1000)\n",
        "animate.save('diffusion.gif')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b02eb802",
      "metadata": {
        "id": "b02eb802"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1eyonQWhfmbQsTq8ndsNjw5QSRQ9em9Au\" width=\"500\" />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 后续阅读📕\n",
        "\n",
        "- Improved Denoising Diffusion Probabilistic Models ([Nichol et al., 2021](https://arxiv.org/abs/2102.09672)): 学习条件分布的方差（除均值外）有助于提高性能。\n",
        "\n",
        "- Cascaded Diffusion Models for High Fidelity Image Generation ([Ho et al., 2021](https://arxiv.org/abs/2106.15282)): 引入级联扩散，包括多个扩散模型的pipeline，用于生成逐步提高分辨率的图像，实现高保真度图像合成。\n",
        "\n",
        "- Diffusion Models Beat GANs on Image Synthesis ([Dhariwal et al., 2021](https://arxiv.org/abs/2105.05233)): 通过改进U-Net架构，并引入分类器指导，证明了扩散模型可以实现比当前最先进的生成模型更好的图像样本质量。\n",
        "\n",
        "- Classifier-Free Diffusion Guidance ([Ho et al., 2021](https://openreview.net/pdf?id=qw8AKxfYbI)): 通过联合训练一个条件和一个无条件扩散模型的单个神经网络，展示了指导扩散模型不需要分类器。\n",
        "\n",
        "- Hierarchical Text-Conditional Image Generation with CLIP Latents (DALL-E 2) ([Ramesh et al., 2022](https://cdn.openai.com/papers/dall-e-2.pdf)): 使用先验将文本标题转化为CLIP图像嵌入，然后扩散模型将其解码成图像。\n",
        "\n",
        "- Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding (ImageGen) ([Saharia et al., 2022](https://arxiv.org/abs/2205.11487)): 将大型预训练语言模型（例如T5）与级联扩散相结合，可以很好地实现文本到图像的合成。\n",
        "\n",
        "\n",
        "目前，扩散模型的主要（或许唯一的）缺点似乎是需要多次前向传递才能生成一张图像（而对于GAN等生成模型则不需要）。然而，[Zhang et al., 2022](https://arxiv.org/abs/2204.13902)可以在不到10个去噪步骤内实现高保真度的生成。"
      ],
      "metadata": {
        "id": "vTBA0onvwwil"
      },
      "id": "vTBA0onvwwil"
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "6fe49a34",
        "2d747688",
        "5153024b",
        "592aa765",
        "9ff47fbb",
        "51d9a24c",
        "9a8031b0",
        "06b3fad0",
        "a30368b2",
        "cc01c63b",
        "f70235f8",
        "b02eb802"
      ],
      "name": "annotated-diffusion.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}