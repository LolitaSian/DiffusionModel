{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LolitaSian/DiffusionModel/blob/main/jupyter/ddpm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5a94671",
      "metadata": {
        "id": "c5a94671"
      },
      "source": [
        "<h1>\n",
        "\tThe Annotated Diffusion Model\n",
        "</h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "290edb0b",
      "metadata": {
        "id": "290edb0b"
      },
      "source": [
        "\n",
        "åœ¨è¿™ç¯‡åšå®¢æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†é€æ­¥è®²è§£([Ho et al., 2020](https://arxiv.org/abs/2006.11239))çš„åŸå§‹DDPMè®ºæ–‡ï¼Œå¹¶åŸºäº[Phil Wangçš„TensorFlowç‰ˆæœ¬]((https://github.com/lucidrains/denoising-diffusion-pytorch))å®ç°Pytorchç‰ˆæœ¬ã€‚è¯·æ³¨æ„ï¼Œæ‰©æ•£ç”¨äºç”Ÿæˆå»ºæ¨¡çš„æ€æƒ³å®é™…ä¸Šå·²ç»åœ¨([Sohl-Dickstein et al., 2015](https://arxiv.org/abs/1503.03585))ä¸­ä»‹ç»è¿‡ï¼Œä½†æ˜¯ï¼Œç›´åˆ°æ–¯å¦ç¦å¤§å­¦çš„([Song et al., 2019](https://arxiv.org/abs/1907.05600))å’Œè°·æ­Œå¤§è„‘çš„([Ho et al., 2020](https://arxiv.org/abs/2006.11239))åˆ†åˆ«æ”¹è¿›äº†è¿™ç§æ–¹æ³•æ‰å¾—ä»¥æµè¡Œèµ·æ¥ã€‚\n",
        "\n",
        "[æ‰©æ•£æ¨¡å‹æœ‰å‡ ä¸ªè§†è§’](https://twitter.com/sedielem/status/1530894256168222722?s=20&t=mfv4afx1GcNQU5fZklpACw)ã€‚åœ¨è¿™é‡Œæˆ‘ä»¬é‡‡ç”¨ç¦»æ•£æ—¶é—´ï¼ˆæ½œå˜é‡æ¨¡å‹ï¼‰çš„è§†è§’."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a1f2d714",
      "metadata": {
        "id": "a1f2d714"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U einops datasets matplotlib tqdm\n",
        "\n",
        "import math\n",
        "from inspect import isfunction\n",
        "from functools import partial\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "from einops import rearrange\n",
        "\n",
        "import torch\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fe49a34",
      "metadata": {
        "id": "6fe49a34"
      },
      "source": [
        "\n",
        "## ä»€ä¹ˆæ˜¯æ‰©æ•£æ¨¡å‹\n",
        "\n",
        "Diffusion model å’Œ Normalizing Flows, GANs or VAEs ä¸€æ ·ï¼Œéƒ½æ˜¯å°†å™ªå£°ä»ä¸€äº›ç®€å•çš„åˆ†å¸ƒè½¬æ¢ä¸ºä¸€ä¸ªæ•°æ®æ ·æœ¬ï¼Œä¹Ÿæ˜¯ç¥ç»ç½‘ç»œå­¦ä¹ ä»çº¯å™ªå£°å¼€å§‹é€æ¸å»å™ªæ•°æ®çš„è¿‡ç¨‹ã€‚\n",
        "åŒ…å«ä¸¤ä¸ªæ­¥éª¤ï¼š\n",
        "- ä¸€ä¸ªæˆ‘ä»¬é€‰æ‹©çš„å›ºå®šçš„ï¼ˆæˆ–è€…è¯´é¢„å®šä¹‰å¥½çš„ï¼‰å‰å‘æ‰©æ•£è¿‡ç¨‹ $q$ ï¼Œå°±æ˜¯é€æ¸ç»™å›¾ç‰‡æ·»åŠ é«˜æ–¯å™ªå£°ï¼Œç›´åˆ°æœ€åè·å¾—çº¯å™ªå£°ã€‚\n",
        "\n",
        "- ä¸€ä¸ªéœ€è¦å­¦ä¹ çš„åå‘çš„å»å™ªè¿‡ç¨‹ $p_\\theta$ï¼Œè®­ç»ƒä¸€ä¸ªç¥ç»ç½‘åšå›¾åƒå»å™ªï¼Œä»çº¯å™ªå£°å¼€å§‹ï¼Œç›´åˆ°è·å¾—æœ€ç»ˆå›¾åƒã€‚\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?id=1t5dUyJwgy2ZpDAqHXw7GhUAp2FE5BWHA\" width=\"600\" />\n",
        "</p>\n",
        "\n",
        "å‰å‘å’Œåå‘è¿‡ç¨‹éƒ½è¦ç»è¿‡æ—¶é—´æ­¥$t$ï¼Œæ€»æ­¥é•¿æ˜¯$T$ï¼ˆDDPMä¸­$T=1000$)ã€‚\n",
        "\n",
        "ä»$t=0$å¼€å§‹ï¼Œä»æ•°æ®é›†åˆ†å¸ƒä¸­é‡‡æ ·ä¸€ä¸ªçœŸå®å›¾ç‰‡$\\mathbf x_0$ã€‚å‰å‘è¿‡ç¨‹å°±æ˜¯åœ¨æ¯ä¸€ä¸ªæ—¶é—´æ­¥$t$ä¸­éƒ½ä»ä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ ·ä¸€ä¸ªå™ªå£°ï¼Œå°†å…¶æ·»åŠ åˆ°ä¸Šä¸€æ—¶é—´æ­¥çš„å›¾åƒä¸Šã€‚ç»™å‡ºä¸€ä¸ªè¶³å¤Ÿå¤§çš„$T$ï¼Œå’Œæ¯ä¸€æ—¶é—´æ­¥ä¸­æ·»åŠ å™ªå£°çš„è¡¨æ ¼ï¼Œæœ€ç»ˆåœ¨$T$æ—¶é—´æ­¥ä½ ä¼šè·å¾—ä¸€ä¸ª[isotropic Gaussian distribution](https://math.stackexchange.com/questions/1991961/gaussian-distribution-is-isotropic)ã€‚\n",
        "\n",
        "\n",
        "\n",
        "## In more mathematical form\n",
        "\n",
        "\n",
        "æˆ‘ä»¬ä»¤$q(\\mathbf x_0)$æ˜¯çœŸå®åˆ†å¸ƒï¼Œä¹Ÿå°±æ˜¯çœŸå®çš„å›¾åƒçš„åˆ†å¸ƒã€‚\n",
        "\n",
        "æˆ‘ä»¬å¯ä»¥ä»ä¸­é‡‡æ ·ä¸€ä¸ªå›¾ç‰‡ï¼Œä¹Ÿå°±æ˜¯$\\mathbf x_0 \\sim q(\\mathbf x_0)$ ã€‚\n",
        "\n",
        "æˆ‘ä»¬è®¾å®šå‰å‘æ‰©æ•£è¿‡ç¨‹$q(\\mathbf x_t|\\mathbf x_{t-1})$æ˜¯ç»™æ¯ä¸ªæ—¶é—´æ­¥$t$æ·»åŠ é«˜æ–¯å™ªå£°ï¼Œè¿™ä¸ªé«˜æ–¯å™ªå£°ä¸æ˜¯éšæœºé€‰æ‹©çš„ï¼Œæ˜¯æ ¹æ®æˆ‘ä»¬é¢„é€‰è®¾å®šå¥½çš„æ–¹å·®è¡¨ï¼ˆ$0 < \\beta_1 < \\beta_2 < ... < \\beta_T < 1$ï¼‰çš„é«˜æ–¯åˆ†å¸ƒä¸­è·å–çš„ã€‚\n",
        "\n",
        "ç„¶åæˆ‘ä»¬å°±å¯ä»¥å¾—åˆ°å‰å‘è¿‡ç¨‹çš„å…¬å¼ä¸ºï¼š\n",
        "$$\n",
        "q(\\mathbf {x}_t | \\mathbf {x}_{t-1}) = \\mathcal{N}(\\mathbf {x}_t; \\sqrt{1 - \\beta_t} \\mathbf {x}_{t-1}, \\beta_t \\mathbf{I}). \n",
        "$$\n",
        "\n",
        "å›æƒ³ä¸€ä¸‹å“¦ã€‚ä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒï¼ˆä¹Ÿå«æ­£æ€åˆ†å¸ƒï¼‰æ˜¯ç”±ä¸¤ä¸ªå‚æ•°å†³å®šçš„ï¼Œå‡å€¼$\\mu$å’Œæ–¹å·®$\\sigma^2 \\geq 0$ã€‚\n",
        "\n",
        "ç„¶åæˆ‘ä»¬å°±å¯ä»¥è®¤ä¸ºæ¯ä¸ªæ—¶é—´æ­¥$t$çš„å›¾åƒæ˜¯ä»ä¸€å‡å€¼ä¸º${\\mu}_t = \\sqrt{1 - \\beta_t} \\mathbf {x}_{t-1}$ã€æ–¹å·®ä¸º$\\sigma^2_t = \\beta_t$çš„æ¡ä»¶é«˜æ–¯åˆ†å¸ƒä¸­ç”»å‡ºæ¥çš„ã€‚å€ŸåŠ©å‚æ•°é‡æ•´åŒ–ï¼ˆreparameterization trickï¼‰å¯ä»¥å†™æˆ\n",
        "\n",
        "$$\n",
        "\\mathbf {x}_t = \\sqrt{1 - \\beta_t}\\mathbf {x}_{t-1} +  \\sqrt{\\beta_t} \\mathbf{\\epsilon}\n",
        "$$\n",
        "\n",
        "å…¶ä¸­$\\mathbf{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ï¼Œæ˜¯ä»æ ‡å‡†é«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ ·çš„å™ªå£°ã€‚ \n",
        "\n",
        "$\\beta_t$åœ¨ä¸åŒçš„æ—¶é—´æ­¥$t$ä¸­ä¸æ˜¯å›ºå®šçš„ï¼Œå› æ­¤æˆ‘ä»¬ç»™$\\beta$åŠ äº†ä¸‹æ ‡ã€‚å¯¹äº$\\beta_t$çš„é€‰æ‹©æˆ‘ä»¬å¯ä»¥è®¾ç½®ä¸ºçº¿æ€§çš„ã€äºŒæ¬¡çš„ã€ä½™å¼¦çš„ç­‰(æœ‰ç‚¹åƒå­¦ä¹ ç‡è®¡åˆ’)ã€‚\n",
        "\n",
        "æ¯”å¦‚åœ¨DDPMä¸­$\\beta_1 = 10^{-4}$, $\\beta_T = 0.02$ï¼Œåœ¨ä¸­é—´æ˜¯åšäº†ä¸€ä¸ªçº¿æ€§æ’å€¼ã€‚è€Œåœ¨Improved DDPMä¸­æ˜¯ä½¿ç”¨ä½™å¼¦å‡½æ•°ã€‚\n",
        "\n",
        "ä»$\\mathbf x_0$å¼€å§‹ï¼Œæˆ‘ä»¬é€šè¿‡$\\mathbf{x}_1,  ..., \\mathbf{x}_t, ..., \\mathbf{x}_T$,æœ€ç»ˆè·å¾—$\\mathbf{x}_T$ ï¼Œå¦‚æœæˆ‘ä»¬çš„é«˜æ–¯å™ªå£°è¡¨è®¾ç½®çš„åˆç†ï¼Œé‚£æœ€åæˆ‘ä»¬è·å¾—çš„åº”è¯¥æ˜¯ä¸€ä¸ªçº¯é«˜æ–¯å™ªå£°ã€‚\n",
        "\n",
        "ç°åœ¨ï¼Œå¦‚æœæˆ‘ä»¬èƒ½çŸ¥é“æ¡ä»¶åˆ†å¸ƒ$p(\\mathbf {x}_{t-1} | \\mathbf {x}_t)$ï¼Œé‚£æˆ‘ä»¬å°±å¯ä»¥å°†è¿™ä¸ªè¿‡ç¨‹å€’è¿‡æ¥ï¼šé‡‡æ ·ä¸€ä¸ªéšæœºé«˜æ–¯å™ªå£°$\\mathbf x_t$ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹å…¶é€æ­¥å»å™ªï¼Œæœ€ç»ˆå¾—åˆ°ä¸€ä¸ªçœŸå®åˆ†å¸ƒçš„å›¾ç‰‡$\\mathbf x_0$ã€‚\n",
        "\n",
        "ä½†æ˜¯æˆ‘ä»¬å®é™…ä¸Šæ²¡åŠæ³•çŸ¥é“$p(\\mathbf {x}_{t-1} | \\mathbf {x}_t)$ã€‚å› ä¸ºå®ƒéœ€è¦çŸ¥é“æ‰€æœ‰å¯èƒ½å›¾åƒçš„åˆ†å¸ƒæ¥è®¡ç®—è¿™ä¸ªæ¡ä»¶æ¦‚ç‡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦å€ŸåŠ©ç¥ç»ç½‘ç»œæ¥è¿‘ä¼¼(å­¦ä¹ )è¿™ä¸ªæ¡ä»¶æ¦‚ç‡åˆ†å¸ƒã€‚ ä¹Ÿå°±æ˜¯$p_\\theta (\\mathbf {x}_{t-1} | \\mathbf {x}_t)$ï¼Œå…¶ä¸­, $\\theta$æ˜¯ç¥ç»ç½‘ç»œçš„å‚æ•°ï¼Œéœ€è¦ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ›´æ–°ã€‚\n",
        "\n",
        "\n",
        "æ‰€ä»¥ç°åœ¨æˆ‘ä»¬éœ€è¦ä¸€ä¸ªç¥ç»ç½‘ç»œæ¥è¡¨ç¤ºé€†å‘è¿‡ç¨‹çš„(æ¡ä»¶)æ¦‚ç‡åˆ†å¸ƒã€‚å¦‚æœæˆ‘ä»¬å‡è®¾è¿™ä¸ªåå‘è¿‡ç¨‹ä¹Ÿæ˜¯é«˜æ–¯åˆ†å¸ƒï¼Œé‚£ä¹ˆå›æƒ³ä¸€ä¸‹ï¼Œä»»ä½•é«˜æ–¯åˆ†å¸ƒéƒ½æ˜¯ç”±ä¸¤ä¸ªå‚æ•°å®šä¹‰çš„:\n",
        "\n",
        "* ä¸€ä¸ªå‡å€¼$\\mu_\\theta$;\n",
        "* ä¸€ä¸ªæ–¹å·®$\\Sigma_\\theta$ã€‚\n",
        "\n",
        "æ‰€ä»¥æˆ‘ä»¬å¯ä»¥æŠŠè¿™ä¸ªè¿‡ç¨‹å‚æ•°åŒ–ä¸º\n",
        "\n",
        "$$\n",
        "p_\\theta (\\mathbf{x}_{t-1} | \\mathbf{x}_t) = \\mathcal{N}(\\mathbf{x}_{t-1}; \\mu_\\theta(\\mathbf{x}_{t},t), \\Sigma_\\theta (\\mathbf{x}_{t},t))\n",
        "$$\n",
        "\n",
        "å…¶ä¸­å‡å€¼å’Œæ–¹å·®ä¹Ÿå–å†³äºå™ªå£°æ°´å¹³$t$ã€‚\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "ä»ä¸Šè¾¹æˆ‘ä»¬å¯ä»¥çŸ¥é“ï¼Œé€†å‘è¿‡ç¨‹æˆ‘ä»¬éœ€è¦ä¸€ä¸ªç¥ç»ç½‘ç»œæ¥å­¦ä¹ ï¼ˆè¡¨ç¤ºï¼‰é«˜æ–¯åˆ†å¸ƒçš„å‡å€¼å’Œæ–¹å·®ã€‚\n",
        "\n",
        "DDPMä¸­ä½œè€…å›ºå®šæ–¹å·®ï¼Œåªè®©ç¥ç»ç½‘ç»œå­¦ä¹ æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒçš„å‡å€¼ã€‚\n",
        "\n",
        "\n",
        "> First, we set $\\Sigma_\\theta ( \\mathbf{x}_t, t) = \\sigma^2_t \\mathbf{I}$ to untrained time dependent constants. Experimentally, both $\\sigma^2_t = \\beta_t$ and $\\sigma^2_t  = \\tilde{\\beta}_t$ (see paper) had similar results. \n",
        "\n",
        "[Improved diffusion models](https://openreview.net/pdf?id=-NEXDKk8gZ)è¿™ç¯‡æ–‡ç« ä¸­è¿›è¡Œäº†æ”¹è¿›ï¼Œç¥ç»ç½‘ç»œæ—¢éœ€è¦å­¦ä¹ å‡å€¼ä¹Ÿè¦å­¦ä¹ æ–¹å·®ã€‚"
      ],
      "metadata": {
        "id": "5wjIt_ftQW6d"
      },
      "id": "5wjIt_ftQW6d"
    },
    {
      "cell_type": "markdown",
      "id": "2d747688",
      "metadata": {
        "id": "2d747688"
      },
      "source": [
        "\n",
        "\n",
        "## é€šè¿‡é‡æ–°å‚æ•°åŒ–å‡å€¼ å®šä¹‰ç›®æ ‡å‡½æ•°\n",
        "\n",
        "ä¸ºäº†æ¨å¯¼å‡ºä¸€ä¸ªç›®æ ‡å‡½æ•°æ¥å­¦ä¹ é€†å‘è¿‡ç¨‹çš„å‡å€¼ï¼Œä½œè€…è§‚å¯Ÿåˆ°$q$å’Œ$p_\\theta$å¯ä»¥çœ‹åšæ˜¯ä¸€ä¸ªVAEæ¨¡å‹ [(Kingma et al., 2013)](https://arxiv.org/abs/1312.6114). \n",
        "\n",
        "å› æ­¤ï¼Œå˜åˆ†ä¸‹ç•Œï¼ˆELBOï¼‰å¯ä»¥ç”¨æ¥æœ€å°åŒ–å…³äºground truth $\\mathbf x_0$çš„è´Ÿå¯¹æ•°ä¼¼ç„¶ã€‚\n",
        "\n",
        "è¿™ä¸ªè¿‡ç¨‹çš„ELBOæ˜¯æ¯ä¸ªæ—¶é—´æ­¥$t$çš„æŸå¤±æ€»å’Œï¼š$L=L_0+L_1+â€¦+L_ğ‘‡$ã€‚\n",
        "\n",
        "é€šè¿‡æ„å»ºæ­£å‘$q$è¿‡ç¨‹å’Œåå‘è¿‡ç¨‹ï¼ŒæŸå¤±çš„æ¯ä¸€é¡¹ï¼Œé™¤äº†$L_0$ï¼Œéƒ½æ˜¯ä¸¤ä¸ªé«˜æ–¯åˆ†å¸ƒä¹‹é—´çš„KLæ•£åº¦ï¼Œå¹¶ä¸”å¯ä»¥å†™ä¸ºå…³äºå‡å€¼çš„$L_2$æŸå¤±!\n",
        "\n",
        "\n",
        "å› ä¸ºé«˜æ–¯åˆ†å¸ƒçš„ç‰¹æ€§ï¼Œæˆ‘ä»¬ä¸éœ€è¦åœ¨æ­£å‘$q$è¿‡ç¨‹ä¸­é€æ­¥æ·»åŠ $t$æ­¥é•¿çš„å™ªå£°ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥è·å¾—$x_t$çš„ç»“æœï¼š\n",
        "\n",
        "$$\n",
        "q(\\mathbf {x}_t | \\mathbf {x}_0) = \\cal{N}(\\mathbf {x}_t; \\sqrt{\\bar{\\alpha}_t} \\mathbf {x}_0, (1- \\bar{\\alpha}_t) \\mathbf{I})\n",
        "$$\n",
        "\n",
        "å…¶ä¸­$\\alpha_t := 1 - \\beta_t$ and $\\bar{\\alpha}_t := \\Pi_{s=1}^{t} \\alpha_s$ã€‚\n",
        "\n",
        "è¿™æ˜¯ä¸€ä¸ªå¾ˆä¼˜ç§€çš„ç‰¹æ€§ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥å¯¹é«˜æ–¯å™ªå£°è¿›è¡Œé‡‡æ ·å¹¶é€‚å½“ç¼©æ”¾ç›´æ¥å°†å…¶æ·»åŠ åˆ°$\\mathbf x_0$ä¸­å°±å¯ä»¥ç›´æ¥å¾—åˆ°$\\mathbf x_t$ã€‚\n",
        "\n",
        "$\\bar{\\alpha}_t$æ˜¯æ–¹å·®è¡¨$\\beta_t$çš„å‡½æ•°ï¼Œå› æ­¤ä¹Ÿæ˜¯å·²çŸ¥çš„ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹å…¶é¢„å…ˆè®¡ç®—ã€‚è¿™æ ·å¯ä»¥è®©æˆ‘ä»¬åœ¨è®­ç»ƒæœŸé—´ä¼˜åŒ–æŸå¤±å‡½æ•°$L$çš„éšæœºé¡¹ï¼ˆæ¢å¥è¯è¯´ï¼Œåœ¨è®­ç»ƒæœŸé—´éšæœºé‡‡æ ·$t$å°±å¯ä»¥ä¼˜åŒ–$L_t$ï¼‰ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68574c28",
      "metadata": {
        "id": "68574c28"
      },
      "source": [
        "\n",
        "\n",
        "è¿™ä¸ªå±æ€§çš„å¦ä¸€ä¸ªä¼˜ç¾ä¹‹å¤„æ˜¯é€šè¿‡é‡æ–°å‚æ•°åŒ–å¹³å‡å€¼ï¼Œä½¿ç¥ç»ç½‘ç»œå­¦ä¹ ï¼ˆé¢„æµ‹ï¼‰æ·»åŠ çš„å™ªå£°ã€‚\n",
        "\n",
        "é€šè¿‡ç¥ç»ç½‘ç»œ$\\epsilon_\\theta(\\mathbf x_tï¼Œt)$é¢„æµ‹å™ªå£°ï¼Œå¯ä»¥æ„æˆæŸå¤±å‡½æ•°ä¸­æ—¶é—´æ­¥$t$çš„KLé¡¹ã€‚\n",
        "\n",
        "è¿™æ„å‘³ç€æˆ‘ä»¬çš„ç¥ç»ç½‘ç»œå˜æˆäº†å™ªå£°é¢„æµ‹å™¨ï¼Œè€Œä¸æ˜¯ç›´æ¥å»é¢„æµ‹å‡å€¼äº†ã€‚\n",
        "\n",
        "å‡å€¼çš„è®¡ç®—æ–¹æ³•å¦‚ä¸‹:\n",
        "\n",
        "$$ \\mathbf{\\mu}_\\theta(\\mathbf{x}_t, t) = \\frac{1}{\\sqrt{\\alpha_t}} \\left(  \\mathbf{x}_t - \\frac{\\beta_t}{\\sqrt{1- \\bar{\\alpha}_t}} \\mathbf{\\epsilon}_\\theta(\\mathbf{x}_t, t) \\right)$$\n",
        "\n",
        "æœ€åçš„ç›®æ ‡å‡½æ•°$L_t$ é•¿è¿™æ ·ï¼Œç»™å®šéšæœºçš„æ—¶é—´æ­¥ $t$ ï¼Œ${\\epsilon} \\sim \\mathcal{N}({0}, {I})$ : \n",
        "\n",
        "$$ \\| \\mathbf{\\epsilon} - \\mathbf{\\epsilon}_\\theta(\\mathbf{x}_t, t) \\|^2 = \\| \\mathbf{\\epsilon} - \\mathbf{\\epsilon}_\\theta( \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0 + \\sqrt{(1- \\bar{\\alpha}_t)  } \\mathbf{\\epsilon}, t) \\|^2.$$\n",
        "\n",
        "\n",
        "\n",
        "$\\mathbf x_0$æ˜¯åˆå§‹å›¾åƒï¼Œæˆ‘ä»¬çœ‹åˆ°å™ªå£°$t$æ ·æœ¬ç”±å›ºå®šçš„å‰å‘è¿‡ç¨‹ç»™å‡ºã€‚$\\epsilon$æ˜¯åœ¨æ—¶é—´æ­¥é•¿$t$é‡‡æ ·çš„çº¯å™ªå£°ï¼Œ$\\epsilon_\\theta(\\mathbf x_tï¼Œt)$æ˜¯æˆ‘ä»¬çš„ç¥ç»ç½‘ç»œã€‚ç¥ç»ç½‘ç»œçš„ä¼˜åŒ–ä½¿ç”¨ä¸€ä¸ªç®€å•çš„å‡æ–¹è¯¯å·®(MSE)è®¡ç®—çœŸå®å™ªå£°å’Œé¢„æµ‹é«˜æ–¯å™ªå£°ä¹‹é—´çš„å·®å¼‚ã€‚\n",
        "\n",
        "è®­ç»ƒç®—æ³•å¦‚ä¸‹ï¼š\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?id=1LJsdkZ3i1J32lmi9ONMqKFg5LMtpSfT4\" width=\"400\" />\n",
        "</p>\n",
        " \n",
        "\n",
        "1. ä»æœªçŸ¥çš„çœŸå®æ•°æ®åˆ†å¸ƒ$q(\\mathbf x_0)$ä¸­éšæœºé‡‡æ ·$\\mathbf x_0$ï¼Œ\n",
        "2. æˆ‘ä»¬åœ¨1å’Œ$T$ä¹‹é—´å‡åŒ€é‡‡ä¸åŒæ—¶é—´æ­¥çš„å™ªå£°ï¼Œ\n",
        "3. æˆ‘ä»¬ä»é«˜æ–¯åˆ†å¸ƒé‡‡æ ·ä¸€äº›å™ªå£°ï¼Œå¹¶åœ¨$ğ‘¡$æ—¶é—´æ­¥ä¸Šä½¿ç”¨å‰è¾¹å®šä¹‰çš„ä¼˜è‰¯å±æ€§æ¥ç ´åè¾“å…¥åˆ†å¸ƒï¼Œ\n",
        "4. ç¥ç»ç½‘ç»œæ ¹æ®æŸåçš„å›¾åƒ$\\mathbf x_t$è¿›è¡Œè®­ç»ƒï¼Œç›®çš„æ˜¯é¢„æµ‹æ–½åŠ åœ¨å›¾ç‰‡ä¸Šçš„å™ªå£°ï¼Œä¹Ÿå°±æ˜¯åŸºäºå·²çŸ¥æ–¹å·®è¡¨$\\beta_t$ä½œç”¨åœ¨$\\mathbf x_0$ä¸Šçš„å™ªå£°\n",
        "\n",
        "\n",
        "æ‰€æœ‰è¿™äº›éƒ½æ˜¯åœ¨æ‰¹é‡æ•°æ®ä¸Šå®Œæˆçš„ï¼Œä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™ä¼˜åŒ–ç¥ç»ç½‘ç»œã€‚\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5153024b",
      "metadata": {
        "id": "5153024b"
      },
      "source": [
        "\n",
        "\n",
        "## ç¥ç»ç½‘ç»œ\n",
        "\n",
        "ç¥ç»ç½‘ç»œéœ€è¦åœ¨ç‰¹å®šçš„æ—¶é—´æ­¥$t$ä¸­è¾“å…¥ä¸€ä¸ªå¸¦æœ‰å™ªå£°çš„å›¾åƒï¼Œå¹¶è¿”å›é¢„æµ‹çš„å™ªå£°ã€‚è¯·æ³¨æ„ï¼Œé¢„æµ‹çš„å™ªå£°æ˜¯ä¸€ä¸ªå¼ é‡ï¼Œå…¶å¤§å°ä¸è¾“å…¥å›¾åƒç›¸åŒã€‚å› æ­¤ï¼Œåœ¨æŠ€æœ¯å®ç°ä¸Šï¼Œç½‘ç»œçš„è¾“å…¥å’Œè¾“å‡ºæ˜¯ç›¸åŒå½¢çŠ¶çš„å¼ é‡ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»€ä¹ˆç±»å‹çš„ç¥ç»ç½‘ç»œæ¥å®ç°è¿™ä¸ªä»»åŠ¡ï¼Ÿ\n",
        "\n",
        "\n",
        "åœ¨è¿™é‡Œé€šå¸¸ä½¿ç”¨çš„æ˜¯ç±»ä¼¼äº[è‡ªç¼–ç å™¨](https://en.wikipedia.org/wiki/Autoencoder)çš„ç½‘ç»œï¼Œè‡ªç¼–ç å™¨åœ¨ç¼–ç å™¨å’Œè§£ç å™¨ä¹‹é—´æœ‰ä¸€ä¸ªæ‰€è°“çš„â€œç“¶é¢ˆâ€ï¼ˆbottleneckï¼‰å±‚ã€‚ç¼–ç å™¨é¦–å…ˆå°†å›¾åƒç¼–ç ä¸ºè¾ƒå°çš„éšè—è¡¨ç¤ºï¼Œç§°ä¸ºâ€œç“¶é¢ˆâ€ï¼Œç„¶åè§£ç å™¨å°†è¯¥éšè—è¡¨ç¤ºè§£ç å›å®é™…å›¾åƒã€‚è¿™ä½¿ç½‘ç»œçš„ç“¶é¢ˆå±‚ä¸­å¯ä»¥ä¿ç•™æœ€é‡è¦çš„ä¿¡æ¯ã€‚\n",
        "\n",
        "\n",
        "åœ¨ä½“ç³»ç»“æ„æ–¹é¢ï¼ŒDDPMä½œè€…é€‰æ‹©äº†U-Netï¼Œç”±([Ronneberger et al., 2015](https://arxiv.org/abs/1505.04597))æå‡ºï¼Œå½“æ—¶åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹é¢å®ç°äº†SOTAã€‚ä¸ä»»ä½•è‡ªç¼–ç å™¨ä¸€æ ·ï¼Œè¯¥ç½‘ç»œåŒ…æ‹¬ä¸­é—´çš„ç“¶é¢ˆï¼Œä»¥ç¡®ä¿ç½‘ç»œå­¦ä¹ åˆ°æœ€é‡è¦çš„ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œå®ƒå¼•å…¥äº†ç¼–ç å™¨å’Œè§£ç å™¨ä¹‹é—´çš„æ®‹å·®è¿æ¥ï¼Œæå¤§åœ°æ”¹å–„äº†æ¢¯åº¦æµåŠ¨ï¼ˆå—[He et al., 2015](https://arxiv.org/abs/1512.03385))ResNetçš„å¯å‘ï¼‰ã€‚\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?id=1_Hej_VTgdUWGsxxIuyZACCGjpbCGIUi6\" width=\"400\" />\n",
        "</p>\n",
        "\n",
        "å¯ä»¥çœ‹å‡ºï¼ŒU-Netæ¨¡å‹é¦–å…ˆå¯¹è¾“å…¥è¿›è¡Œä¸‹é‡‡æ ·ï¼ˆå³åœ¨ç©ºé—´åˆ†è¾¨ç‡ä¸Šä½¿è¾“å…¥æ›´å°ï¼‰ï¼Œç„¶åæ‰§è¡Œä¸Šé‡‡æ ·ã€‚\n",
        "\n",
        "ä¸‹é¢ï¼Œæˆ‘ä»¬å°†é€æ­¥å®ç°è¿™ä¸ªç½‘ç»œã€‚\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ç½‘ç»œè¾…åŠ©æ¨¡å—\n",
        "\n",
        "é¦–å…ˆï¼Œæˆ‘ä»¬å®šä¹‰ä¸€äº›è¾…åŠ©å‡½æ•°å’Œç±»ï¼Œåœ¨å®ç°ç¥ç»ç½‘ç»œæ—¶ä¼šç”¨åˆ°å®ƒä»¬ã€‚é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªâ€œæ®‹å·®â€æ¨¡å—ï¼Œå®ƒå°†è¾“å…¥ç®€å•åœ°åŠ åˆ°ç‰¹å®šå‡½æ•°çš„è¾“å‡ºä¸­ï¼ˆä¸ºç‰¹å®šå‡½æ•°æ·»åŠ äº†ä¸€ä¸ªæ®‹å·®è¿æ¥ï¼‰ã€‚\n",
        "\n",
        "æˆ‘ä»¬è¿˜ä¸ºä¸Šé‡‡æ ·å’Œä¸‹é‡‡æ ·æ“ä½œå®šä¹‰äº†åˆ«åã€‚"
      ],
      "metadata": {
        "id": "xsm73idkszVl"
      },
      "id": "xsm73idkszVl"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "68907f5d",
      "metadata": {
        "id": "68907f5d"
      },
      "outputs": [],
      "source": [
        "def exists(x):\n",
        "    return x is not None\n",
        "\n",
        "def default(val, d):\n",
        "    if exists(val):\n",
        "        return val\n",
        "    return d() if isfunction(d) else d\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        return self.fn(x, *args, **kwargs) + x\n",
        "\n",
        "def Upsample(dim):\n",
        "    return nn.ConvTranspose2d(dim, dim, 4, 2, 1)\n",
        "\n",
        "def Downsample(dim):\n",
        "    return nn.Conv2d(dim, dim, 4, 2, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "592aa765",
      "metadata": {
        "id": "592aa765"
      },
      "source": [
        "### ä½ç½®åµŒå…¥\n",
        "\n",
        "ç¥ç»ç½‘ç»œçš„å‚æ•°åœ¨ä¸åŒæ—¶é—´æ­¥ä¹‹é—´æ˜¯å…±äº«çš„ï¼Œå› æ­¤ä½œè€…å—Transformer([Vaswani et al., 2017](https://arxiv.org/abs/1706.03762))å¯å‘ï¼Œä½¿ç”¨æ­£å¼¦ä½ç½®åµŒå…¥æ¥å¯¹$t$è¿›è¡Œç¼–ç ã€‚è¿™ä½¿å¾—ç¥ç»ç½‘ç»œèƒ½å¤Ÿâ€œçŸ¥é“â€å®ƒæ­£åœ¨å¤„ç†çš„batchä¸­æ¯ä¸ªå›¾åƒçš„æ—¶é—´æ­¥é•¿ï¼ˆå™ªå£°æ°´å¹³ï¼‰ã€‚\n",
        "\n",
        "`SinusoidalPositionEmbeddings`æ¨¡å—ä»¥å½¢çŠ¶ä¸º`(batch_size, 1)`çš„å¼ é‡ä½œä¸ºè¾“å…¥ï¼Œå³å½“å‰batchä¸­æ¯ä¸ªå›¾ç‰‡çš„æ—¶é—´æ­¥ï¼ˆå™ªå£°æ°´å¹³ï¼‰ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºå½¢çŠ¶ä¸º(batch_sizeï¼Œdim)çš„å¼ é‡ï¼Œå…¶ä¸­dimæ˜¯ä½ç½®åµŒå…¥çš„ç»´åº¦ã€‚ç„¶åå°†å…¶æ·»åŠ åˆ°æ¯ä¸ªæ®‹å·®å—ä¸­ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5ed0757b",
      "metadata": {
        "id": "5ed0757b"
      },
      "outputs": [],
      "source": [
        "class SinusoidalPositionEmbeddings(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, time):\n",
        "        device = time.device\n",
        "        half_dim = self.dim // 2\n",
        "        embeddings = math.log(10000) / (half_dim - 1)\n",
        "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
        "        embeddings = time[:, None] * embeddings[None, :]\n",
        "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
        "        return embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ff47fbb",
      "metadata": {
        "id": "9ff47fbb"
      },
      "source": [
        "###  ResNet/ConvNeXTå—\n",
        "\n",
        "æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®šä¹‰U-Netæ¨¡å‹çš„æ ¸å¿ƒæ„å»ºå—ã€‚\n",
        "\n",
        "DDPMä½œè€…ä½¿ç”¨äº†Wide ResNetå—([Zagoruyko et al., 2016](https://arxiv.org/abs/1605.07146))ï¼Œä½†Phil Wangä»£ç ä¸­è¿˜å®ç°äº†ConvNeXTå—([Liu et al., 2022](https://arxiv.org/abs/2201.03545))ã€‚åœ¨U-Netæ¶æ„ä¸­ï¼Œå¯ä»¥ä»»é€‰å…¶ä¸€ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2c2d1219",
      "metadata": {
        "id": "2c2d1219"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, dim_out, groups = 8):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv2d(dim, dim_out, 3, padding = 1)\n",
        "        self.norm = nn.GroupNorm(groups, dim_out)\n",
        "        self.act = nn.SiLU()\n",
        "\n",
        "    def forward(self, x, scale_shift = None):\n",
        "        x = self.proj(x)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        if exists(scale_shift):\n",
        "            scale, shift = scale_shift\n",
        "            x = x * (scale + 1) + shift\n",
        "\n",
        "        x = self.act(x)\n",
        "        return x\n",
        "\n",
        "class ResnetBlock(nn.Module):\n",
        "    \"\"\"https://arxiv.org/abs/1512.03385\"\"\"\n",
        "    \n",
        "    def __init__(self, dim, dim_out, *, time_emb_dim=None, groups=8):\n",
        "        super().__init__()\n",
        "        self.mlp = (\n",
        "            nn.Sequential(nn.SiLU(), nn.Linear(time_emb_dim, dim_out))\n",
        "            if exists(time_emb_dim)\n",
        "            else None\n",
        "        )\n",
        "\n",
        "        self.block1 = Block(dim, dim_out, groups=groups)\n",
        "        self.block2 = Block(dim_out, dim_out, groups=groups)\n",
        "        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x, time_emb=None):\n",
        "        h = self.block1(x)\n",
        "\n",
        "        if exists(self.mlp) and exists(time_emb):\n",
        "            time_emb = self.mlp(time_emb)\n",
        "            h = rearrange(time_emb, \"b c -> b c 1 1\") + h\n",
        "\n",
        "        h = self.block2(h)\n",
        "        return h + self.res_conv(x)\n",
        "    \n",
        "class ConvNextBlock(nn.Module):\n",
        "    \"\"\"https://arxiv.org/abs/2201.03545\"\"\"\n",
        "\n",
        "    def __init__(self, dim, dim_out, *, time_emb_dim=None, mult=2, norm=True):\n",
        "        super().__init__()\n",
        "        self.mlp = (\n",
        "            nn.Sequential(nn.GELU(), nn.Linear(time_emb_dim, dim))\n",
        "            if exists(time_emb_dim)\n",
        "            else None\n",
        "        )\n",
        "\n",
        "        self.ds_conv = nn.Conv2d(dim, dim, 7, padding=3, groups=dim)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.GroupNorm(1, dim) if norm else nn.Identity(),\n",
        "            nn.Conv2d(dim, dim_out * mult, 3, padding=1),\n",
        "            nn.GELU(),\n",
        "            nn.GroupNorm(1, dim_out * mult),\n",
        "            nn.Conv2d(dim_out * mult, dim_out, 3, padding=1),\n",
        "        )\n",
        "\n",
        "        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x, time_emb=None):\n",
        "        h = self.ds_conv(x)\n",
        "\n",
        "        if exists(self.mlp) and exists(time_emb):\n",
        "            assert exists(time_emb), \"time embedding must be passed in\"\n",
        "            condition = self.mlp(time_emb)\n",
        "            h = h + rearrange(condition, \"b c -> b c 1 1\")\n",
        "\n",
        "        h = self.net(h)\n",
        "        return h + self.res_conv(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51d9a24c",
      "metadata": {
        "id": "51d9a24c"
      },
      "source": [
        "### Attention æ¨¡å—\n",
        "\n",
        "æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®šä¹‰æ³¨æ„åŠ›æ¨¡å—ï¼Œè¯¥æ¨¡å—ç”±DDPMä½œè€…æ·»åŠ åœ¨å·ç§¯å—ä¹‹é—´ã€‚æ³¨æ„åŠ›æ˜¯Transformer([Vaswani et al., 2017](https://arxiv.org/abs/1706.03762))çš„æ„å»ºå—ï¼Œåœ¨AIé¢†åŸŸï¼Œä»NLPå’ŒCVåˆ°è›‹ç™½è´¨æŠ˜å ï¼Œéƒ½å–å¾—äº†å·¨å¤§çš„æˆåŠŸã€‚Phil Wangå®ç°äº†2ç§æ³¨æ„åŠ›å˜ä½“ï¼šä¸€ç§æ˜¯å¸¸è§„çš„å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼ˆå’ŒTransformerä¸­çš„ä¸€æ ·ï¼‰ï¼Œå¦ä¸€ç§æ˜¯[çº¿æ€§æ³¨æ„åŠ›å˜ä½“](https://github.com/lucidrains/linear-attention-transformer)ï¼ˆ[Shen et al., 2018](https://arxiv.org/abs/1812.01243)ï¼‰ï¼Œå…¶æ—¶é—´å’Œå†…å­˜éœ€æ±‚éšåºåˆ—é•¿åº¦çº¿æ€§ç¼©æ”¾ï¼ŒèŠ‚çœè®¡ç®—èµ„æºã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "07bbd544",
      "metadata": {
        "id": "07bbd544"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads=4, dim_head=32):\n",
        "        super().__init__()\n",
        "        self.scale = dim_head**-0.5\n",
        "        self.heads = heads\n",
        "        hidden_dim = dim_head * heads\n",
        "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
        "        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
        "        q, k, v = map(\n",
        "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
        "        )\n",
        "        q = q * self.scale\n",
        "\n",
        "        sim = einsum(\"b h d i, b h d j -> b h i j\", q, k)\n",
        "        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n",
        "        attn = sim.softmax(dim=-1)\n",
        "\n",
        "        out = einsum(\"b h i j, b h d j -> b h i d\", attn, v)\n",
        "        out = rearrange(out, \"b h (x y) d -> b (h d) x y\", x=h, y=w)\n",
        "        return self.to_out(out)\n",
        "\n",
        "class LinearAttention(nn.Module):\n",
        "    def __init__(self, dim, heads=4, dim_head=32):\n",
        "        super().__init__()\n",
        "        self.scale = dim_head**-0.5\n",
        "        self.heads = heads\n",
        "        hidden_dim = dim_head * heads\n",
        "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
        "\n",
        "        self.to_out = nn.Sequential(nn.Conv2d(hidden_dim, dim, 1), \n",
        "                                    nn.GroupNorm(1, dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
        "        q, k, v = map(\n",
        "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
        "        )\n",
        "\n",
        "        q = q.softmax(dim=-2)\n",
        "        k = k.softmax(dim=-1)\n",
        "\n",
        "        q = q * self.scale\n",
        "        context = torch.einsum(\"b h d n, b h e n -> b h d e\", k, v)\n",
        "\n",
        "        out = torch.einsum(\"b h d e, b h d n -> b h e n\", context, q)\n",
        "        out = rearrange(out, \"b h c (x y) -> b (h c) x y\", h=self.heads, x=h, y=w)\n",
        "        return self.to_out(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a8031b0",
      "metadata": {
        "id": "9a8031b0"
      },
      "source": [
        "### Group normalization\n",
        "\n",
        "DDPM çš„ä½œè€…åœ¨ U-Net çš„å·ç§¯/æ³¨æ„åŠ›å±‚ä¹‹é—´äº¤æ›¿ä½¿ç”¨äº†ç¾¤ç»„å½’ä¸€åŒ–([Wu et al., 2018](https://arxiv.org/abs/1803.08494))ã€‚ä¸‹é¢ï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ª`PreNorm`ç±»ï¼Œå®ƒå°†ç”¨äºåœ¨æ³¨æ„åŠ›å±‚ä¹‹å‰å®ç°ç¾¤ç»„å½’ä¸€åŒ–ã€‚\n",
        "\n",
        "è¯·æ³¨æ„ï¼Œåœ¨ Transformer ä¸­ï¼Œæ˜¯å¦åœ¨æ³¨æ„åŠ›ä¹‹å‰æˆ–ä¹‹åä½¿ç”¨å½’ä¸€åŒ–å­˜åœ¨äº‰è®ºï¼Œå…·ä½“å¯ä»¥çœ‹ï¼š[Transformers without Tears](https://tnq177.github.io/data/transformers_without_tears.pdf)ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "5e2ce68f",
      "metadata": {
        "id": "5e2ce68f"
      },
      "outputs": [],
      "source": [
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        self.norm = nn.GroupNorm(1, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.norm(x)\n",
        "        return self.fn(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06b3fad0",
      "metadata": {
        "id": "06b3fad0"
      },
      "source": [
        "### Conditional U-Net\n",
        "\n",
        "æœ‰äº†æ‰€æœ‰æ„å»ºå—ï¼ˆä½ç½®åµŒå…¥ã€ResNet/ConvNeXTå—ã€æ³¨æ„åŠ›å’Œç»„å½’ä¸€åŒ–ï¼‰çš„å®šä¹‰ï¼Œç°åœ¨å®šä¹‰æ•´ä¸ªç¥ç»ç½‘ç»œã€‚\n",
        "\n",
        "å›æƒ³ä¸€ä¸‹ï¼Œç½‘ç»œ $\\mathbf{\\epsilon}_\\theta(\\mathbf{x}_t, t)$ çš„ä»»åŠ¡æ˜¯æ¥æ”¶ä¸€æ‰¹æœ‰å™ªå£°çš„å›¾åƒå’Œå™ªå£°æ°´å¹³ï¼Œå¹¶è¾“å‡ºåŠ å…¥å™ªå£°åçš„å›¾åƒã€‚æ›´æ­£å¼åœ°è¯´ï¼š\n",
        "\n",
        "ç½‘ç»œæ¥æ”¶ä¸€ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„æœ‰å™ªå£°å›¾åƒæ‰¹æ¬¡å’Œä¸€ä¸ªå½¢çŠ¶ä¸º`(batch_size, 1)`çš„å™ªå£°æ°´å¹³æ‰¹æ¬¡ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›ä¸€ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„å¼ é‡ã€‚\n",
        "\n",
        "ç½‘ç»œçš„æ„å»ºå¦‚ä¸‹ï¼š\n",
        "\n",
        "é¦–å…ˆï¼Œåœ¨æœ‰å™ªå£°å›¾åƒçš„æ‰¹æ¬¡ä¸Šåº”ç”¨å·ç§¯å±‚ï¼Œå¹¶ä¸ºå™ªå£°æ°´å¹³è®¡ç®—ä½ç½®åµŒå…¥ã€‚\n",
        "æ¥ä¸‹æ¥ï¼Œåº”ç”¨ä¸€ç³»åˆ—çš„ä¸‹é‡‡æ ·é˜¶æ®µã€‚æ¯ä¸ªä¸‹é‡‡æ ·é˜¶æ®µåŒ…å«2ä¸ªResNet/ConvNeXTå—+ç»„å½’ä¸€åŒ–+æ³¨æ„åŠ›+æ®‹å·®è¿æ¥+ä¸‹é‡‡æ ·æ“ä½œã€‚\n",
        "åœ¨ç½‘ç»œçš„ä¸­å¿ƒï¼Œå†æ¬¡åº”ç”¨ResNetæˆ–ConvNeXTå—ï¼Œäº¤æ›¿ä½¿ç”¨æ³¨æ„åŠ›ã€‚\n",
        "æ¥ä¸‹æ¥ï¼Œåº”ç”¨ä¸€ç³»åˆ—çš„ä¸Šé‡‡æ ·é˜¶æ®µã€‚æ¯ä¸ªä¸Šé‡‡æ ·é˜¶æ®µåŒ…å«2ä¸ªResNet/ConvNeXTå—+ç»„å½’ä¸€åŒ–+æ³¨æ„åŠ›+æ®‹å·®è¿æ¥+ä¸Šé‡‡æ ·æ“ä½œã€‚\n",
        "æœ€åï¼Œåº”ç”¨ä¸€ä¸ªResNet/ConvNeXTå—ï¼Œç„¶åæ˜¯ä¸€ä¸ªå·ç§¯å±‚ã€‚\n",
        "æœ€ç»ˆï¼Œç¥ç»ç½‘ç»œå †å å±‚å°±åƒç§¯æœ¨å—ä¸€æ ·ï¼ˆä½†ç†è§£å®ƒä»¬çš„å·¥ä½œæ–¹å¼å¾ˆé‡è¦ï¼‰ã€‚\n",
        "\n",
        "Now that we've defined all building blocks (position embeddings, ResNet/ConvNeXT blocks, attention and group normalization), it's time to define the entire neural network. Recall that the job of the network \\\\(\\mathbf{\\epsilon}_\\theta(\\mathbf{x}_t, t)\\\\) is to take in a batch of noisy images + noise levels, and output the noise added to the input. More formally:\n",
        "\n",
        "- the network takes a batch of noisy images of shape `(batch_size, num_channels, height, width)` and a batch of noise levels of shape `(batch_size, 1)` as input, and returns a tensor of shape `(batch_size, num_channels, height, width)`\n",
        "\n",
        "The network is built up as follows:\n",
        "* first, a convolutional layer is applied on the batch of noisy images, and position embeddings are computed for the noise levels\n",
        "* next, a sequence of downsampling stages are applied. Each downsampling stage consists of 2 ResNet/ConvNeXT blocks + groupnorm + attention + residual connection + a downsample operation\n",
        "* at the middle of the network, again ResNet or ConvNeXT blocks are applied, interleaved with attention\n",
        "* next, a sequence of upsampling stages are applied. Each upsampling stage consists of 2 ResNet/ConvNeXT blocks + groupnorm + attention + residual connection + an upsample operation\n",
        "* finally, a ResNet/ConvNeXT block followed by a convolutional layer is applied.\n",
        "\n",
        "Ultimately, neural networks stack up layers as if they were lego blocks (but it's important to [understand how they work](http://karpathy.github.io/2019/04/25/recipe/)).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "3a159023",
      "metadata": {
        "id": "3a159023"
      },
      "outputs": [],
      "source": [
        "class Unet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        init_dim=None,\n",
        "        out_dim=None,\n",
        "        dim_mults=(1, 2, 4, 8),\n",
        "        channels=3,\n",
        "        with_time_emb=True,\n",
        "        resnet_block_groups=8,\n",
        "        use_convnext=True,\n",
        "        convnext_mult=2,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # determine dimensions\n",
        "        self.channels = channels\n",
        "\n",
        "        init_dim = default(init_dim, dim // 3 * 2)\n",
        "        self.init_conv = nn.Conv2d(channels, init_dim, 7, padding=3)\n",
        "\n",
        "        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n",
        "        in_out = list(zip(dims[:-1], dims[1:]))\n",
        "        \n",
        "        if use_convnext:\n",
        "            block_klass = partial(ConvNextBlock, mult=convnext_mult)\n",
        "        else:\n",
        "            block_klass = partial(ResnetBlock, groups=resnet_block_groups)\n",
        "\n",
        "        # time embeddings\n",
        "        if with_time_emb:\n",
        "            time_dim = dim * 4\n",
        "            self.time_mlp = nn.Sequential(\n",
        "                SinusoidalPositionEmbeddings(dim),\n",
        "                nn.Linear(dim, time_dim),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(time_dim, time_dim),\n",
        "            )\n",
        "        else:\n",
        "            time_dim = None\n",
        "            self.time_mlp = None\n",
        "\n",
        "        # layers\n",
        "        self.downs = nn.ModuleList([])\n",
        "        self.ups = nn.ModuleList([])\n",
        "        num_resolutions = len(in_out)\n",
        "\n",
        "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
        "            is_last = ind >= (num_resolutions - 1)\n",
        "\n",
        "            self.downs.append(\n",
        "                nn.ModuleList(\n",
        "                    [\n",
        "                        block_klass(dim_in, dim_out, time_emb_dim=time_dim),\n",
        "                        block_klass(dim_out, dim_out, time_emb_dim=time_dim),\n",
        "                        Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n",
        "                        Downsample(dim_out) if not is_last else nn.Identity(),\n",
        "                    ]\n",
        "                )\n",
        "            )\n",
        "\n",
        "        mid_dim = dims[-1]\n",
        "        self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
        "        self.mid_attn = Residual(PreNorm(mid_dim, Attention(mid_dim)))\n",
        "        self.mid_block2 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
        "\n",
        "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n",
        "            is_last = ind >= (num_resolutions - 1)\n",
        "\n",
        "            self.ups.append(\n",
        "                nn.ModuleList(\n",
        "                    [\n",
        "                        block_klass(dim_out * 2, dim_in, time_emb_dim=time_dim),\n",
        "                        block_klass(dim_in, dim_in, time_emb_dim=time_dim),\n",
        "                        Residual(PreNorm(dim_in, LinearAttention(dim_in))),\n",
        "                        Upsample(dim_in) if not is_last else nn.Identity(),\n",
        "                    ]\n",
        "                )\n",
        "            )\n",
        "\n",
        "        out_dim = default(out_dim, channels)\n",
        "        self.final_conv = nn.Sequential(\n",
        "            block_klass(dim, dim), nn.Conv2d(dim, out_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, time):\n",
        "        x = self.init_conv(x)\n",
        "\n",
        "        t = self.time_mlp(time) if exists(self.time_mlp) else None\n",
        "\n",
        "        h = []\n",
        "\n",
        "        # downsample\n",
        "        for block1, block2, attn, downsample in self.downs:\n",
        "            x = block1(x, t)\n",
        "            x = block2(x, t)\n",
        "            x = attn(x)\n",
        "            h.append(x)\n",
        "            x = downsample(x)\n",
        "\n",
        "        # bottleneck\n",
        "        x = self.mid_block1(x, t)\n",
        "        x = self.mid_attn(x)\n",
        "        x = self.mid_block2(x, t)\n",
        "\n",
        "        # upsample\n",
        "        for block1, block2, attn, upsample in self.ups:\n",
        "            x = torch.cat((x, h.pop()), dim=1)\n",
        "            x = block1(x, t)\n",
        "            x = block2(x, t)\n",
        "            x = attn(x)\n",
        "            x = upsample(x)\n",
        "\n",
        "        return self.final_conv(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a30368b2",
      "metadata": {
        "id": "a30368b2"
      },
      "source": [
        "## Defining the forward diffusion process\n",
        "\n",
        "The forward diffusion process gradually adds noise to an image from the real distribution, in a number of time steps $T$. This happens according to a **variance schedule**. The original DDPM authors employed a linear schedule:\n",
        "\n",
        "> We set the forward process variances to constants\n",
        "increasing linearly from $\\beta_1 = 10^{âˆ’4}$\n",
        "to $\\beta_T = 0.02$.\n",
        "\n",
        "However, it was shown in ([Nichol et al., 2021](https://arxiv.org/abs/2102.09672)) that better results can be achieved when employing a cosine schedule. \n",
        "\n",
        "Below, we define various schedules for the $T$ timesteps, as well as corresponding variables which we'll need, such as cumulative variances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "5d751df2",
      "metadata": {
        "id": "5d751df2"
      },
      "outputs": [],
      "source": [
        "def cosine_beta_schedule(timesteps, s=0.008):\n",
        "    \"\"\"\n",
        "    cosine schedule as proposed in https://arxiv.org/abs/2102.09672\n",
        "    \"\"\"\n",
        "    steps = timesteps + 1\n",
        "    x = torch.linspace(0, timesteps, steps)\n",
        "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
        "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
        "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
        "    return torch.clip(betas, 0.0001, 0.9999)\n",
        "\n",
        "def linear_beta_schedule(timesteps):\n",
        "    beta_start = 0.0001\n",
        "    beta_end = 0.02\n",
        "    return torch.linspace(beta_start, beta_end, timesteps)\n",
        "\n",
        "def quadratic_beta_schedule(timesteps):\n",
        "    beta_start = 0.0001\n",
        "    beta_end = 0.02\n",
        "    return torch.linspace(beta_start**0.5, beta_end**0.5, timesteps) ** 2\n",
        "\n",
        "def sigmoid_beta_schedule(timesteps):\n",
        "    beta_start = 0.0001\n",
        "    beta_end = 0.02\n",
        "    betas = torch.linspace(-6, 6, timesteps)\n",
        "    return torch.sigmoid(betas) * (beta_end - beta_start) + beta_start"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bfc3841",
      "metadata": {
        "id": "6bfc3841"
      },
      "source": [
        "To start with, let's use the linear schedule for \\\\(T=200\\\\) time steps and define the various variables from the \\\\(\\beta_t\\\\) which we will need, such as the cumulative product of the variances \\\\(\\bar{\\alpha}_t\\\\). Each of the variables below are just 1-dimensional tensors, storing values from \\\\(t\\\\) to \\\\(T\\\\). Importantly, we also define an `extract` function, which will allow us to extract the appropriate \\\\(t\\\\) index for a batch of indices.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc57b01f",
      "metadata": {
        "id": "cc57b01f"
      },
      "outputs": [],
      "source": [
        "timesteps = 200\n",
        "\n",
        "# define beta schedule\n",
        "betas = linear_beta_schedule(timesteps=timesteps)\n",
        "\n",
        "# define alphas \n",
        "alphas = 1. - betas\n",
        "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
        "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
        "sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
        "\n",
        "# calculations for diffusion q(x_t | x_{t-1}) and others\n",
        "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
        "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n",
        "\n",
        "# calculations for posterior q(x_{t-1} | x_t, x_0)\n",
        "posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
        "\n",
        "def extract(a, t, x_shape):\n",
        "    batch_size = t.shape[0]\n",
        "    out = a.gather(-1, t.cpu())\n",
        "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48f8a004",
      "metadata": {
        "id": "48f8a004"
      },
      "source": [
        "We'll illustrate with a cats image how noise is added at each time step of the diffusion process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9f13b16",
      "metadata": {
        "id": "c9f13b16",
        "lines_to_next_cell": 0
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bde062f",
      "metadata": {
        "id": "4bde062f"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=17FXnvCTl96lDhqZ_io54guXO8hM-rsQ2\" width=\"400\" />\n",
        "\n",
        "Noise is added to PyTorch tensors, rather than Pillow Images. We'll first define image transformations that allow us to go from a PIL image to a PyTorch tensor (on which we can add the noise), and vice versa.\n",
        "\n",
        "These transformations are fairly simple: we first normalize images by dividing by $255$ (such that they are in the $[0,1]$ range), and then make sure they are in the $[-1, 1]$ range. From the DPPM paper:\n",
        "\n",
        "> We assume that image data consists of integers in $\\{0, 1, ... , 255\\}$ scaled linearly to $[âˆ’1, 1]$. This\n",
        "ensures that the neural network reverse process operates on consistently scaled inputs starting from\n",
        "the standard normal prior $p(\\mathbf{x}_T )$. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71aba861",
      "metadata": {
        "id": "71aba861"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import Compose, ToTensor, Lambda, ToPILImage, CenterCrop, Resize\n",
        "\n",
        "image_size = 128\n",
        "transform = Compose([\n",
        "    Resize(image_size),\n",
        "    CenterCrop(image_size),\n",
        "    ToTensor(), # turn into Numpy array of shape HWC, divide by 255\n",
        "    Lambda(lambda t: (t * 2) - 1),\n",
        "    \n",
        "])\n",
        "\n",
        "x_start = transform(image).unsqueeze(0)\n",
        "x_start.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e143cc62",
      "metadata": {
        "id": "e143cc62"
      },
      "source": [
        "<div class=\"output stream stdout\">\n",
        "\n",
        "    Output:\n",
        "    ----------------------------------------------------------------------------------------------------\n",
        "    torch.Size([1, 3, 128, 128])\n",
        "\n",
        "</div>\n",
        "\n",
        "We also define the reverse transform, which takes in a PyTorch tensor containing values in $[-1, 1]$ and turn them back into a PIL image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b98e91ff",
      "metadata": {
        "id": "b98e91ff"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "reverse_transform = Compose([\n",
        "     Lambda(lambda t: (t + 1) / 2),\n",
        "     Lambda(lambda t: t.permute(1, 2, 0)), # CHW to HWC\n",
        "     Lambda(lambda t: t * 255.),\n",
        "     Lambda(lambda t: t.numpy().astype(np.uint8)),\n",
        "     ToPILImage(),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6869ab7",
      "metadata": {
        "id": "f6869ab7"
      },
      "source": [
        "Let's verify this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "366a770c",
      "metadata": {
        "id": "366a770c"
      },
      "outputs": [],
      "source": [
        "reverse_transform(x_start.squeeze())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f38d30ab",
      "metadata": {
        "id": "f38d30ab"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1WT22KYvqJbHFdYYfkV7ohKNO4alnvesB\" width=\"100\" />\n",
        "\n",
        "We can now define the forward diffusion process as in the paper:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3752480",
      "metadata": {
        "id": "f3752480"
      },
      "outputs": [],
      "source": [
        "# forward diffusion\n",
        "def q_sample(x_start, t, noise=None):\n",
        "    if noise is None:\n",
        "        noise = torch.randn_like(x_start)\n",
        "\n",
        "    sqrt_alphas_cumprod_t = extract(sqrt_alphas_cumprod, t, x_start.shape)\n",
        "    sqrt_one_minus_alphas_cumprod_t = extract(\n",
        "        sqrt_one_minus_alphas_cumprod, t, x_start.shape\n",
        "    )\n",
        "\n",
        "    return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e82bac28",
      "metadata": {
        "id": "e82bac28"
      },
      "source": [
        "Let's test it on a particular time step:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bd64f89",
      "metadata": {
        "id": "6bd64f89"
      },
      "outputs": [],
      "source": [
        "def get_noisy_image(x_start, t):\n",
        "  # add noise\n",
        "  x_noisy = q_sample(x_start, t=t)\n",
        "\n",
        "  # turn back into PIL image\n",
        "  noisy_image = reverse_transform(x_noisy.squeeze())\n",
        "\n",
        "  return noisy_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52d22667",
      "metadata": {
        "id": "52d22667"
      },
      "outputs": [],
      "source": [
        "# take time step\n",
        "t = torch.tensor([40])\n",
        "\n",
        "get_noisy_image(x_start, t)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "003e1d95",
      "metadata": {
        "id": "003e1d95"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1Ra33wxuw3QxPlUG0iqZGtxgKBNdjNsqz\" width=\"100\" />\n",
        "\n",
        "Let's visualize this for various time steps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e27c37d",
      "metadata": {
        "id": "8e27c37d"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# use seed for reproducability\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# source: https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py\n",
        "def plot(imgs, with_orig=False, row_title=None, **imshow_kwargs):\n",
        "    if not isinstance(imgs[0], list):\n",
        "        # Make a 2d grid even if there's just 1 row\n",
        "        imgs = [imgs]\n",
        "\n",
        "    num_rows = len(imgs)\n",
        "    num_cols = len(imgs[0]) + with_orig\n",
        "    fig, axs = plt.subplots(figsize=(200,200), nrows=num_rows, ncols=num_cols, squeeze=False)\n",
        "    for row_idx, row in enumerate(imgs):\n",
        "        row = [image] + row if with_orig else row\n",
        "        for col_idx, img in enumerate(row):\n",
        "            ax = axs[row_idx, col_idx]\n",
        "            ax.imshow(np.asarray(img), **imshow_kwargs)\n",
        "            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
        "\n",
        "    if with_orig:\n",
        "        axs[0, 0].set(title='Original image')\n",
        "        axs[0, 0].title.set_size(8)\n",
        "    if row_title is not None:\n",
        "        for row_idx in range(num_rows):\n",
        "            axs[row_idx, 0].set(ylabel=row_title[row_idx])\n",
        "\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "323102d0",
      "metadata": {
        "id": "323102d0"
      },
      "outputs": [],
      "source": [
        "plot([get_noisy_image(x_start, torch.tensor([t])) for t in [0, 50, 100, 150, 199]])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a205c24",
      "metadata": {
        "id": "4a205c24"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1QifsBnYiijwTqru6gur9C0qKkFYrm-lN\" width=\"800\" />\n",
        "    \n",
        "This means that we can now define the loss function given the model as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7725f6cf",
      "metadata": {
        "id": "7725f6cf"
      },
      "outputs": [],
      "source": [
        "def p_losses(denoise_model, x_start, t, noise=None, loss_type=\"l1\"):\n",
        "    if noise is None:\n",
        "        noise = torch.randn_like(x_start)\n",
        "\n",
        "    x_noisy = q_sample(x_start=x_start, t=t, noise=noise)\n",
        "    predicted_noise = denoise_model(x_noisy, t)\n",
        "\n",
        "    if loss_type == 'l1':\n",
        "        loss = F.l1_loss(noise, predicted_noise)\n",
        "    elif loss_type == 'l2':\n",
        "        loss = F.mse_loss(noise, predicted_noise)\n",
        "    elif loss_type == \"huber\":\n",
        "        loss = F.smooth_l1_loss(noise, predicted_noise)\n",
        "    else:\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc01c63b",
      "metadata": {
        "id": "cc01c63b"
      },
      "source": [
        "The `denoise_model` will be our U-Net defined above. We'll employ the Huber loss between the true and the predicted noise.\n",
        "\n",
        "## Define a PyTorch Dataset + DataLoader\n",
        "\n",
        "Here we define a regular [PyTorch Dataset](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html). The dataset simply consists of images from a real dataset, like Fashion-MNIST, CIFAR-10 or ImageNet, scaled linearly to \\\\([âˆ’1, 1]\\\\).\n",
        "\n",
        "Each image is resized to the same size. Interesting to note is that images are also randomly horizontally flipped. From the paper:\n",
        "\n",
        "> We used random horizontal flips during training for CIFAR10; we tried training both with and without flips, and found flips to improve sample quality slightly.\n",
        "\n",
        "Here we use the ğŸ¤— [Datasets library](https://huggingface.co/docs/datasets/index) to easily load the Fashion MNIST dataset from the [hub](https://huggingface.co/datasets/fashion_mnist). This dataset consists of images which already have the same resolution, namely 28x28."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6134d691",
      "metadata": {
        "id": "6134d691"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# load dataset from the hub\n",
        "dataset = load_dataset(\"fashion_mnist\")\n",
        "image_size = 28\n",
        "channels = 1\n",
        "batch_size = 128"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db6f5875",
      "metadata": {
        "id": "db6f5875"
      },
      "source": [
        "Next, we define a function which we'll apply on-the-fly on the entire dataset. We use the `with_transform` [functionality](https://huggingface.co/docs/datasets/v2.2.1/en/package_reference/main_classes#datasets.Dataset.with_transform) for that. The function just applies some basic image preprocessing: random horizontal flips, rescaling and finally make them have values in the $[-1,1]$ range."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3e78945",
      "metadata": {
        "id": "b3e78945"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# define image transformations (e.g. using torchvision)\n",
        "transform = Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Lambda(lambda t: (t * 2) - 1)\n",
        "])\n",
        "\n",
        "# define function\n",
        "def transforms(examples):\n",
        "   examples[\"pixel_values\"] = [transform(image.convert(\"L\")) for image in examples[\"image\"]]\n",
        "   del examples[\"image\"]\n",
        "\n",
        "   return examples\n",
        "\n",
        "transformed_dataset = dataset.with_transform(transforms).remove_columns(\"label\")\n",
        "\n",
        "# create dataloader\n",
        "dataloader = DataLoader(transformed_dataset[\"train\"], batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52e8273b",
      "metadata": {
        "id": "52e8273b"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(dataloader))\n",
        "print(batch.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4325faf",
      "metadata": {
        "id": "e4325faf"
      },
      "source": [
        "<div class=\"output stream stdout\">\n",
        "\n",
        "    Output:\n",
        "    ----------------------------------------------------------------------------------------------------\n",
        "    dict_keys(['pixel_values'])\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cf98443",
      "metadata": {
        "id": "4cf98443"
      },
      "source": [
        "## Sampling\n",
        "\n",
        "As we'll sample from the model during training (in order to track progress), we define the code for that below. Sampling is summarized in the paper as Algorithm 2:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1ij80f8TNBDzpKtqHjk_sh8o5aby3lmD7\" width=\"500\" />\n",
        "\n",
        "Generating new images from a diffusion model happens by reversing the diffusion process: we start from $T$, where we sample pure noise from a Gaussian distribution, and then use our neural network to gradually denoise it (using the conditional probability it has learned), until we end up at time step $t = 0$. As shown above, we can derive a slighly less denoised image $\\mathbf{x}_{t-1 }$ by plugging in the reparametrization of the mean, using our noise predictor. Remember that the variance is known ahead of time.\n",
        "\n",
        "Ideally, we end up with an image that looks like it came from the real data distribution.\n",
        "\n",
        "The code below implements this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7628fb3",
      "metadata": {
        "id": "f7628fb3"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def p_sample(model, x, t, t_index):\n",
        "    betas_t = extract(betas, t, x.shape)\n",
        "    sqrt_one_minus_alphas_cumprod_t = extract(\n",
        "        sqrt_one_minus_alphas_cumprod, t, x.shape\n",
        "    )\n",
        "    sqrt_recip_alphas_t = extract(sqrt_recip_alphas, t, x.shape)\n",
        "    \n",
        "    # Equation 11 in the paper\n",
        "    # Use our model (noise predictor) to predict the mean\n",
        "    model_mean = sqrt_recip_alphas_t * (\n",
        "        x - betas_t * model(x, t) / sqrt_one_minus_alphas_cumprod_t\n",
        "    )\n",
        "\n",
        "    if t_index == 0:\n",
        "        return model_mean\n",
        "    else:\n",
        "        posterior_variance_t = extract(posterior_variance, t, x.shape)\n",
        "        noise = torch.randn_like(x)\n",
        "        # Algorithm 2 line 4:\n",
        "        return model_mean + torch.sqrt(posterior_variance_t) * noise \n",
        "\n",
        "# Algorithm 2 but save all images:\n",
        "@torch.no_grad()\n",
        "def p_sample_loop(model, shape):\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    b = shape[0]\n",
        "    # start from pure noise (for each example in the batch)\n",
        "    img = torch.randn(shape, device=device)\n",
        "    imgs = []\n",
        "    \n",
        "    for i in tqdm(reversed(range(0, timesteps)), desc='sampling loop time step', total=timesteps):\n",
        "        img = p_sample(model, img, torch.full((b,), i, device=device, dtype=torch.long), i)\n",
        "        imgs.append(img.cpu().numpy())\n",
        "    return imgs\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample(model, image_size, batch_size=16, channels=3):\n",
        "    return p_sample_loop(model, shape=(batch_size, channels, image_size, image_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f70235f8",
      "metadata": {
        "id": "f70235f8"
      },
      "source": [
        "\n",
        "Note that the code above is a simplified version of the original implementation. We found our simplification (which is in line with Algorithm 2 in the paper) to work just as well as the [original, more complex implementation](https://github.com/hojonathanho/diffusion/blob/master/diffusion_tf/diffusion_utils.py).\n",
        "\n",
        "\n",
        "## è®­ç»ƒ\n",
        "\n",
        "Next, we train the model in regular PyTorch fashion. We also define some logic to peridiocally save generated images, using the `sample` method defined above.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c1ad663",
      "metadata": {
        "id": "0c1ad663"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def num_to_groups(num, divisor):\n",
        "    groups = num // divisor\n",
        "    remainder = num % divisor\n",
        "    arr = [divisor] * groups\n",
        "    if remainder > 0:\n",
        "        arr.append(remainder)\n",
        "    return arr\n",
        "\n",
        "results_folder = Path(\"./results\")\n",
        "results_folder.mkdir(exist_ok = True)\n",
        "save_and_sample_every = 1000"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22e4c0fd",
      "metadata": {
        "id": "22e4c0fd"
      },
      "source": [
        "\n",
        "å®šä¹‰å¥½æ¨¡å‹ï¼Œå°†å…¶ä¸¢åˆ°GPUä¸Šï¼Œä½¿ç”¨Adamè¿›è¡Œä¼˜åŒ–ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5126e21",
      "metadata": {
        "id": "a5126e21"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = Unet(\n",
        "    dim=image_size,\n",
        "    channels=channels,\n",
        "    dim_mults=(1, 2, 4,)\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7444b0b",
      "metadata": {
        "id": "f7444b0b"
      },
      "source": [
        "å¼€å§‹è®­ç»ƒ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92b12ed1",
      "metadata": {
        "id": "92b12ed1"
      },
      "outputs": [],
      "source": [
        "from torchvision.utils import save_image\n",
        "\n",
        "epochs = 5\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for step, batch in enumerate(dataloader):\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      batch_size = batch[\"pixel_values\"].shape[0]\n",
        "      batch = batch[\"pixel_values\"].to(device)\n",
        "\n",
        "      # Algorithm 1 line 3: sample t uniformally for every example in the batch\n",
        "      t = torch.randint(0, timesteps, (batch_size,), device=device).long()\n",
        "\n",
        "      loss = p_losses(model, batch, t, loss_type=\"huber\")\n",
        "\n",
        "      if step % 100 == 0:\n",
        "        print(\"Loss:\", loss.item())\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # save generated images\n",
        "      if step != 0 and step % save_and_sample_every == 0:\n",
        "        milestone = step // save_and_sample_every\n",
        "        batches = num_to_groups(4, batch_size)\n",
        "        all_images_list = list(map(lambda n: sample(model, batch_size=n, channels=channels), batches))\n",
        "        all_images = torch.cat(all_images_list, dim=0)\n",
        "        all_images = (all_images + 1) * 0.5\n",
        "        save_image(all_images, str(results_folder / f'sample-{milestone}.png'), nrow = 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e617a66a",
      "metadata": {
        "id": "e617a66a"
      },
      "source": [
        "<div class=\"output stream stdout\">\n",
        "\n",
        "    Output:\n",
        "    ----------------------------------------------------------------------------------------------------\n",
        "    Loss: 0.46477368474006653\n",
        "    Loss: 0.12143351882696152\n",
        "    Loss: 0.08106148988008499\n",
        "    Loss: 0.0801810547709465\n",
        "    Loss: 0.06122320517897606\n",
        "    Loss: 0.06310459971427917\n",
        "    Loss: 0.05681884288787842\n",
        "    Loss: 0.05729678273200989\n",
        "    Loss: 0.05497899278998375\n",
        "    Loss: 0.04439849033951759\n",
        "    Loss: 0.05415581166744232\n",
        "    Loss: 0.06020551547408104\n",
        "    Loss: 0.046830907464027405\n",
        "    Loss: 0.051029372960329056\n",
        "    Loss: 0.0478244312107563\n",
        "    Loss: 0.046767622232437134\n",
        "    Loss: 0.04305662214756012\n",
        "    Loss: 0.05216279625892639\n",
        "    Loss: 0.04748568311333656\n",
        "    Loss: 0.05107741802930832\n",
        "    Loss: 0.04588869959115982\n",
        "    Loss: 0.043014321476221085\n",
        "    Loss: 0.046371955424547195\n",
        "    Loss: 0.04952816292643547\n",
        "    Loss: 0.04472338408231735\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8337c82",
      "metadata": {
        "id": "a8337c82"
      },
      "source": [
        "## é‡‡æ ·/æ¨ç†\n",
        "\n",
        "To sample from the model, we can just use our sample function defined above:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3d8a814",
      "metadata": {
        "id": "f3d8a814"
      },
      "outputs": [],
      "source": [
        "# sample 64 images\n",
        "samples = sample(model, image_size=image_size, batch_size=64, channels=channels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_s-Al2lJ2c8T",
      "metadata": {
        "id": "_s-Al2lJ2c8T"
      },
      "outputs": [],
      "source": [
        "# show a random one\n",
        "random_index = 5\n",
        "plt.imshow(samples[-1][random_index].reshape(image_size, image_size, channels), cmap=\"gray\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26ad579f",
      "metadata": {
        "id": "26ad579f"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1ytnzS7IW7ortC6ub85q7nud1IvXe2QTE\" width=\"300\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0k4H1fmlKvzR",
      "metadata": {
        "id": "0k4H1fmlKvzR"
      },
      "source": [
        "Seems like the model is capable of generating a nice T-shirt! Keep in mind that the dataset we trained on is pretty low-resolution (28x28).\n",
        "\n",
        "We can also create a gif of the denoising process:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "spE1I9aVNwzZ",
      "metadata": {
        "id": "spE1I9aVNwzZ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.animation as animation\n",
        "\n",
        "random_index = 53\n",
        "\n",
        "fig = plt.figure()\n",
        "ims = []\n",
        "for i in range(timesteps):\n",
        "    im = plt.imshow(samples[i][random_index].reshape(image_size, image_size, channels), cmap=\"gray\", animated=True)\n",
        "    ims.append([im])\n",
        "\n",
        "animate = animation.ArtistAnimation(fig, ims, interval=50, blit=True, repeat_delay=1000)\n",
        "animate.save('diffusion.gif')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b02eb802",
      "metadata": {
        "id": "b02eb802"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1eyonQWhfmbQsTq8ndsNjw5QSRQ9em9Au\" width=\"500\" />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# åç»­é˜…è¯»ğŸ“•\n",
        "\n",
        "- Improved Denoising Diffusion Probabilistic Models ([Nichol et al., 2021](https://arxiv.org/abs/2102.09672)): å­¦ä¹ æ¡ä»¶åˆ†å¸ƒçš„æ–¹å·®ï¼ˆé™¤å‡å€¼å¤–ï¼‰æœ‰åŠ©äºæé«˜æ€§èƒ½ã€‚\n",
        "\n",
        "- Cascaded Diffusion Models for High Fidelity Image Generation ([Ho et al., 2021](https://arxiv.org/abs/2106.15282)): å¼•å…¥çº§è”æ‰©æ•£ï¼ŒåŒ…æ‹¬å¤šä¸ªæ‰©æ•£æ¨¡å‹çš„pipelineï¼Œç”¨äºç”Ÿæˆé€æ­¥æé«˜åˆ†è¾¨ç‡çš„å›¾åƒï¼Œå®ç°é«˜ä¿çœŸåº¦å›¾åƒåˆæˆã€‚\n",
        "\n",
        "- Diffusion Models Beat GANs on Image Synthesis ([Dhariwal et al., 2021](https://arxiv.org/abs/2105.05233)): é€šè¿‡æ”¹è¿›U-Netæ¶æ„ï¼Œå¹¶å¼•å…¥åˆ†ç±»å™¨æŒ‡å¯¼ï¼Œè¯æ˜äº†æ‰©æ•£æ¨¡å‹å¯ä»¥å®ç°æ¯”å½“å‰æœ€å…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹æ›´å¥½çš„å›¾åƒæ ·æœ¬è´¨é‡ã€‚\n",
        "\n",
        "- Classifier-Free Diffusion Guidance ([Ho et al., 2021](https://openreview.net/pdf?id=qw8AKxfYbI)): é€šè¿‡è”åˆè®­ç»ƒä¸€ä¸ªæ¡ä»¶å’Œä¸€ä¸ªæ— æ¡ä»¶æ‰©æ•£æ¨¡å‹çš„å•ä¸ªç¥ç»ç½‘ç»œï¼Œå±•ç¤ºäº†æŒ‡å¯¼æ‰©æ•£æ¨¡å‹ä¸éœ€è¦åˆ†ç±»å™¨ã€‚\n",
        "\n",
        "- Hierarchical Text-Conditional Image Generation with CLIP Latents (DALL-E 2) ([Ramesh et al., 2022](https://cdn.openai.com/papers/dall-e-2.pdf)): ä½¿ç”¨å…ˆéªŒå°†æ–‡æœ¬æ ‡é¢˜è½¬åŒ–ä¸ºCLIPå›¾åƒåµŒå…¥ï¼Œç„¶åæ‰©æ•£æ¨¡å‹å°†å…¶è§£ç æˆå›¾åƒã€‚\n",
        "\n",
        "- Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding (ImageGen) ([Saharia et al., 2022](https://arxiv.org/abs/2205.11487)): å°†å¤§å‹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆä¾‹å¦‚T5ï¼‰ä¸çº§è”æ‰©æ•£ç›¸ç»“åˆï¼Œå¯ä»¥å¾ˆå¥½åœ°å®ç°æ–‡æœ¬åˆ°å›¾åƒçš„åˆæˆã€‚\n",
        "\n",
        "\n",
        "ç›®å‰ï¼Œæ‰©æ•£æ¨¡å‹çš„ä¸»è¦ï¼ˆæˆ–è®¸å”¯ä¸€çš„ï¼‰ç¼ºç‚¹ä¼¼ä¹æ˜¯éœ€è¦å¤šæ¬¡å‰å‘ä¼ é€’æ‰èƒ½ç”Ÿæˆä¸€å¼ å›¾åƒï¼ˆè€Œå¯¹äºGANç­‰ç”Ÿæˆæ¨¡å‹åˆ™ä¸éœ€è¦ï¼‰ã€‚ç„¶è€Œï¼Œ[Zhang et al., 2022](https://arxiv.org/abs/2204.13902)å¯ä»¥åœ¨ä¸åˆ°10ä¸ªå»å™ªæ­¥éª¤å†…å®ç°é«˜ä¿çœŸåº¦çš„ç”Ÿæˆã€‚"
      ],
      "metadata": {
        "id": "vTBA0onvwwil"
      },
      "id": "vTBA0onvwwil"
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "6fe49a34",
        "2d747688",
        "5153024b",
        "592aa765",
        "9ff47fbb",
        "51d9a24c",
        "9a8031b0",
        "06b3fad0",
        "a30368b2",
        "cc01c63b",
        "f70235f8",
        "b02eb802"
      ],
      "name": "annotated-diffusion.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}