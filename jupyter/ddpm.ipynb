{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LolitaSian/DiffusionModel/blob/main/jupyter/ddpm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5a94671",
      "metadata": {
        "id": "c5a94671"
      },
      "source": [
        "<h1>\n",
        "\tThe Annotated Diffusion Model\n",
        "</h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "290edb0b",
      "metadata": {
        "id": "290edb0b"
      },
      "source": [
        "\n",
        "åœ¨è¿™ç¯‡åšå®¢æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†é€æ­¥è®²è§£([Ho et al., 2020](https://arxiv.org/abs/2006.11239))çš„åŸå§‹DDPMè®ºæ–‡ï¼Œå¹¶åŸºäº[Phil Wangçš„TensorFlowç‰ˆæœ¬]((https://github.com/lucidrains/denoising-diffusion-pytorch))å®ç°Pytorchç‰ˆæœ¬ã€‚è¯·æ³¨æ„ï¼Œæ‰©æ•£ç”¨äºç”Ÿæˆå»ºæ¨¡çš„æ€æƒ³å®é™…ä¸Šå·²ç»åœ¨([Sohl-Dickstein et al., 2015](https://arxiv.org/abs/1503.03585))ä¸­ä»‹ç»è¿‡ï¼Œä½†æ˜¯ï¼Œç›´åˆ°æ–¯å¦ç¦å¤§å­¦çš„([Song et al., 2019](https://arxiv.org/abs/1907.05600))å’Œè°·æ­Œå¤§è„‘çš„([Ho et al., 2020](https://arxiv.org/abs/2006.11239))åˆ†åˆ«æ”¹è¿›äº†è¿™ç§æ–¹æ³•æ‰å¾—ä»¥æµè¡Œèµ·æ¥ã€‚\n",
        "\n",
        "[æ‰©æ•£æ¨¡å‹æœ‰å‡ ä¸ªè§†è§’](https://twitter.com/sedielem/status/1530894256168222722?s=20&t=mfv4afx1GcNQU5fZklpACw)ã€‚åœ¨è¿™é‡Œæˆ‘ä»¬é‡‡ç”¨ç¦»æ•£æ—¶é—´ï¼ˆæ½œå˜é‡æ¨¡å‹ï¼‰çš„è§†è§’."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1f2d714",
      "metadata": {
        "id": "a1f2d714"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U einops datasets matplotlib tqdm\n",
        "\n",
        "import math\n",
        "from inspect import isfunction\n",
        "from functools import partial\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "from einops import rearrange\n",
        "\n",
        "import torch\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fe49a34",
      "metadata": {
        "id": "6fe49a34"
      },
      "source": [
        "\n",
        "## ä»€ä¹ˆæ˜¯æ‰©æ•£æ¨¡å‹\n",
        "\n",
        "Diffusion model å’Œ Normalizing Flows, GANs or VAEs ä¸€æ ·ï¼Œéƒ½æ˜¯å°†å™ªå£°ä»ä¸€äº›ç®€å•çš„åˆ†å¸ƒè½¬æ¢ä¸ºä¸€ä¸ªæ•°æ®æ ·æœ¬ï¼Œä¹Ÿæ˜¯ç¥ç»ç½‘ç»œå­¦ä¹ ä»çº¯å™ªå£°å¼€å§‹é€æ¸å»å™ªæ•°æ®çš„è¿‡ç¨‹ã€‚\n",
        "åŒ…å«ä¸¤ä¸ªæ­¥éª¤ï¼š\n",
        "- ä¸€ä¸ªæˆ‘ä»¬é€‰æ‹©çš„å›ºå®šçš„ï¼ˆæˆ–è€…è¯´é¢„å®šä¹‰å¥½çš„ï¼‰å‰å‘æ‰©æ•£è¿‡ç¨‹ $q$ ï¼Œå°±æ˜¯é€æ¸ç»™å›¾ç‰‡æ·»åŠ é«˜æ–¯å™ªå£°ï¼Œç›´åˆ°æœ€åè·å¾—çº¯å™ªå£°ã€‚\n",
        "\n",
        "- ä¸€ä¸ªéœ€è¦å­¦ä¹ çš„åå‘çš„å»å™ªè¿‡ç¨‹ $p_\\theta$ï¼Œè®­ç»ƒä¸€ä¸ªç¥ç»ç½‘åšå›¾åƒå»å™ªï¼Œä»çº¯å™ªå£°å¼€å§‹ï¼Œç›´åˆ°è·å¾—æœ€ç»ˆå›¾åƒã€‚\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?id=1t5dUyJwgy2ZpDAqHXw7GhUAp2FE5BWHA\" width=\"600\" />\n",
        "</p>\n",
        "\n",
        "å‰å‘å’Œåå‘è¿‡ç¨‹éƒ½è¦ç»è¿‡æ—¶é—´æ­¥$t$ï¼Œæ€»æ­¥é•¿æ˜¯$T$ï¼ˆDDPMä¸­$T=1000$)ã€‚\n",
        "\n",
        "ä»$t=0$å¼€å§‹ï¼Œä»æ•°æ®é›†åˆ†å¸ƒä¸­é‡‡æ ·ä¸€ä¸ªçœŸå®å›¾ç‰‡$\\mathbf x_0$ã€‚å‰å‘è¿‡ç¨‹å°±æ˜¯åœ¨æ¯ä¸€ä¸ªæ—¶é—´æ­¥$t$ä¸­éƒ½ä»ä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ ·ä¸€ä¸ªå™ªå£°ï¼Œå°†å…¶æ·»åŠ åˆ°ä¸Šä¸€æ—¶é—´æ­¥çš„å›¾åƒä¸Šã€‚ç»™å‡ºä¸€ä¸ªè¶³å¤Ÿå¤§çš„$T$ï¼Œå’Œæ¯ä¸€æ—¶é—´æ­¥ä¸­æ·»åŠ å™ªå£°çš„è¡¨æ ¼ï¼Œæœ€ç»ˆåœ¨$T$æ—¶é—´æ­¥ä½ ä¼šè·å¾—ä¸€ä¸ª[isotropic Gaussian distribution](https://math.stackexchange.com/questions/1991961/gaussian-distribution-is-isotropic)ã€‚\n",
        "\n",
        "\n",
        "\n",
        "## In more mathematical form\n",
        "\n",
        "\n",
        "æˆ‘ä»¬ä»¤$q(\\mathbf x_0)$æ˜¯çœŸå®åˆ†å¸ƒï¼Œä¹Ÿå°±æ˜¯çœŸå®çš„å›¾åƒçš„åˆ†å¸ƒã€‚\n",
        "\n",
        "æˆ‘ä»¬å¯ä»¥ä»ä¸­é‡‡æ ·ä¸€ä¸ªå›¾ç‰‡ï¼Œä¹Ÿå°±æ˜¯$\\mathbf x_0 \\sim q(\\mathbf x_0)$ ã€‚\n",
        "\n",
        "æˆ‘ä»¬è®¾å®šå‰å‘æ‰©æ•£è¿‡ç¨‹$q(\\mathbf x_t|\\mathbf x_{t-1})$æ˜¯ç»™æ¯ä¸ªæ—¶é—´æ­¥$t$æ·»åŠ é«˜æ–¯å™ªå£°ï¼Œè¿™ä¸ªé«˜æ–¯å™ªå£°ä¸æ˜¯éšæœºé€‰æ‹©çš„ï¼Œæ˜¯æ ¹æ®æˆ‘ä»¬é¢„é€‰è®¾å®šå¥½çš„æ–¹å·®è¡¨ï¼ˆ$0 < \\beta_1 < \\beta_2 < ... < \\beta_T < 1$ï¼‰çš„é«˜æ–¯åˆ†å¸ƒä¸­è·å–çš„ã€‚\n",
        "\n",
        "ç„¶åæˆ‘ä»¬å°±å¯ä»¥å¾—åˆ°å‰å‘è¿‡ç¨‹çš„å…¬å¼ä¸ºï¼š\n",
        "$$\n",
        "q(\\mathbf {x}_t | \\mathbf {x}_{t-1}) = \\mathcal{N}(\\mathbf {x}_t; \\sqrt{1 - \\beta_t} \\mathbf {x}_{t-1}, \\beta_t \\mathbf{I}). \n",
        "$$\n",
        "\n",
        "å›æƒ³ä¸€ä¸‹å“¦ã€‚ä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒï¼ˆä¹Ÿå«æ­£æ€åˆ†å¸ƒï¼‰æ˜¯ç”±ä¸¤ä¸ªå‚æ•°å†³å®šçš„ï¼Œå‡å€¼$\\mu$å’Œæ–¹å·®$\\sigma^2 \\geq 0$ã€‚\n",
        "\n",
        "ç„¶åæˆ‘ä»¬å°±å¯ä»¥è®¤ä¸ºæ¯ä¸ªæ—¶é—´æ­¥$t$çš„å›¾åƒæ˜¯ä»ä¸€å‡å€¼ä¸º${\\mu}_t = \\sqrt{1 - \\beta_t} \\mathbf {x}_{t-1}$ã€æ–¹å·®ä¸º$\\sigma^2_t = \\beta_t$çš„æ¡ä»¶é«˜æ–¯åˆ†å¸ƒä¸­ç”»å‡ºæ¥çš„ã€‚å€ŸåŠ©å‚æ•°é‡æ•´åŒ–ï¼ˆreparameterization trickï¼‰å¯ä»¥å†™æˆ\n",
        "\n",
        "$$\n",
        "\\mathbf {x}_t = \\sqrt{1 - \\beta_t}\\mathbf {x}_{t-1} +  \\sqrt{\\beta_t} \\mathbf{\\epsilon}\n",
        "$$\n",
        "\n",
        "å…¶ä¸­$\\mathbf{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ï¼Œæ˜¯ä»æ ‡å‡†é«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ ·çš„å™ªå£°ã€‚ \n",
        "\n",
        "$\\beta_t$åœ¨ä¸åŒçš„æ—¶é—´æ­¥$t$ä¸­ä¸æ˜¯å›ºå®šçš„ï¼Œå› æ­¤æˆ‘ä»¬ç»™$\\beta$åŠ äº†ä¸‹æ ‡ã€‚å¯¹äº$\\beta_t$çš„é€‰æ‹©æˆ‘ä»¬å¯ä»¥è®¾ç½®ä¸ºçº¿æ€§çš„ã€äºŒæ¬¡çš„ã€ä½™å¼¦çš„ç­‰(æœ‰ç‚¹åƒå­¦ä¹ ç‡è®¡åˆ’)ã€‚\n",
        "\n",
        "æ¯”å¦‚åœ¨DDPMä¸­$\\beta_1 = 10^{-4}$, $\\beta_T = 0.02$ï¼Œåœ¨ä¸­é—´æ˜¯åšäº†ä¸€ä¸ªçº¿æ€§æ’å€¼ã€‚è€Œåœ¨Improved DDPMä¸­æ˜¯ä½¿ç”¨ä½™å¼¦å‡½æ•°ã€‚\n",
        "\n",
        "ä»$\\mathbf x_0$å¼€å§‹ï¼Œæˆ‘ä»¬é€šè¿‡$\\mathbf{x}_1,  ..., \\mathbf{x}_t, ..., \\mathbf{x}_T$,æœ€ç»ˆè·å¾—$\\mathbf{x}_T$ ï¼Œå¦‚æœæˆ‘ä»¬çš„é«˜æ–¯å™ªå£°è¡¨è®¾ç½®çš„åˆç†ï¼Œé‚£æœ€åæˆ‘ä»¬è·å¾—çš„åº”è¯¥æ˜¯ä¸€ä¸ªçº¯é«˜æ–¯å™ªå£°ã€‚\n",
        "\n",
        "ç°åœ¨ï¼Œå¦‚æœæˆ‘ä»¬èƒ½çŸ¥é“æ¡ä»¶åˆ†å¸ƒ$p(\\mathbf {x}_{t-1} | \\mathbf {x}_t)$ï¼Œé‚£æˆ‘ä»¬å°±å¯ä»¥å°†è¿™ä¸ªè¿‡ç¨‹å€’è¿‡æ¥ï¼šé‡‡æ ·ä¸€ä¸ªéšæœºé«˜æ–¯å™ªå£°$\\mathbf x_t$ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹å…¶é€æ­¥å»å™ªï¼Œæœ€ç»ˆå¾—åˆ°ä¸€ä¸ªçœŸå®åˆ†å¸ƒçš„å›¾ç‰‡$\\mathbf x_0$ã€‚\n",
        "\n",
        "ä½†æ˜¯æˆ‘ä»¬å®é™…ä¸Šæ²¡åŠæ³•çŸ¥é“$p(\\mathbf {x}_{t-1} | \\mathbf {x}_t)$ã€‚å› ä¸ºå®ƒéœ€è¦çŸ¥é“æ‰€æœ‰å¯èƒ½å›¾åƒçš„åˆ†å¸ƒæ¥è®¡ç®—è¿™ä¸ªæ¡ä»¶æ¦‚ç‡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦å€ŸåŠ©ç¥ç»ç½‘ç»œæ¥è¿‘ä¼¼(å­¦ä¹ )è¿™ä¸ªæ¡ä»¶æ¦‚ç‡åˆ†å¸ƒã€‚ ä¹Ÿå°±æ˜¯$p_\\theta (\\mathbf {x}_{t-1} | \\mathbf {x}_t)$ï¼Œå…¶ä¸­, $\\theta$æ˜¯ç¥ç»ç½‘ç»œçš„å‚æ•°ï¼Œéœ€è¦ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ›´æ–°ã€‚\n",
        "\n",
        "\n",
        "æ‰€ä»¥ç°åœ¨æˆ‘ä»¬éœ€è¦ä¸€ä¸ªç¥ç»ç½‘ç»œæ¥è¡¨ç¤ºé€†å‘è¿‡ç¨‹çš„(æ¡ä»¶)æ¦‚ç‡åˆ†å¸ƒã€‚å¦‚æœæˆ‘ä»¬å‡è®¾è¿™ä¸ªåå‘è¿‡ç¨‹ä¹Ÿæ˜¯é«˜æ–¯åˆ†å¸ƒï¼Œé‚£ä¹ˆå›æƒ³ä¸€ä¸‹ï¼Œä»»ä½•é«˜æ–¯åˆ†å¸ƒéƒ½æ˜¯ç”±ä¸¤ä¸ªå‚æ•°å®šä¹‰çš„:\n",
        "\n",
        "* ä¸€ä¸ªå‡å€¼$\\mu_\\theta$;\n",
        "* ä¸€ä¸ªæ–¹å·®$\\Sigma_\\theta$ã€‚\n",
        "\n",
        "æ‰€ä»¥æˆ‘ä»¬å¯ä»¥æŠŠè¿™ä¸ªè¿‡ç¨‹å‚æ•°åŒ–ä¸º\n",
        "\n",
        "$$\n",
        "p_\\theta (\\mathbf{x}_{t-1} | \\mathbf{x}_t) = \\mathcal{N}(\\mathbf{x}_{t-1}; \\mu_\\theta(\\mathbf{x}_{t},t), \\Sigma_\\theta (\\mathbf{x}_{t},t))\n",
        "$$\n",
        "\n",
        "å…¶ä¸­å‡å€¼å’Œæ–¹å·®ä¹Ÿå–å†³äºå™ªå£°æ°´å¹³$t$ã€‚\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "ä»ä¸Šè¾¹æˆ‘ä»¬å¯ä»¥çŸ¥é“ï¼Œé€†å‘è¿‡ç¨‹æˆ‘ä»¬éœ€è¦ä¸€ä¸ªç¥ç»ç½‘ç»œæ¥å­¦ä¹ ï¼ˆè¡¨ç¤ºï¼‰é«˜æ–¯åˆ†å¸ƒçš„å‡å€¼å’Œæ–¹å·®ã€‚\n",
        "\n",
        "DDPMä¸­ä½œè€…å›ºå®šæ–¹å·®ï¼Œåªè®©ç¥ç»ç½‘ç»œå­¦ä¹ æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒçš„å‡å€¼ã€‚\n",
        "\n",
        "\n",
        "> First, we set $\\Sigma_\\theta ( \\mathbf{x}_t, t) = \\sigma^2_t \\mathbf{I}$ to untrained time dependent constants. Experimentally, both $\\sigma^2_t = \\beta_t$ and $\\sigma^2_t  = \\tilde{\\beta}_t$ (see paper) had similar results. \n",
        "\n",
        "[Improved diffusion models](https://openreview.net/pdf?id=-NEXDKk8gZ)è¿™ç¯‡æ–‡ç« ä¸­è¿›è¡Œäº†æ”¹è¿›ï¼Œç¥ç»ç½‘ç»œæ—¢éœ€è¦å­¦ä¹ å‡å€¼ä¹Ÿè¦å­¦ä¹ æ–¹å·®ã€‚"
      ],
      "metadata": {
        "id": "5wjIt_ftQW6d"
      },
      "id": "5wjIt_ftQW6d"
    },
    {
      "cell_type": "markdown",
      "id": "2d747688",
      "metadata": {
        "id": "2d747688"
      },
      "source": [
        "\n",
        "\n",
        "## é€šè¿‡é‡æ–°å‚æ•°åŒ–å‡å€¼ å®šä¹‰ç›®æ ‡å‡½æ•°\n",
        "\n",
        "ä¸ºäº†æ¨å¯¼å‡ºä¸€ä¸ªç›®æ ‡å‡½æ•°æ¥å­¦ä¹ é€†å‘è¿‡ç¨‹çš„å‡å€¼ï¼Œä½œè€…è§‚å¯Ÿåˆ°$q$å’Œ$p_\\theta$å¯ä»¥çœ‹åšæ˜¯ä¸€ä¸ªVAEæ¨¡å‹ [(Kingma et al., 2013)](https://arxiv.org/abs/1312.6114). \n",
        "\n",
        "å› æ­¤ï¼Œå˜åˆ†ä¸‹ç•Œï¼ˆELBOï¼‰å¯ä»¥ç”¨æ¥æœ€å°åŒ–å…³äºground truth $\\mathbf x_0$çš„è´Ÿå¯¹æ•°ä¼¼ç„¶ã€‚\n",
        "\n",
        "è¿™ä¸ªè¿‡ç¨‹çš„ELBOæ˜¯æ¯ä¸ªæ—¶é—´æ­¥$t$çš„æŸå¤±æ€»å’Œï¼š$L=L_0+L_1+â€¦+L_ğ‘‡$ã€‚\n",
        "\n",
        "é€šè¿‡æ„å»ºæ­£å‘$q$è¿‡ç¨‹å’Œåå‘è¿‡ç¨‹ï¼ŒæŸå¤±çš„æ¯ä¸€é¡¹ï¼Œé™¤äº†$L_0$ï¼Œéƒ½æ˜¯ä¸¤ä¸ªé«˜æ–¯åˆ†å¸ƒä¹‹é—´çš„KLæ•£åº¦ï¼Œå¹¶ä¸”å¯ä»¥å†™ä¸ºå…³äºå‡å€¼çš„$L_2$æŸå¤±!\n",
        "\n",
        "\n",
        "å› ä¸ºé«˜æ–¯åˆ†å¸ƒçš„ç‰¹æ€§ï¼Œæˆ‘ä»¬ä¸éœ€è¦åœ¨æ­£å‘$q$è¿‡ç¨‹ä¸­é€æ­¥æ·»åŠ $t$æ­¥é•¿çš„å™ªå£°ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥è·å¾—$x_t$çš„ç»“æœï¼š\n",
        "\n",
        "$$\n",
        "q(\\mathbf {x}_t | \\mathbf {x}_0) = \\cal{N}(\\mathbf {x}_t; \\sqrt{\\bar{\\alpha}_t} \\mathbf {x}_0, (1- \\bar{\\alpha}_t) \\mathbf{I})\n",
        "$$\n",
        "\n",
        "å…¶ä¸­$\\alpha_t := 1 - \\beta_t$ and $\\bar{\\alpha}_t := \\Pi_{s=1}^{t} \\alpha_s$ã€‚\n",
        "\n",
        "è¿™æ˜¯ä¸€ä¸ªå¾ˆä¼˜ç§€çš„ç‰¹æ€§ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥å¯¹é«˜æ–¯å™ªå£°è¿›è¡Œé‡‡æ ·å¹¶é€‚å½“ç¼©æ”¾ç›´æ¥å°†å…¶æ·»åŠ åˆ°$\\mathbf x_0$ä¸­å°±å¯ä»¥ç›´æ¥å¾—åˆ°$\\mathbf x_t$ã€‚\n",
        "\n",
        "$\\bar{\\alpha}_t$æ˜¯æ–¹å·®è¡¨$\\beta_t$çš„å‡½æ•°ï¼Œå› æ­¤ä¹Ÿæ˜¯å·²çŸ¥çš„ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹å…¶é¢„å…ˆè®¡ç®—ã€‚è¿™æ ·å¯ä»¥è®©æˆ‘ä»¬åœ¨è®­ç»ƒæœŸé—´ä¼˜åŒ–æŸå¤±å‡½æ•°$L$çš„éšæœºé¡¹ï¼ˆæ¢å¥è¯è¯´ï¼Œåœ¨è®­ç»ƒæœŸé—´éšæœºé‡‡æ ·$t$å°±å¯ä»¥ä¼˜åŒ–$L_t$ï¼‰ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68574c28",
      "metadata": {
        "id": "68574c28"
      },
      "source": [
        "\n",
        "\n",
        "è¿™ä¸ªå±æ€§çš„å¦ä¸€ä¸ªä¼˜ç¾ä¹‹å¤„æ˜¯é€šè¿‡é‡æ–°å‚æ•°åŒ–å¹³å‡å€¼ï¼Œä½¿ç¥ç»ç½‘ç»œå­¦ä¹ ï¼ˆé¢„æµ‹ï¼‰æ·»åŠ çš„å™ªå£°ã€‚\n",
        "\n",
        "é€šè¿‡ç¥ç»ç½‘ç»œ$\\epsilon_\\theta(\\mathbf x_tï¼Œt)$é¢„æµ‹å™ªå£°ï¼Œå¯ä»¥æ„æˆæŸå¤±å‡½æ•°ä¸­æ—¶é—´æ­¥$t$çš„KLé¡¹ã€‚\n",
        "\n",
        "è¿™æ„å‘³ç€æˆ‘ä»¬çš„ç¥ç»ç½‘ç»œå˜æˆäº†å™ªå£°é¢„æµ‹å™¨ï¼Œè€Œä¸æ˜¯ç›´æ¥å»é¢„æµ‹å‡å€¼äº†ã€‚\n",
        "\n",
        "å‡å€¼çš„è®¡ç®—æ–¹æ³•å¦‚ä¸‹:\n",
        "\n",
        "$$ \\mathbf{\\mu}_\\theta(\\mathbf{x}_t, t) = \\frac{1}{\\sqrt{\\alpha_t}} \\left(  \\mathbf{x}_t - \\frac{\\beta_t}{\\sqrt{1- \\bar{\\alpha}_t}} \\mathbf{\\epsilon}_\\theta(\\mathbf{x}_t, t) \\right)$$\n",
        "\n",
        "æœ€åçš„ç›®æ ‡å‡½æ•°$L_t$ é•¿è¿™æ ·ï¼Œç»™å®šéšæœºçš„æ—¶é—´æ­¥ $t$ ï¼Œ${\\epsilon} \\sim \\mathcal{N}({0}, {I})$ : \n",
        "\n",
        "$$ \\| \\mathbf{\\epsilon} - \\mathbf{\\epsilon}_\\theta(\\mathbf{x}_t, t) \\|^2 = \\| \\mathbf{\\epsilon} - \\mathbf{\\epsilon}_\\theta( \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0 + \\sqrt{(1- \\bar{\\alpha}_t)  } \\mathbf{\\epsilon}, t) \\|^2.$$\n",
        "\n",
        "\n",
        "\n",
        "$\\mathbf x_0$æ˜¯åˆå§‹å›¾åƒï¼Œæˆ‘ä»¬çœ‹åˆ°å™ªå£°$t$æ ·æœ¬ç”±å›ºå®šçš„å‰å‘è¿‡ç¨‹ç»™å‡ºã€‚$\\epsilon$æ˜¯åœ¨æ—¶é—´æ­¥é•¿$t$é‡‡æ ·çš„çº¯å™ªå£°ï¼Œ$\\epsilon_\\theta(\\mathbf x_tï¼Œt)$æ˜¯æˆ‘ä»¬çš„ç¥ç»ç½‘ç»œã€‚ç¥ç»ç½‘ç»œçš„ä¼˜åŒ–ä½¿ç”¨ä¸€ä¸ªç®€å•çš„å‡æ–¹è¯¯å·®(MSE)è®¡ç®—çœŸå®å™ªå£°å’Œé¢„æµ‹é«˜æ–¯å™ªå£°ä¹‹é—´çš„å·®å¼‚ã€‚\n",
        "\n",
        "è®­ç»ƒç®—æ³•å¦‚ä¸‹ï¼š\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?id=1LJsdkZ3i1J32lmi9ONMqKFg5LMtpSfT4\" width=\"400\" />\n",
        "</p>\n",
        " \n",
        "\n",
        "1. ä»æœªçŸ¥çš„çœŸå®æ•°æ®åˆ†å¸ƒ$q(\\mathbf x_0)$ä¸­éšæœºé‡‡æ ·$\\mathbf x_0$ï¼Œ\n",
        "2. æˆ‘ä»¬åœ¨1å’Œ$T$ä¹‹é—´å‡åŒ€é‡‡ä¸åŒæ—¶é—´æ­¥çš„å™ªå£°ï¼Œ\n",
        "3. æˆ‘ä»¬ä»é«˜æ–¯åˆ†å¸ƒé‡‡æ ·ä¸€äº›å™ªå£°ï¼Œå¹¶åœ¨$ğ‘¡$æ—¶é—´æ­¥ä¸Šä½¿ç”¨å‰è¾¹å®šä¹‰çš„ä¼˜è‰¯å±æ€§æ¥ç ´åè¾“å…¥åˆ†å¸ƒï¼Œ\n",
        "4. ç¥ç»ç½‘ç»œæ ¹æ®æŸåçš„å›¾åƒ$\\mathbf x_t$è¿›è¡Œè®­ç»ƒï¼Œç›®çš„æ˜¯é¢„æµ‹æ–½åŠ åœ¨å›¾ç‰‡ä¸Šçš„å™ªå£°ï¼Œä¹Ÿå°±æ˜¯åŸºäºå·²çŸ¥æ–¹å·®è¡¨$\\beta_t$ä½œç”¨åœ¨$\\mathbf x_0$ä¸Šçš„å™ªå£°\n",
        "\n",
        "\n",
        "æ‰€æœ‰è¿™äº›éƒ½æ˜¯åœ¨æ‰¹é‡æ•°æ®ä¸Šå®Œæˆçš„ï¼Œä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™ä¼˜åŒ–ç¥ç»ç½‘ç»œã€‚\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5153024b",
      "metadata": {
        "id": "5153024b"
      },
      "source": [
        "\n",
        "\n",
        "## ç¥ç»ç½‘ç»œ\n",
        "\n",
        "ç¥ç»ç½‘ç»œéœ€è¦åœ¨ç‰¹å®šçš„æ—¶é—´æ­¥$t$ä¸­è¾“å…¥ä¸€ä¸ªå¸¦æœ‰å™ªå£°çš„å›¾åƒï¼Œå¹¶è¿”å›é¢„æµ‹çš„å™ªå£°ã€‚è¯·æ³¨æ„ï¼Œé¢„æµ‹çš„å™ªå£°æ˜¯ä¸€ä¸ªå¼ é‡ï¼Œå…¶å¤§å°ä¸è¾“å…¥å›¾åƒç›¸åŒã€‚å› æ­¤ï¼Œåœ¨æŠ€æœ¯å®ç°ä¸Šï¼Œç½‘ç»œçš„è¾“å…¥å’Œè¾“å‡ºæ˜¯ç›¸åŒå½¢çŠ¶çš„å¼ é‡ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»€ä¹ˆç±»å‹çš„ç¥ç»ç½‘ç»œæ¥å®ç°è¿™ä¸ªä»»åŠ¡ï¼Ÿ\n",
        "\n",
        "\n",
        "åœ¨è¿™é‡Œé€šå¸¸ä½¿ç”¨çš„æ˜¯ç±»ä¼¼äº[è‡ªç¼–ç å™¨](https://en.wikipedia.org/wiki/Autoencoder)çš„ç½‘ç»œï¼Œè‡ªç¼–ç å™¨åœ¨ç¼–ç å™¨å’Œè§£ç å™¨ä¹‹é—´æœ‰ä¸€ä¸ªæ‰€è°“çš„â€œç“¶é¢ˆâ€ï¼ˆbottleneckï¼‰å±‚ã€‚ç¼–ç å™¨é¦–å…ˆå°†å›¾åƒç¼–ç ä¸ºè¾ƒå°çš„éšè—è¡¨ç¤ºï¼Œç§°ä¸ºâ€œç“¶é¢ˆâ€ï¼Œç„¶åè§£ç å™¨å°†è¯¥éšè—è¡¨ç¤ºè§£ç å›å®é™…å›¾åƒã€‚è¿™ä½¿ç½‘ç»œçš„ç“¶é¢ˆå±‚ä¸­å¯ä»¥ä¿ç•™æœ€é‡è¦çš„ä¿¡æ¯ã€‚\n",
        "\n",
        "\n",
        "åœ¨ä½“ç³»ç»“æ„æ–¹é¢ï¼ŒDDPMä½œè€…é€‰æ‹©äº†U-Netï¼Œç”±([Ronneberger et al., 2015](https://arxiv.org/abs/1505.04597))æå‡ºï¼Œå½“æ—¶åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹é¢å®ç°äº†SOTAã€‚ä¸ä»»ä½•è‡ªç¼–ç å™¨ä¸€æ ·ï¼Œè¯¥ç½‘ç»œåŒ…æ‹¬ä¸­é—´çš„ç“¶é¢ˆï¼Œä»¥ç¡®ä¿ç½‘ç»œå­¦ä¹ åˆ°æœ€é‡è¦çš„ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œå®ƒå¼•å…¥äº†ç¼–ç å™¨å’Œè§£ç å™¨ä¹‹é—´çš„æ®‹å·®è¿æ¥ï¼Œæå¤§åœ°æ”¹å–„äº†æ¢¯åº¦æµåŠ¨ï¼ˆå—[He et al., 2015](https://arxiv.org/abs/1512.03385))ResNetçš„å¯å‘ï¼‰ã€‚\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?id=1_Hej_VTgdUWGsxxIuyZACCGjpbCGIUi6\" width=\"400\" />\n",
        "</p>\n",
        "\n",
        "å¯ä»¥çœ‹å‡ºï¼ŒU-Netæ¨¡å‹é¦–å…ˆå¯¹è¾“å…¥è¿›è¡Œä¸‹é‡‡æ ·ï¼ˆå³åœ¨ç©ºé—´åˆ†è¾¨ç‡ä¸Šä½¿è¾“å…¥æ›´å°ï¼‰ï¼Œç„¶åæ‰§è¡Œä¸Šé‡‡æ ·ã€‚\n",
        "\n",
        "ä¸‹é¢ï¼Œæˆ‘ä»¬å°†é€æ­¥å®ç°è¿™ä¸ªç½‘ç»œã€‚\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ç½‘ç»œè¾…åŠ©æ¨¡å—\n",
        "\n",
        "é¦–å…ˆï¼Œæˆ‘ä»¬å®šä¹‰ä¸€äº›è¾…åŠ©å‡½æ•°å’Œç±»ï¼Œåœ¨å®ç°ç¥ç»ç½‘ç»œæ—¶ä¼šç”¨åˆ°å®ƒä»¬ã€‚é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªâ€œæ®‹å·®â€æ¨¡å—ï¼Œå®ƒå°†è¾“å…¥ç®€å•åœ°åŠ åˆ°ç‰¹å®šå‡½æ•°çš„è¾“å‡ºä¸­ï¼ˆä¸ºç‰¹å®šå‡½æ•°æ·»åŠ äº†ä¸€ä¸ªæ®‹å·®è¿æ¥ï¼‰ã€‚\n",
        "\n",
        "æˆ‘ä»¬è¿˜ä¸ºä¸Šé‡‡æ ·å’Œä¸‹é‡‡æ ·æ“ä½œå®šä¹‰äº†åˆ«åã€‚"
      ],
      "metadata": {
        "id": "xsm73idkszVl"
      },
      "id": "xsm73idkszVl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68907f5d",
      "metadata": {
        "id": "68907f5d"
      },
      "outputs": [],
      "source": [
        "def exists(x):\n",
        "    return x is not None\n",
        "\n",
        "def default(val, d):\n",
        "    if exists(val):\n",
        "        return val\n",
        "    return d() if isfunction(d) else d\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        return self.fn(x, *args, **kwargs) + x\n",
        "\n",
        "def Upsample(dim):\n",
        "    return nn.ConvTranspose2d(dim, dim, 4, 2, 1)\n",
        "\n",
        "def Downsample(dim):\n",
        "    return nn.Conv2d(dim, dim, 4, 2, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "592aa765",
      "metadata": {
        "id": "592aa765"
      },
      "source": [
        "### ä½ç½®åµŒå…¥\n",
        "\n",
        "ç¥ç»ç½‘ç»œçš„å‚æ•°åœ¨ä¸åŒæ—¶é—´æ­¥ä¹‹é—´æ˜¯å…±äº«çš„ï¼Œå› æ­¤ä½œè€…å—Transformer([Vaswani et al., 2017](https://arxiv.org/abs/1706.03762))å¯å‘ï¼Œä½¿ç”¨æ­£å¼¦ä½ç½®åµŒå…¥æ¥å¯¹$t$è¿›è¡Œç¼–ç ã€‚è¿™ä½¿å¾—ç¥ç»ç½‘ç»œèƒ½å¤Ÿâ€œçŸ¥é“â€å®ƒæ­£åœ¨å¤„ç†çš„batchä¸­æ¯ä¸ªå›¾åƒçš„æ—¶é—´æ­¥é•¿ï¼ˆå™ªå£°æ°´å¹³ï¼‰ã€‚\n",
        "\n",
        "`SinusoidalPositionEmbeddings`æ¨¡å—ä»¥å½¢çŠ¶ä¸º`(batch_size, 1)`çš„å¼ é‡ä½œä¸ºè¾“å…¥ï¼Œå³å½“å‰batchä¸­æ¯ä¸ªå›¾ç‰‡çš„æ—¶é—´æ­¥ï¼ˆå™ªå£°æ°´å¹³ï¼‰ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºå½¢çŠ¶ä¸º(batch_sizeï¼Œdim)çš„å¼ é‡ï¼Œå…¶ä¸­dimæ˜¯ä½ç½®åµŒå…¥çš„ç»´åº¦ã€‚ç„¶åå°†å…¶æ·»åŠ åˆ°æ¯ä¸ªæ®‹å·®å—ä¸­ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ed0757b",
      "metadata": {
        "id": "5ed0757b"
      },
      "outputs": [],
      "source": [
        "class SinusoidalPositionEmbeddings(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, time):\n",
        "        device = time.device\n",
        "        half_dim = self.dim // 2\n",
        "        embeddings = math.log(10000) / (half_dim - 1)\n",
        "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
        "        embeddings = time[:, None] * embeddings[None, :]\n",
        "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
        "        return embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ff47fbb",
      "metadata": {
        "id": "9ff47fbb"
      },
      "source": [
        "###  ResNet/ConvNeXTå—\n",
        "\n",
        "æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®šä¹‰U-Netæ¨¡å‹çš„æ ¸å¿ƒæ„å»ºå—ã€‚\n",
        "\n",
        "DDPMä½œè€…ä½¿ç”¨äº†Wide ResNetå—([Zagoruyko et al., 2016](https://arxiv.org/abs/1605.07146))ï¼Œä½†Phil Wangä»£ç ä¸­è¿˜å®ç°äº†ConvNeXTå—([Liu et al., 2022](https://arxiv.org/abs/2201.03545))ã€‚åœ¨U-Netæ¶æ„ä¸­ï¼Œå¯ä»¥ä»»é€‰å…¶ä¸€ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c2d1219",
      "metadata": {
        "id": "2c2d1219"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, dim_out, groups = 8):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv2d(dim, dim_out, 3, padding = 1)\n",
        "        self.norm = nn.GroupNorm(groups, dim_out)\n",
        "        self.act = nn.SiLU()\n",
        "\n",
        "    def forward(self, x, scale_shift = None):\n",
        "        x = self.proj(x)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        if exists(scale_shift):\n",
        "            scale, shift = scale_shift\n",
        "            x = x * (scale + 1) + shift\n",
        "\n",
        "        x = self.act(x)\n",
        "        return x\n",
        "\n",
        "class ResnetBlock(nn.Module):\n",
        "    \"\"\"https://arxiv.org/abs/1512.03385\"\"\"\n",
        "    \n",
        "    def __init__(self, dim, dim_out, *, time_emb_dim=None, groups=8):\n",
        "        super().__init__()\n",
        "        self.mlp = (\n",
        "            nn.Sequential(nn.SiLU(), nn.Linear(time_emb_dim, dim_out))\n",
        "            if exists(time_emb_dim)\n",
        "            else None\n",
        "        )\n",
        "\n",
        "        self.block1 = Block(dim, dim_out, groups=groups)\n",
        "        self.block2 = Block(dim_out, dim_out, groups=groups)\n",
        "        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x, time_emb=None):\n",
        "        h = self.block1(x)\n",
        "\n",
        "        if exists(self.mlp) and exists(time_emb):\n",
        "            time_emb = self.mlp(time_emb)\n",
        "            h = rearrange(time_emb, \"b c -> b c 1 1\") + h\n",
        "\n",
        "        h = self.block2(h)\n",
        "        return h + self.res_conv(x)\n",
        "    \n",
        "class ConvNextBlock(nn.Module):\n",
        "    \"\"\"https://arxiv.org/abs/2201.03545\"\"\"\n",
        "\n",
        "    def __init__(self, dim, dim_out, *, time_emb_dim=None, mult=2, norm=True):\n",
        "        super().__init__()\n",
        "        self.mlp = (\n",
        "            nn.Sequential(nn.GELU(), nn.Linear(time_emb_dim, dim))\n",
        "            if exists(time_emb_dim)\n",
        "            else None\n",
        "        )\n",
        "\n",
        "        self.ds_conv = nn.Conv2d(dim, dim, 7, padding=3, groups=dim)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.GroupNorm(1, dim) if norm else nn.Identity(),\n",
        "            nn.Conv2d(dim, dim_out * mult, 3, padding=1),\n",
        "            nn.GELU(),\n",
        "            nn.GroupNorm(1, dim_out * mult),\n",
        "            nn.Conv2d(dim_out * mult, dim_out, 3, padding=1),\n",
        "        )\n",
        "\n",
        "        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x, time_emb=None):\n",
        "        h = self.ds_conv(x)\n",
        "\n",
        "        if exists(self.mlp) and exists(time_emb):\n",
        "            assert exists(time_emb), \"time embedding must be passed in\"\n",
        "            condition = self.mlp(time_emb)\n",
        "            h = h + rearrange(condition, \"b c -> b c 1 1\")\n",
        "\n",
        "        h = self.net(h)\n",
        "        return h + self.res_conv(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51d9a24c",
      "metadata": {
        "id": "51d9a24c"
      },
      "source": [
        "### Attention æ¨¡å—\n",
        "\n",
        "æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®šä¹‰æ³¨æ„åŠ›æ¨¡å—ï¼Œè¯¥æ¨¡å—ç”±DDPMä½œè€…æ·»åŠ åœ¨å·ç§¯å—ä¹‹é—´ã€‚æ³¨æ„åŠ›æ˜¯Transformer([Vaswani et al., 2017](https://arxiv.org/abs/1706.03762))çš„æ„å»ºå—ï¼Œåœ¨AIé¢†åŸŸï¼Œä»NLPå’ŒCVåˆ°è›‹ç™½è´¨æŠ˜å ï¼Œéƒ½å–å¾—äº†å·¨å¤§çš„æˆåŠŸã€‚Phil Wangå®ç°äº†2ç§æ³¨æ„åŠ›å˜ä½“ï¼šä¸€ç§æ˜¯å¸¸è§„çš„å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼ˆå’ŒTransformerä¸­çš„ä¸€æ ·ï¼‰ï¼Œå¦ä¸€ç§æ˜¯[çº¿æ€§æ³¨æ„åŠ›å˜ä½“](https://github.com/lucidrains/linear-attention-transformer)ï¼ˆ[Shen et al., 2018](https://arxiv.org/abs/1812.01243)ï¼‰ï¼Œå…¶æ—¶é—´å’Œå†…å­˜éœ€æ±‚éšåºåˆ—é•¿åº¦çº¿æ€§ç¼©æ”¾ï¼ŒèŠ‚çœè®¡ç®—èµ„æºã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07bbd544",
      "metadata": {
        "id": "07bbd544"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads=4, dim_head=32):\n",
        "        super().__init__()\n",
        "        self.scale = dim_head**-0.5\n",
        "        self.heads = heads\n",
        "        hidden_dim = dim_head * heads\n",
        "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
        "        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
        "        q, k, v = map(\n",
        "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
        "        )\n",
        "        q = q * self.scale\n",
        "\n",
        "        sim = einsum(\"b h d i, b h d j -> b h i j\", q, k)\n",
        "        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n",
        "        attn = sim.softmax(dim=-1)\n",
        "\n",
        "        out = einsum(\"b h i j, b h d j -> b h i d\", attn, v)\n",
        "        out = rearrange(out, \"b h (x y) d -> b (h d) x y\", x=h, y=w)\n",
        "        return self.to_out(out)\n",
        "\n",
        "class LinearAttention(nn.Module):\n",
        "    def __init__(self, dim, heads=4, dim_head=32):\n",
        "        super().__init__()\n",
        "        self.scale = dim_head**-0.5\n",
        "        self.heads = heads\n",
        "        hidden_dim = dim_head * heads\n",
        "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
        "\n",
        "        self.to_out = nn.Sequential(nn.Conv2d(hidden_dim, dim, 1), \n",
        "                                    nn.GroupNorm(1, dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
        "        q, k, v = map(\n",
        "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
        "        )\n",
        "\n",
        "        q = q.softmax(dim=-2)\n",
        "        k = k.softmax(dim=-1)\n",
        "\n",
        "        q = q * self.scale\n",
        "        context = torch.einsum(\"b h d n, b h e n -> b h d e\", k, v)\n",
        "\n",
        "        out = torch.einsum(\"b h d e, b h d n -> b h e n\", context, q)\n",
        "        out = rearrange(out, \"b h c (x y) -> b (h c) x y\", h=self.heads, x=h, y=w)\n",
        "        return self.to_out(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a8031b0",
      "metadata": {
        "id": "9a8031b0"
      },
      "source": [
        "### Group normalization\n",
        "\n",
        "DDPM çš„ä½œè€…åœ¨ U-Net çš„å·ç§¯/æ³¨æ„åŠ›å±‚ä¹‹é—´äº¤æ›¿ä½¿ç”¨äº†ç¾¤ç»„å½’ä¸€åŒ–([Wu et al., 2018](https://arxiv.org/abs/1803.08494))ã€‚ä¸‹é¢ï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ª`PreNorm`ç±»ï¼Œå®ƒå°†ç”¨äºåœ¨æ³¨æ„åŠ›å±‚ä¹‹å‰å®ç°ç¾¤ç»„å½’ä¸€åŒ–ã€‚\n",
        "\n",
        "è¯·æ³¨æ„ï¼Œåœ¨ Transformer ä¸­ï¼Œæ˜¯å¦åœ¨æ³¨æ„åŠ›ä¹‹å‰æˆ–ä¹‹åä½¿ç”¨å½’ä¸€åŒ–å­˜åœ¨äº‰è®ºï¼Œå…·ä½“å¯ä»¥çœ‹ï¼š[Transformers without Tears](https://tnq177.github.io/data/transformers_without_tears.pdf)ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e2ce68f",
      "metadata": {
        "id": "5e2ce68f"
      },
      "outputs": [],
      "source": [
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        self.norm = nn.GroupNorm(1, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.norm(x)\n",
        "        return self.fn(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06b3fad0",
      "metadata": {
        "id": "06b3fad0"
      },
      "source": [
        "### Conditional U-Net\n",
        "\n",
        "æœ‰äº†æ‰€æœ‰æ„å»ºå—ï¼ˆä½ç½®åµŒå…¥ã€ResNet/ConvNeXTå—ã€æ³¨æ„åŠ›å’Œç»„å½’ä¸€åŒ–ï¼‰çš„å®šä¹‰ï¼Œç°åœ¨å®šä¹‰æ•´ä¸ªç¥ç»ç½‘ç»œã€‚\n",
        "\n",
        "å›æƒ³ä¸€ä¸‹ï¼Œç½‘ç»œ $\\mathbf{\\epsilon}_\\theta(\\mathbf{x}_t, t)$ çš„ä»»åŠ¡æ˜¯æ¥æ”¶ä¸€æ‰¹æœ‰å™ªå£°çš„å›¾åƒå’Œå™ªå£°æ°´å¹³ï¼Œå¹¶è¾“å‡ºåŠ å…¥å™ªå£°åçš„å›¾åƒã€‚æ›´æ­£å¼åœ°è¯´ï¼š\n",
        "\n",
        "ç½‘ç»œæ¥æ”¶ä¸€ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„æœ‰å™ªå£°å›¾åƒæ‰¹æ¬¡å’Œä¸€ä¸ªå½¢çŠ¶ä¸º`(batch_size, 1)`çš„å™ªå£°æ°´å¹³æ‰¹æ¬¡ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›ä¸€ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„å¼ é‡ã€‚\n",
        "\n",
        "ç½‘ç»œçš„æ„å»ºå¦‚ä¸‹ï¼š\n",
        "\n",
        "- åœ¨ä¸€ä¸ªbatchçš„å¸¦å™ªå›¾åƒä¸Šå·ç§¯ï¼Œå¹¶ä¸ºå™ªå£°æ°´å¹³ï¼ˆæ—¶é—´æ­¥$t$ï¼‰è®¡ç®—ä½ç½®åµŒå…¥ã€‚\n",
        "- è¿›è¡Œä¸€ç³»åˆ—çš„ä¸‹é‡‡æ ·ã€‚\n",
        "  æ¯ä¸ªä¸‹é‡‡æ ·é˜¶æ®µåŒ…å«2ä¸ªResNet/ConvNeXTå— + groupå½’ä¸€åŒ– + æ³¨æ„åŠ› + æ®‹å·®è¿æ¥ + ä¸‹é‡‡æ ·ã€‚\n",
        "- åœ¨ç½‘ç»œçš„ä¸­å¿ƒï¼Œå†æ¬¡åº”ç”¨ResNet/ConvNeXTå—ï¼Œäº¤æ›¿ä½¿ç”¨æ³¨æ„åŠ›ã€‚\n",
        "\n",
        "- è¿›è¡Œä¸€ç³»åˆ—çš„ä¸Šé‡‡æ ·ã€‚\n",
        "  æ¯ä¸ªä¸Šé‡‡æ ·é˜¶æ®µåŒ…å«2ä¸ªResNet/ConvNeXTå— + groupå½’ä¸€åŒ– + æ³¨æ„åŠ› + æ®‹å·®è¿æ¥ + ä¸Šé‡‡æ ·æ“ä½œã€‚\n",
        "\n",
        "- åº”ç”¨ä¸€ä¸ªResNet/ConvNeXTå—ï¼Œç„¶åæ˜¯ä¸€ä¸ªå·ç§¯å±‚ã€‚\n",
        "\n",
        "\n",
        "[ç¥ç»ç½‘ç»œå †å å±‚å°±åƒç§¯æœ¨å—ä¸€æ ·](http://karpathy.github.io/2019/04/25/recipe/)ã€‚\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a159023",
      "metadata": {
        "id": "3a159023"
      },
      "outputs": [],
      "source": [
        "class Unet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        init_dim=None,\n",
        "        out_dim=None,\n",
        "        dim_mults=(1, 2, 4, 8),\n",
        "        channels=3,\n",
        "        with_time_emb=True,\n",
        "        resnet_block_groups=8,\n",
        "        use_convnext=True,\n",
        "        convnext_mult=2,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # determine dimensions\n",
        "        self.channels = channels\n",
        "\n",
        "        init_dim = default(init_dim, dim // 3 * 2)\n",
        "        self.init_conv = nn.Conv2d(channels, init_dim, 7, padding=3)\n",
        "\n",
        "        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n",
        "        in_out = list(zip(dims[:-1], dims[1:]))\n",
        "        \n",
        "        if use_convnext:\n",
        "            block_klass = partial(ConvNextBlock, mult=convnext_mult)\n",
        "        else:\n",
        "            block_klass = partial(ResnetBlock, groups=resnet_block_groups)\n",
        "\n",
        "        # time embeddings\n",
        "        if with_time_emb:\n",
        "            time_dim = dim * 4\n",
        "            self.time_mlp = nn.Sequential(\n",
        "                SinusoidalPositionEmbeddings(dim),\n",
        "                nn.Linear(dim, time_dim),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(time_dim, time_dim),\n",
        "            )\n",
        "        else:\n",
        "            time_dim = None\n",
        "            self.time_mlp = None\n",
        "\n",
        "        # layers\n",
        "        self.downs = nn.ModuleList([])\n",
        "        self.ups = nn.ModuleList([])\n",
        "        num_resolutions = len(in_out)\n",
        "\n",
        "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
        "            is_last = ind >= (num_resolutions - 1)\n",
        "\n",
        "            self.downs.append(\n",
        "                nn.ModuleList(\n",
        "                    [\n",
        "                        block_klass(dim_in, dim_out, time_emb_dim=time_dim),\n",
        "                        block_klass(dim_out, dim_out, time_emb_dim=time_dim),\n",
        "                        Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n",
        "                        Downsample(dim_out) if not is_last else nn.Identity(),\n",
        "                    ]\n",
        "                )\n",
        "            )\n",
        "\n",
        "        mid_dim = dims[-1]\n",
        "        self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
        "        self.mid_attn = Residual(PreNorm(mid_dim, Attention(mid_dim)))\n",
        "        self.mid_block2 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
        "\n",
        "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n",
        "            is_last = ind >= (num_resolutions - 1)\n",
        "\n",
        "            self.ups.append(\n",
        "                nn.ModuleList(\n",
        "                    [\n",
        "                        block_klass(dim_out * 2, dim_in, time_emb_dim=time_dim),\n",
        "                        block_klass(dim_in, dim_in, time_emb_dim=time_dim),\n",
        "                        Residual(PreNorm(dim_in, LinearAttention(dim_in))),\n",
        "                        Upsample(dim_in) if not is_last else nn.Identity(),\n",
        "                    ]\n",
        "                )\n",
        "            )\n",
        "\n",
        "        out_dim = default(out_dim, channels)\n",
        "        self.final_conv = nn.Sequential(\n",
        "            block_klass(dim, dim), nn.Conv2d(dim, out_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, time):\n",
        "        x = self.init_conv(x)\n",
        "\n",
        "        t = self.time_mlp(time) if exists(self.time_mlp) else None\n",
        "\n",
        "        h = []\n",
        "\n",
        "        # downsample\n",
        "        for block1, block2, attn, downsample in self.downs:\n",
        "            x = block1(x, t)\n",
        "            x = block2(x, t)\n",
        "            x = attn(x)\n",
        "            h.append(x)\n",
        "            x = downsample(x)\n",
        "\n",
        "        # bottleneck\n",
        "        x = self.mid_block1(x, t)\n",
        "        x = self.mid_attn(x)\n",
        "        x = self.mid_block2(x, t)\n",
        "\n",
        "        # upsample\n",
        "        for block1, block2, attn, upsample in self.ups:\n",
        "            x = torch.cat((x, h.pop()), dim=1)\n",
        "            x = block1(x, t)\n",
        "            x = block2(x, t)\n",
        "            x = attn(x)\n",
        "            x = upsample(x)\n",
        "\n",
        "        return self.final_conv(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a30368b2",
      "metadata": {
        "id": "a30368b2"
      },
      "source": [
        "## å‰å‘æ‰©æ•£\n",
        "\n",
        "å‰å‘è¿‡ç¨‹å°±æ˜¯åœ¨$T$æ—¶é—´æ­¥æŒ‰ç…§å®šä¹‰å¥½çš„**æ–¹å·®è¡¨**ç»™å›¾åƒé€æ¸æ·»åŠ å™ªå£°çš„è¿‡ç¨‹ã€‚\n",
        "\n",
        "DDPMæ˜¯ä½¿ç”¨çº¿æ€§æ’å€¼åšçš„æ–¹å·®è¡¨:\n",
        "\n",
        "> We set the forward process variances to constants\n",
        "increasing linearly from $\\beta_1 = 10^{âˆ’4}$\n",
        "to $\\beta_T = 0.02$.\n",
        "\n",
        "ä½†æ˜¯([Nichol et al., 2021](https://arxiv.org/abs/2102.09672)) è¡¨ç¤ºå°†å™ªå£°è¡¨é¢„è®¾ä¸ºcosine scheduleå¯ä»¥è·å¾—æ›´å¥½çš„æ•ˆæœã€‚\n",
        "\n",
        "ä¸‹é¢ï¼Œæˆ‘ä»¬å®šä¹‰äº† $T$ ä¸ªæ—¶é—´æ­¥éª¤çš„å„ç§æ–¹å·®è¡¨ï¼Œä»¥åŠæˆ‘ä»¬éœ€è¦çš„ç›¸åº”å˜é‡ï¼Œå¦‚ç´¯ç§¯æ–¹å·®ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d751df2",
      "metadata": {
        "id": "5d751df2"
      },
      "outputs": [],
      "source": [
        "def cosine_beta_schedule(timesteps, s=0.008):\n",
        "    \"\"\"\n",
        "    cosine schedule as proposed in https://arxiv.org/abs/2102.09672\n",
        "    \"\"\"\n",
        "    steps = timesteps + 1\n",
        "    x = torch.linspace(0, timesteps, steps)\n",
        "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
        "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
        "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
        "    return torch.clip(betas, 0.0001, 0.9999)\n",
        "\n",
        "def linear_beta_schedule(timesteps):\n",
        "    beta_start = 0.0001\n",
        "    beta_end = 0.02\n",
        "    return torch.linspace(beta_start, beta_end, timesteps)\n",
        "\n",
        "def quadratic_beta_schedule(timesteps):\n",
        "    beta_start = 0.0001\n",
        "    beta_end = 0.02\n",
        "    return torch.linspace(beta_start**0.5, beta_end**0.5, timesteps) ** 2\n",
        "\n",
        "def sigmoid_beta_schedule(timesteps):\n",
        "    beta_start = 0.0001\n",
        "    beta_end = 0.02\n",
        "    betas = torch.linspace(-6, 6, timesteps)\n",
        "    return torch.sigmoid(betas) * (beta_end - beta_start) + beta_start"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bfc3841",
      "metadata": {
        "id": "6bfc3841"
      },
      "source": [
        "\n",
        "æˆ‘ä»¬å…ˆä½¿ç”¨çº¿æ€§æ–¹å·®è¡¨æ¥è¿›è¡Œ$T=200$æ—¶é—´æ­¥é•¿çš„è®­ç»ƒã€‚\n",
        "\n",
        "é¦–å…ˆå®šä¹‰æˆ‘ä»¬å°†éœ€è¦çš„å„ç§å˜é‡ï¼Œæ¯”å¦‚æ–¹å·®å˜é‡$\\beta_t$ å’Œ ç´¯ä¹˜æ–¹å·® $\\bar{\\alpha}_t$ã€‚\n",
        "\n",
        "ä¸‹é¢åˆ—å‡ºçš„æ¯ä¸ªå˜é‡éƒ½æ˜¯ä¸€ç»´å¼ é‡ï¼Œå­˜å‚¨äº†ä» $t$ åˆ° $T$ çš„å€¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å®šä¹‰äº†ä¸€ä¸ª `extract` å‡½æ•°ï¼Œå®ƒå°†å…è®¸æˆ‘ä»¬è·å–ä¸€ä¸ªbatchä¸­ç´¢å¼•çš„é€‚å½“ $t$ ç´¢å¼•ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc57b01f",
      "metadata": {
        "id": "cc57b01f"
      },
      "outputs": [],
      "source": [
        "timesteps = 200\n",
        "\n",
        "# define beta schedule\n",
        "betas = linear_beta_schedule(timesteps=timesteps)\n",
        "\n",
        "# define alphas \n",
        "alphas = 1. - betas\n",
        "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
        "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
        "sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
        "\n",
        "# calculations for diffusion q(x_t | x_{t-1}) and others\n",
        "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
        "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n",
        "\n",
        "# calculations for posterior q(x_{t-1} | x_t, x_0)\n",
        "posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
        "\n",
        "def extract(a, t, x_shape):\n",
        "    batch_size = t.shape[0]\n",
        "    out = a.gather(-1, t.cpu())\n",
        "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48f8a004",
      "metadata": {
        "id": "48f8a004"
      },
      "source": [
        "æˆ‘ä»¬ç”¨è¿™ä¸ªçŒ«çš„å›¾å±•ç¤ºä¸€ä¸‹å‰å‘è¿‡ç¨‹ä¸­æ˜¯å¦‚ä½•åŠ å™ªçš„ï¼š"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9f13b16",
      "metadata": {
        "id": "c9f13b16",
        "lines_to_next_cell": 0
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bde062f",
      "metadata": {
        "id": "4bde062f"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=17FXnvCTl96lDhqZ_io54guXO8hM-rsQ2\" width=\"400\" />\n",
        "\n",
        "\n",
        "å™ªå£°æ˜¯åŠ åˆ° PyTorch å¼ é‡ä¸Šï¼Œè€Œä¸æ˜¯ Pillow å›¾åƒä¸Šã€‚\n",
        "\n",
        "æˆ‘ä»¬é¦–å…ˆå®šä¹‰å›¾åƒå˜æ¢ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥ä» PIL å›¾åƒè½¬æ¢ä¸º PyTorch å¼ é‡ï¼ˆæˆ‘ä»¬å¯ä»¥åœ¨å…¶ä¸Šæ·»åŠ å™ªå£°ï¼‰ï¼Œåä¹‹äº¦ç„¶ã€‚\n",
        "\n",
        "è¿™äº›å˜æ¢éå¸¸ç®€å•ï¼šæˆ‘ä»¬é¦–å…ˆé€šè¿‡é™¤ä»¥ 255 æ¥å½’ä¸€åŒ–å›¾åƒï¼ˆä½¿å®ƒä»¬åœ¨ `[0,1]` èŒƒå›´å†…ï¼‰ï¼Œç„¶åç¡®ä¿å®ƒä»¬åœ¨ `[-1,1]` èŒƒå›´å†…ã€‚æ–¹æ³•æ¥è‡ª DDPM è®ºæ–‡ï¼š\n",
        "\n",
        "> We assume that image data consists of integers in $\\{0, 1, ... , 255\\}$ scaled linearly to $[âˆ’1, 1]$. This\n",
        "ensures that the neural network reverse process operates on consistently scaled inputs starting from\n",
        "the standard normal prior $p(\\mathbf{x}_T )$. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71aba861",
      "metadata": {
        "id": "71aba861"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import Compose, ToTensor, Lambda, ToPILImage, CenterCrop, Resize\n",
        "\n",
        "image_size = 128\n",
        "transform = Compose([\n",
        "    Resize(image_size),\n",
        "    CenterCrop(image_size),\n",
        "    ToTensor(), # turn into Numpy array of shape HWC, divide by 255\n",
        "    Lambda(lambda t: (t * 2) - 1),\n",
        "    \n",
        "])\n",
        "\n",
        "x_start = transform(image).unsqueeze(0)\n",
        "x_start.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e143cc62",
      "metadata": {
        "id": "e143cc62"
      },
      "source": [
        "æˆ‘ä»¬ä¹Ÿå®šä¹‰åå‘è¿‡ç¨‹ï¼ŒæŠŠå›¾åƒä»å¼ é‡è½¬æ¢å›PILå›¾åƒï¼š"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b98e91ff",
      "metadata": {
        "id": "b98e91ff"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "reverse_transform = Compose([\n",
        "     Lambda(lambda t: (t + 1) / 2),\n",
        "     Lambda(lambda t: t.permute(1, 2, 0)), # CHW to HWC\n",
        "     Lambda(lambda t: t * 255.),\n",
        "     Lambda(lambda t: t.numpy().astype(np.uint8)),\n",
        "     ToPILImage(),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6869ab7",
      "metadata": {
        "id": "f6869ab7"
      },
      "source": [
        "è®©æˆ‘ä»¬éªŒè¯ä¸€ä¸‹ï¼š\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "366a770c",
      "metadata": {
        "id": "366a770c"
      },
      "outputs": [],
      "source": [
        "reverse_transform(x_start.squeeze())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f38d30ab",
      "metadata": {
        "id": "f38d30ab"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1WT22KYvqJbHFdYYfkV7ohKNO4alnvesB\" width=\"100\" />\n",
        "\n",
        "ç°åœ¨æˆ‘ä»¬å¯ä»¥å’ŒDDPMè®ºæ–‡å†™çš„ä¸€æ ·å®šä¹‰ç½‘ç»œå‰å‘è¿‡ç¨‹äº†ï¼š\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3752480",
      "metadata": {
        "id": "f3752480"
      },
      "outputs": [],
      "source": [
        "# forward diffusion\n",
        "def q_sample(x_start, t, noise=None):\n",
        "    if noise is None:\n",
        "        noise = torch.randn_like(x_start)\n",
        "\n",
        "    sqrt_alphas_cumprod_t = extract(sqrt_alphas_cumprod, t, x_start.shape)\n",
        "    sqrt_one_minus_alphas_cumprod_t = extract(\n",
        "        sqrt_one_minus_alphas_cumprod, t, x_start.shape\n",
        "    )\n",
        "\n",
        "    return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e82bac28",
      "metadata": {
        "id": "e82bac28"
      },
      "source": [
        "æµ‹è¯•ä¸€ä¸‹ï¼š"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bd64f89",
      "metadata": {
        "id": "6bd64f89"
      },
      "outputs": [],
      "source": [
        "def get_noisy_image(x_start, t):\n",
        "  # add noise\n",
        "  x_noisy = q_sample(x_start, t=t)\n",
        "\n",
        "  # turn back into PIL image\n",
        "  noisy_image = reverse_transform(x_noisy.squeeze())\n",
        "\n",
        "  return noisy_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52d22667",
      "metadata": {
        "id": "52d22667"
      },
      "outputs": [],
      "source": [
        "# take time step\n",
        "t = torch.tensor([40])\n",
        "\n",
        "get_noisy_image(x_start, t)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "003e1d95",
      "metadata": {
        "id": "003e1d95"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1Ra33wxuw3QxPlUG0iqZGtxgKBNdjNsqz\" width=\"100\" />\n",
        "\n",
        "è®©æˆ‘ä»¬å¯è§†åŒ–ä¸€ä¸‹ä¸åŒæ—¶é—´æ­¥éª¤çš„å›¾åƒï¼š"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e27c37d",
      "metadata": {
        "id": "8e27c37d"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# use seed for reproducability\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# source: https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py\n",
        "def plot(imgs, with_orig=False, row_title=None, **imshow_kwargs):\n",
        "    if not isinstance(imgs[0], list):\n",
        "        # Make a 2d grid even if there's just 1 row\n",
        "        imgs = [imgs]\n",
        "\n",
        "    num_rows = len(imgs)\n",
        "    num_cols = len(imgs[0]) + with_orig\n",
        "    fig, axs = plt.subplots(figsize=(200,200), nrows=num_rows, ncols=num_cols, squeeze=False)\n",
        "    for row_idx, row in enumerate(imgs):\n",
        "        row = [image] + row if with_orig else row\n",
        "        for col_idx, img in enumerate(row):\n",
        "            ax = axs[row_idx, col_idx]\n",
        "            ax.imshow(np.asarray(img), **imshow_kwargs)\n",
        "            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
        "\n",
        "    if with_orig:\n",
        "        axs[0, 0].set(title='Original image')\n",
        "        axs[0, 0].title.set_size(8)\n",
        "    if row_title is not None:\n",
        "        for row_idx in range(num_rows):\n",
        "            axs[row_idx, 0].set(ylabel=row_title[row_idx])\n",
        "\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "323102d0",
      "metadata": {
        "id": "323102d0"
      },
      "outputs": [],
      "source": [
        "plot([get_noisy_image(x_start, torch.tensor([t])) for t in [0, 50, 100, 150, 199]])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a205c24",
      "metadata": {
        "id": "4a205c24"
      },
      "source": [
        "è¿™æ„å‘³ç€æˆ‘ä»¬ç°åœ¨å¯ä»¥æ ¹æ®æ¨¡å‹å®šä¹‰æŸå¤±å‡½æ•°ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7725f6cf",
      "metadata": {
        "id": "7725f6cf"
      },
      "outputs": [],
      "source": [
        "def p_losses(denoise_model, x_start, t, noise=None, loss_type=\"l1\"):\n",
        "    if noise is None:\n",
        "        noise = torch.randn_like(x_start)\n",
        "\n",
        "    x_noisy = q_sample(x_start=x_start, t=t, noise=noise)\n",
        "    predicted_noise = denoise_model(x_noisy, t)\n",
        "\n",
        "    if loss_type == 'l1':\n",
        "        loss = F.l1_loss(noise, predicted_noise)\n",
        "    elif loss_type == 'l2':\n",
        "        loss = F.mse_loss(noise, predicted_noise)\n",
        "    elif loss_type == \"huber\":\n",
        "        loss = F.smooth_l1_loss(noise, predicted_noise)\n",
        "    else:\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc01c63b",
      "metadata": {
        "id": "cc01c63b"
      },
      "source": [
        "`denoise_model`æ˜¯æˆ‘ä»¬ä¸Šé¢å®šä¹‰çš„ U-Netã€‚æˆ‘ä»¬å°†ä½¿ç”¨çœŸå®å™ªå£°å’Œé¢„æµ‹å™ªå£°ä¹‹é—´çš„ Huber æŸå¤±ã€‚\n",
        "\n",
        "## å®šä¹‰ Dataset + DataLoader\n",
        "\n",
        "è¿™é‡Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªæ™®é€šçš„PyTorchæ•°æ®é›†ã€‚å¯ä»¥ä½¿ç”¨å¦‚Fashion-MNISTã€CIFAR-10æˆ–ImageNetç­‰ï¼Œå°†å›¾çº¿æ€§ç¼©æ”¾åˆ°`[âˆ’1,1]`ã€‚\n",
        "\n",
        "æ¯ä¸ªå›¾åƒéƒ½è¢«ç¼©æ”¾åˆ°ç›¸åŒçš„å¤§å°ã€‚æœ‰è¶£çš„æ˜¯ï¼Œå›¾åƒè¿˜ä¼šè¢«éšæœºæ°´å¹³ç¿»è½¬ã€‚æ¥è‡ªè®ºæ–‡çš„æè¿°ï¼š\n",
        "\n",
        "> We used random horizontal flips during training for CIFAR10; we tried training both with and without flips, and found flips to improve sample quality slightly.\n",
        "\n",
        "åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨ğŸ¤— [Datasets library](https://huggingface.co/docs/datasets/index) çš„Fashion MNISTæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ç”±å·²ç»å…·æœ‰ç›¸åŒåˆ†è¾¨ç‡çš„å›¾åƒç»„æˆï¼Œå³28x28ã€‚\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6134d691",
      "metadata": {
        "id": "6134d691"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# load dataset from the hub\n",
        "dataset = load_dataset(\"fashion_mnist\")\n",
        "image_size = 28\n",
        "channels = 1\n",
        "batch_size = 128"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db6f5875",
      "metadata": {
        "id": "db6f5875"
      },
      "source": [
        "\n",
        "\n",
        "æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºä¿®æ”¹æ•°æ®é›†ã€‚\n",
        "\n",
        "æˆ‘ä»¬ä½¿ç”¨[`with_transform`]([functionality](https://huggingface.co/docs/datasets/v2.2.1/en/package_reference/main_classes#datasets.Dataset.with_transform))åŠŸèƒ½æ¥å®ç°ã€‚è¯¥å‡½æ•°åªæ˜¯åº”ç”¨ä¸€äº›åŸºæœ¬çš„å›¾åƒé¢„å¤„ç†ï¼šéšæœºæ°´å¹³ç¿»è½¬ï¼Œé‡æ–°ç¼©æ”¾ï¼Œæœ€åä½¿å®ƒä»¬çš„å€¼åœ¨ [-1, 1] èŒƒå›´å†…ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3e78945",
      "metadata": {
        "id": "b3e78945"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# define image transformations (e.g. using torchvision)\n",
        "transform = Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Lambda(lambda t: (t * 2) - 1)\n",
        "])\n",
        "\n",
        "# define function\n",
        "def transforms(examples):\n",
        "   examples[\"pixel_values\"] = [transform(image.convert(\"L\")) for image in examples[\"image\"]]\n",
        "   del examples[\"image\"]\n",
        "\n",
        "   return examples\n",
        "\n",
        "transformed_dataset = dataset.with_transform(transforms).remove_columns(\"label\")\n",
        "\n",
        "# create dataloader\n",
        "dataloader = DataLoader(transformed_dataset[\"train\"], batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52e8273b",
      "metadata": {
        "id": "52e8273b"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(dataloader))\n",
        "print(batch.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cf98443",
      "metadata": {
        "id": "4cf98443"
      },
      "source": [
        "## é‡‡æ ·\n",
        "\n",
        "ä»¥ä¸‹æ˜¯æ¨¡å‹è®­ç»ƒæ—¶çš„é‡‡æ ·ä»£ç ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨äºè·å–è®­ç»ƒè¿›åº¦ã€‚æŒ‰ç…§è®ºæ–‡ä¸­çš„ç®—æ³•2å®ç°ï¼š\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1ij80f8TNBDzpKtqHjk_sh8o5aby3lmD7\" width=\"500\" />\n",
        "\n",
        "\n",
        "ä»æ‰©æ•£æ¨¡å‹ä¸­ç”Ÿæˆæ–°å›¾åƒçš„è¿‡ç¨‹æ˜¯é€šè¿‡æ‰©æ•£è¿‡ç¨‹çš„åå‘è¿‡ç¨‹æ¥å®ç°çš„ï¼šæˆ‘ä»¬ä»æ—¶é—´æ­¥$T$å¼€å§‹ï¼Œä»é«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ ·çº¯å™ªå£°ï¼Œç„¶åé€æ­¥å»å™ªï¼ˆä½¿ç”¨å®ƒæ‰€å­¦ä¹ çš„æ¡ä»¶æ¦‚ç‡ï¼‰ï¼Œç›´åˆ°æ—¶é—´æ­¥ $t=0$ã€‚å¦‚ä¸Šæ‰€ç¤ºï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ’å…¥ä½¿ç”¨æˆ‘ä»¬çš„å™ªå£°é¢„æµ‹å™¨çš„å‡å€¼çš„é‡å‚æ•°åŒ–æ¥å¾—åˆ°ç¨å¾®å»å™ªä¹‹åçš„å›¾åƒï¼Œæ–¹å·®æ˜¯é¢„å®šä¹‰å¥½çš„ã€‚\n",
        "\n",
        "ç†æƒ³æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¾—åˆ°çš„å›¾åƒçœ‹èµ·æ¥åƒæ¥è‡ªçœŸå®æ•°æ®åˆ†å¸ƒçš„å›¾åƒã€‚\n",
        "\n",
        "\n",
        "\n",
        "ä»£ç å®ç°å¦‚ä¸‹ï¼š"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7628fb3",
      "metadata": {
        "id": "f7628fb3"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def p_sample(model, x, t, t_index):\n",
        "    betas_t = extract(betas, t, x.shape)\n",
        "    sqrt_one_minus_alphas_cumprod_t = extract(\n",
        "        sqrt_one_minus_alphas_cumprod, t, x.shape\n",
        "    )\n",
        "    sqrt_recip_alphas_t = extract(sqrt_recip_alphas, t, x.shape)\n",
        "    \n",
        "    # Equation 11 in the paper\n",
        "    # Use our model (noise predictor) to predict the mean\n",
        "    model_mean = sqrt_recip_alphas_t * (\n",
        "        x - betas_t * model(x, t) / sqrt_one_minus_alphas_cumprod_t\n",
        "    )\n",
        "\n",
        "    if t_index == 0:\n",
        "        return model_mean\n",
        "    else:\n",
        "        posterior_variance_t = extract(posterior_variance, t, x.shape)\n",
        "        noise = torch.randn_like(x)\n",
        "        # Algorithm 2 line 4:\n",
        "        return model_mean + torch.sqrt(posterior_variance_t) * noise \n",
        "\n",
        "# Algorithm 2 but save all images:\n",
        "@torch.no_grad()\n",
        "def p_sample_loop(model, shape):\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    b = shape[0]\n",
        "    # start from pure noise (for each example in the batch)\n",
        "    img = torch.randn(shape, device=device)\n",
        "    imgs = []\n",
        "    \n",
        "    for i in tqdm(reversed(range(0, timesteps)), desc='sampling loop time step', total=timesteps):\n",
        "        img = p_sample(model, img, torch.full((b,), i, device=device, dtype=torch.long), i)\n",
        "        imgs.append(img.cpu().numpy())\n",
        "    return imgs\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample(model, image_size, batch_size=16, channels=3):\n",
        "    return p_sample_loop(model, shape=(batch_size, channels, image_size, image_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f70235f8",
      "metadata": {
        "id": "f70235f8"
      },
      "source": [
        "\n",
        "æ³¨æ„ï¼Œä¸Šé¢çš„ä»£ç æ˜¯åŸå§‹å®ç°çš„ç®€åŒ–ç‰ˆæœ¬ã€‚æˆ‘ä»¬å‘ç°æˆ‘ä»¬çš„ç®€åŒ–ç‰ˆæœ¬ï¼ˆä¸è®ºæ–‡ä¸­çš„ç®—æ³•2ä¸€è‡´ï¼‰ä¸[åŸå§‹æ›´å¤æ‚çš„å®ç°](https://github.com/hojonathanho/diffusion/blob/master/diffusion_tf/diffusion_utils.py)åŒæ ·æœ‰æ•ˆã€‚\n",
        "\n",
        "\n",
        "## è®­ç»ƒ\n",
        "\n",
        "æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æŒ‰ç…§é€šå¸¸çš„ PyTorch è®­ç»ƒæ–¹å¼æ¥è®­ç»ƒæ¨¡å‹ã€‚æˆ‘ä»¬è¿˜å®šä¹‰äº†ä¸€äº›é€»è¾‘ï¼Œä½¿ç”¨ä¸Šé¢å®šä¹‰çš„`sample`æ–¹æ³•ï¼Œä»¥ä¾¿å®šæœŸä¿å­˜ç”Ÿæˆçš„å›¾åƒã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c1ad663",
      "metadata": {
        "id": "0c1ad663"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def num_to_groups(num, divisor):\n",
        "    groups = num // divisor\n",
        "    remainder = num % divisor\n",
        "    arr = [divisor] * groups\n",
        "    if remainder > 0:\n",
        "        arr.append(remainder)\n",
        "    return arr\n",
        "\n",
        "results_folder = Path(\"./results\")\n",
        "results_folder.mkdir(exist_ok = True)\n",
        "save_and_sample_every = 1000"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22e4c0fd",
      "metadata": {
        "id": "22e4c0fd"
      },
      "source": [
        "\n",
        "å®šä¹‰å¥½æ¨¡å‹ï¼Œå°†å…¶ä¸¢åˆ°GPUä¸Šï¼Œä½¿ç”¨Adamè¿›è¡Œä¼˜åŒ–ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5126e21",
      "metadata": {
        "id": "a5126e21"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = Unet(\n",
        "    dim=image_size,\n",
        "    channels=channels,\n",
        "    dim_mults=(1, 2, 4,)\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fpDEauFFAcSL"
      },
      "id": "fpDEauFFAcSL"
    },
    {
      "cell_type": "markdown",
      "id": "f7444b0b",
      "metadata": {
        "id": "f7444b0b"
      },
      "source": [
        "å¼€å§‹è®­ç»ƒ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "92b12ed1",
      "metadata": {
        "id": "92b12ed1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "ff85d29b-c3d1-4c25-dab7-3dffb25f58e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0475182868540287\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-2f29e2e18cb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"huber\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-f7aa6893fb2e>\u001b[0m in \u001b[0;36mp_losses\u001b[0;34m(denoise_model, x_start, t, noise, loss_type)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mx_noisy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mpredicted_noise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdenoise_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_noisy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mloss_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l1'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-8a0af4eeafe6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, time)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownsample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdowns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-60b4818a128c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, time_emb)\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrearrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"b c -> b c 1 1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         return F.group_norm(\n\u001b[0m\u001b[1;32m    274\u001b[0m             input, self.num_groups, self.weight, self.bias, self.eps)\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mgroup_norm\u001b[0;34m(input, num_groups, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2526\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_groups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2527\u001b[0m     \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mnum_groups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_groups\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2528\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_groups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/backends/__init__.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobjtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__set__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from torchvision.utils import save_image\n",
        "\n",
        "epochs = 5\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for step, batch in enumerate(dataloader):\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      batch_size = batch[\"pixel_values\"].shape[0]\n",
        "      batch = batch[\"pixel_values\"].to(device)\n",
        "\n",
        "      # Algorithm 1 line 3: sample t uniformally for every example in the batch\n",
        "      t = torch.randint(0, timesteps, (batch_size,), device=device).long()\n",
        "\n",
        "      loss = p_losses(model, batch, t, loss_type=\"huber\")\n",
        "\n",
        "      if step % 100 == 0:\n",
        "        print(\"Loss:\", loss.item())\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # save generated images\n",
        "      if step != 0 and step % save_and_sample_every == 0:\n",
        "        milestone = step // save_and_sample_every\n",
        "        batches = num_to_groups(4, batch_size)\n",
        "        all_images_list = list(map(lambda n: sample(model, batch_size=n, channels=channels), batches))\n",
        "        all_images = torch.cat(all_images_list, dim=0)\n",
        "        all_images = (all_images + 1) * 0.5\n",
        "        save_image(all_images, str(results_folder / f'sample-{milestone}.png'), nrow = 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e617a66a",
      "metadata": {
        "id": "e617a66a"
      },
      "source": [
        "<div class=\"output stream stdout\">\n",
        "\n",
        "    Output:\n",
        "    ----------------------------------------------------------------------------------------------------\n",
        "    Loss: 0.46477368474006653\n",
        "    Loss: 0.12143351882696152\n",
        "    Loss: 0.08106148988008499\n",
        "    Loss: 0.0801810547709465\n",
        "    Loss: 0.06122320517897606\n",
        "    Loss: 0.06310459971427917\n",
        "    Loss: 0.05681884288787842\n",
        "    Loss: 0.05729678273200989\n",
        "    Loss: 0.05497899278998375\n",
        "    Loss: 0.04439849033951759\n",
        "    Loss: 0.05415581166744232\n",
        "    Loss: 0.06020551547408104\n",
        "    Loss: 0.046830907464027405\n",
        "    Loss: 0.051029372960329056\n",
        "    Loss: 0.0478244312107563\n",
        "    Loss: 0.046767622232437134\n",
        "    Loss: 0.04305662214756012\n",
        "    Loss: 0.05216279625892639\n",
        "    Loss: 0.04748568311333656\n",
        "    Loss: 0.05107741802930832\n",
        "    Loss: 0.04588869959115982\n",
        "    Loss: 0.043014321476221085\n",
        "    Loss: 0.046371955424547195\n",
        "    Loss: 0.04952816292643547\n",
        "    Loss: 0.04472338408231735\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8337c82",
      "metadata": {
        "id": "a8337c82"
      },
      "source": [
        "## é‡‡æ ·/æ¨ç†\n",
        "\n",
        "è®­ç»ƒå®Œäº†æˆ‘ä»¬å°±å¯ä»¥ä½¿ç”¨ä¸‹è¾¹è¿™ä¸ªå‡½æ•°è¿›è¡Œæ¨ç†é‡‡æ ·äº†ï¼š"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3d8a814",
      "metadata": {
        "id": "f3d8a814"
      },
      "outputs": [],
      "source": [
        "# sample 64 images\n",
        "samples = sample(model, image_size=image_size, batch_size=64, channels=channels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_s-Al2lJ2c8T",
      "metadata": {
        "id": "_s-Al2lJ2c8T"
      },
      "outputs": [],
      "source": [
        "# show a random one\n",
        "random_index = 5\n",
        "plt.imshow(samples[-1][random_index].reshape(image_size, image_size, channels), cmap=\"gray\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26ad579f",
      "metadata": {
        "id": "26ad579f"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1ytnzS7IW7ortC6ub85q7nud1IvXe2QTE\" width=\"300\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0k4H1fmlKvzR",
      "metadata": {
        "id": "0k4H1fmlKvzR"
      },
      "source": [
        "çœ‹èµ·æ¥è¿™ä¸ªæ¨¡å‹èƒ½å¤Ÿç”Ÿæˆä¸€ä¸ªä¸é”™çš„Tæ¤è¡«ï¼è¯·è®°ä½ï¼Œæˆ‘ä»¬è®­ç»ƒç”¨æ•°æ®é›†åˆ†è¾¨ç‡æ¯”è¾ƒä½ï¼ˆ28x28ï¼‰ï¼Œæ‰€ä»¥è¿™ä¸ªç»“æœè¿˜OKã€‚\n",
        "\n",
        "æˆ‘ä»¬è¿˜å¯ä»¥åˆ›å»ºä¸€ä¸ªå»å™ªè¿‡ç¨‹çš„ GIFï¼š"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "spE1I9aVNwzZ",
      "metadata": {
        "id": "spE1I9aVNwzZ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.animation as animation\n",
        "\n",
        "random_index = 53\n",
        "\n",
        "fig = plt.figure()\n",
        "ims = []\n",
        "for i in range(timesteps):\n",
        "    im = plt.imshow(samples[i][random_index].reshape(image_size, image_size, channels), cmap=\"gray\", animated=True)\n",
        "    ims.append([im])\n",
        "\n",
        "animate = animation.ArtistAnimation(fig, ims, interval=50, blit=True, repeat_delay=1000)\n",
        "animate.save('diffusion.gif')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b02eb802",
      "metadata": {
        "id": "b02eb802"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1eyonQWhfmbQsTq8ndsNjw5QSRQ9em9Au\" width=\"500\" />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# åç»­é˜…è¯»ğŸ“•\n",
        "\n",
        "- Improved Denoising Diffusion Probabilistic Models ([Nichol et al., 2021](https://arxiv.org/abs/2102.09672)): å­¦ä¹ æ¡ä»¶åˆ†å¸ƒçš„æ–¹å·®ï¼ˆé™¤å‡å€¼å¤–ï¼‰æœ‰åŠ©äºæé«˜æ€§èƒ½ã€‚\n",
        "\n",
        "- Cascaded Diffusion Models for High Fidelity Image Generation ([Ho et al., 2021](https://arxiv.org/abs/2106.15282)): å¼•å…¥çº§è”æ‰©æ•£ï¼ŒåŒ…æ‹¬å¤šä¸ªæ‰©æ•£æ¨¡å‹çš„pipelineï¼Œç”¨äºç”Ÿæˆé€æ­¥æé«˜åˆ†è¾¨ç‡çš„å›¾åƒï¼Œå®ç°é«˜ä¿çœŸåº¦å›¾åƒåˆæˆã€‚\n",
        "\n",
        "- Diffusion Models Beat GANs on Image Synthesis ([Dhariwal et al., 2021](https://arxiv.org/abs/2105.05233)): é€šè¿‡æ”¹è¿›U-Netæ¶æ„ï¼Œå¹¶å¼•å…¥åˆ†ç±»å™¨æŒ‡å¯¼ï¼Œè¯æ˜äº†æ‰©æ•£æ¨¡å‹å¯ä»¥å®ç°æ¯”å½“å‰æœ€å…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹æ›´å¥½çš„å›¾åƒæ ·æœ¬è´¨é‡ã€‚\n",
        "\n",
        "- Classifier-Free Diffusion Guidance ([Ho et al., 2021](https://openreview.net/pdf?id=qw8AKxfYbI)): é€šè¿‡è”åˆè®­ç»ƒä¸€ä¸ªæ¡ä»¶å’Œä¸€ä¸ªæ— æ¡ä»¶æ‰©æ•£æ¨¡å‹çš„å•ä¸ªç¥ç»ç½‘ç»œï¼Œå±•ç¤ºäº†æŒ‡å¯¼æ‰©æ•£æ¨¡å‹ä¸éœ€è¦åˆ†ç±»å™¨ã€‚\n",
        "\n",
        "- Hierarchical Text-Conditional Image Generation with CLIP Latents (DALL-E 2) ([Ramesh et al., 2022](https://cdn.openai.com/papers/dall-e-2.pdf)): ä½¿ç”¨å…ˆéªŒå°†æ–‡æœ¬æ ‡é¢˜è½¬åŒ–ä¸ºCLIPå›¾åƒåµŒå…¥ï¼Œç„¶åæ‰©æ•£æ¨¡å‹å°†å…¶è§£ç æˆå›¾åƒã€‚\n",
        "\n",
        "- Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding (ImageGen) ([Saharia et al., 2022](https://arxiv.org/abs/2205.11487)): å°†å¤§å‹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆä¾‹å¦‚T5ï¼‰ä¸çº§è”æ‰©æ•£ç›¸ç»“åˆï¼Œå¯ä»¥å¾ˆå¥½åœ°å®ç°æ–‡æœ¬åˆ°å›¾åƒçš„åˆæˆã€‚\n",
        "\n",
        "\n",
        "ç›®å‰ï¼Œæ‰©æ•£æ¨¡å‹çš„ä¸»è¦ï¼ˆæˆ–è®¸å”¯ä¸€çš„ï¼‰ç¼ºç‚¹ä¼¼ä¹æ˜¯éœ€è¦å¤šæ¬¡å‰å‘ä¼ é€’æ‰èƒ½ç”Ÿæˆä¸€å¼ å›¾åƒï¼ˆè€Œå¯¹äºGANç­‰ç”Ÿæˆæ¨¡å‹åˆ™ä¸éœ€è¦ï¼‰ã€‚ç„¶è€Œï¼Œ[Zhang et al., 2022](https://arxiv.org/abs/2204.13902)å¯ä»¥åœ¨ä¸åˆ°10ä¸ªå»å™ªæ­¥éª¤å†…å®ç°é«˜ä¿çœŸåº¦çš„ç”Ÿæˆã€‚"
      ],
      "metadata": {
        "id": "vTBA0onvwwil"
      },
      "id": "vTBA0onvwwil"
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "6fe49a34",
        "2d747688",
        "5153024b",
        "592aa765",
        "9ff47fbb",
        "51d9a24c",
        "9a8031b0",
        "06b3fad0",
        "a30368b2",
        "cc01c63b",
        "f70235f8",
        "b02eb802"
      ],
      "name": "annotated-diffusion.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}